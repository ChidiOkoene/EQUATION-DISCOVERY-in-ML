{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        # Dcefine layers: Linear -> ReLU -> Linear\n",
    "        self.fc1 = nn.Linear(2, 4) # input: 2 features output: 4 neurons\n",
    "        self.fc2 = nn.Linear(4, 1) # input: 4 features, output: 1 neuron\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define the forward pass\n",
    "        x = torch.relu(self.fc1(x)) # Apply ReLU activation \n",
    "        x = self.fc2(x) # Output layer (no activation here)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model, loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intantiate the model\n",
    "model = SimpleNN()\n",
    "\n",
    "# Define the loss function (Mean Squared Error for Regression)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define the optimizer (Stochastic Gradient Descent with learning Rate)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "y_data = torch.tensor([[5.0], [7.0], [9.0]])\n",
    "\n",
    "# Create a dataloarder for batch processing\n",
    "dataset = TensorDataset(x_data, y_data)\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 2.6190\n",
      "Epoch 20, Loss: 4.3560\n",
      "Epoch 30, Loss: 2.1745\n",
      "Epoch 40, Loss: 1.5392\n",
      "Epoch 50, Loss: 0.4031\n",
      "Epoch 60, Loss: 1.0287\n",
      "Epoch 70, Loss: 0.0655\n",
      "Epoch 80, Loss: 0.0011\n",
      "Epoch 90, Loss: 0.0029\n",
      "Epoch 100, Loss: 0.0020\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    for x_batch, y_batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(x_batch)\n",
    "        loss = criterion(predictions, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for input [[7.0, 8.0]]: 11.0581\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "test_data = torch.tensor([[7.0, 8.0]])\n",
    "predicted = model(test_data)\n",
    "print(f'Prediction for input {test_data.tolist()}: {predicted.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import a sample csv dataset for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('reconstructed_data/sample_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features (x_tensor):\n",
      " tensor([[52., 93., 15., 72., 61., 21.],\n",
      "        [83., 87., 75., 75., 88., 24.],\n",
      "        [ 3., 22., 53.,  2., 88., 30.],\n",
      "        [38.,  2., 64., 60., 21., 33.],\n",
      "        [76., 58., 22., 89., 49., 91.],\n",
      "        [59., 42., 92., 60., 80., 15.],\n",
      "        [62., 62., 47., 62., 51., 55.],\n",
      "        [64.,  3., 51.,  7., 21., 73.],\n",
      "        [39., 18.,  4., 89., 60., 14.],\n",
      "        [ 9., 90., 53.,  2., 84., 92.],\n",
      "        [60., 71., 44.,  8., 47., 35.],\n",
      "        [78., 81., 36., 50.,  4.,  2.],\n",
      "        [ 6., 54.,  4., 54., 93., 63.],\n",
      "        [18., 90., 44., 34., 74., 62.],\n",
      "        [14., 95., 48., 15., 72., 78.],\n",
      "        [87., 62., 40., 85., 80., 82.],\n",
      "        [53., 24., 26., 89., 60., 41.],\n",
      "        [29., 15., 45., 65., 89., 71.],\n",
      "        [ 9., 88.,  1.,  8., 88., 63.],\n",
      "        [11., 81.,  8., 35., 35., 33.],\n",
      "        [ 5., 41., 28.,  7., 73., 72.],\n",
      "        [12., 34., 33., 48., 23., 62.],\n",
      "        [88., 37., 99., 44., 86., 91.],\n",
      "        [35., 65., 99., 47., 78.,  3.],\n",
      "        [ 1.,  5., 90., 14., 27.,  9.],\n",
      "        [79., 15., 90., 42., 77., 51.],\n",
      "        [63., 96., 52., 96.,  4., 94.],\n",
      "        [23., 15., 43., 29., 36., 13.],\n",
      "        [32., 71., 59., 86., 28., 66.],\n",
      "        [42., 45., 62., 57.,  6., 28.],\n",
      "        [28., 44., 84., 30., 62., 75.],\n",
      "        [92., 89., 62., 97.,  1., 27.],\n",
      "        [62., 77.,  3., 70., 72., 27.],\n",
      "        [ 9., 62., 37., 97., 51., 44.],\n",
      "        [24., 79., 59., 32., 96., 88.],\n",
      "        [52., 62., 58., 52., 12., 39.],\n",
      "        [ 2.,  3., 56., 81., 59.,  2.],\n",
      "        [ 2., 92., 54., 87., 96., 97.],\n",
      "        [ 1., 19.,  2., 53., 44., 90.],\n",
      "        [32., 70., 32., 68., 55., 75.],\n",
      "        [56., 17., 38., 24., 69., 98.],\n",
      "        [70., 86., 11., 16., 97., 73.],\n",
      "        [59., 70., 80., 93.,  3., 20.],\n",
      "        [59., 36., 19., 90., 67., 19.],\n",
      "        [20., 96., 71., 52., 33., 40.],\n",
      "        [39., 82.,  1., 11., 92., 57.],\n",
      "        [89., 50., 23., 31., 94., 42.],\n",
      "        [99.,  7., 16., 90., 60.,  2.],\n",
      "        [ 1., 48., 12., 69., 37., 32.],\n",
      "        [ 9., 99., 19., 48., 80.,  3.],\n",
      "        [20., 24., 54., 33., 24., 75.],\n",
      "        [72., 36., 38., 84., 99., 89.],\n",
      "        [99., 25., 93., 18., 82., 66.],\n",
      "        [54., 35., 80., 61., 41., 33.],\n",
      "        [68., 33., 14., 21., 48., 20.],\n",
      "        [ 8.,  7., 67., 17., 33., 48.],\n",
      "        [76., 59., 86., 22., 30., 38.],\n",
      "        [51., 54.,  8., 27., 27., 98.],\n",
      "        [21., 30., 97., 28., 64., 97.],\n",
      "        [69., 61., 48., 19.,  4., 35.],\n",
      "        [64., 49., 17., 44., 92., 30.],\n",
      "        [93., 46.,  6., 99., 37., 24.],\n",
      "        [93., 46., 53., 95., 99., 60.],\n",
      "        [97., 63., 85., 32., 87., 33.],\n",
      "        [67., 18., 25., 95., 54., 58.],\n",
      "        [67., 46., 24., 32., 47., 86.],\n",
      "        [23., 66., 27.,  2., 90., 17.],\n",
      "        [33.,  9., 43., 48., 39., 93.],\n",
      "        [42., 26., 99., 50., 25., 24.],\n",
      "        [13., 60.,  7., 57., 36., 45.],\n",
      "        [20., 65.,  8., 16., 14., 76.],\n",
      "        [87., 15., 92., 98., 66., 32.],\n",
      "        [87., 63., 86., 51., 25., 58.],\n",
      "        [63., 62., 22., 58., 58., 86.],\n",
      "        [49., 52., 42., 70., 15., 54.],\n",
      "        [60., 97.,  8., 53., 60.,  5.],\n",
      "        [68.,  6., 96., 94., 47., 99.],\n",
      "        [55., 40., 52., 16., 13., 30.],\n",
      "        [19., 17., 63., 19., 92., 58.],\n",
      "        [55., 90., 90., 62., 23.,  9.],\n",
      "        [12.,  1., 58.,  1., 34., 96.],\n",
      "        [48., 89.,  1., 16., 61., 64.],\n",
      "        [63., 69., 22., 93., 67., 76.],\n",
      "        [26., 16., 51., 86., 57., 29.],\n",
      "        [78., 92., 69., 47., 94., 62.],\n",
      "        [69., 76., 16., 90., 90., 48.],\n",
      "        [85., 39., 33., 94., 23., 10.],\n",
      "        [69., 34., 52., 95., 10., 19.],\n",
      "        [58., 96.,  1., 69.,  4., 16.],\n",
      "        [24., 80.,  2., 92., 32., 91.],\n",
      "        [84., 24., 12., 50., 35., 33.],\n",
      "        [33., 61., 51., 43., 12., 67.],\n",
      "        [65., 33., 40., 74., 43., 44.],\n",
      "        [29., 13., 12., 95., 46.,  2.],\n",
      "        [35., 87., 81., 90.,  8., 93.],\n",
      "        [26., 74., 90., 34.,  7., 68.],\n",
      "        [58., 75., 29., 36., 89., 21.],\n",
      "        [36., 10., 73., 24., 64., 99.],\n",
      "        [49., 99., 36., 82., 96., 24.],\n",
      "        [23., 62., 96., 37., 12., 55.]])\n",
      "Targets (y_tensor):\n",
      " tensor([0.0085, 0.0130, 0.0434, 0.0610, 0.0176, 0.0193, 0.0221, 0.4649, 0.0059,\n",
      "        0.0181, 0.0323, 0.0376, 0.0044, 0.0144, 0.0168, 0.0165, 0.0173, 0.0232,\n",
      "        0.0000, 0.0098, 0.0264, 0.0334, 0.0369, 0.0118, 0.0707, 0.0473, 0.0585,\n",
      "        0.0446, 0.0227, 0.0679, 0.0351, 0.1060, 0.0039, 0.0105, 0.0173, 0.0472,\n",
      "        0.0082, 0.0065, 0.0036, 0.0161, 0.0598, 0.0132, 0.0639, 0.0116, 0.0191,\n",
      "        0.0000, 0.0207, 0.0106, 0.0070, 0.0043, 0.0617, 0.0176, 0.0675, 0.0319,\n",
      "        0.0299, 0.1003, 0.0494, 0.0271, 0.0453, 0.1149, 0.0144, 0.0095, 0.0159,\n",
      "        0.0252, 0.0202, 0.0336, 0.0169, 0.0526, 0.0482, 0.0099, 0.0297, 0.0241,\n",
      "        0.0430, 0.0189, 0.0383, 0.0056, 0.0388, 0.0861, 0.0487, 0.0214, 1.4259,\n",
      "        0.0000, 0.0127, 0.0192, 0.0171, 0.0089, 0.0211, 0.0407, 0.0000, 0.0033,\n",
      "        0.0277, 0.0509, 0.0258, 0.0076, 0.0442, 0.0703, 0.0133, 0.0810, 0.0083,\n",
      "        0.0555])\n"
     ]
    }
   ],
   "source": [
    "x_data = data.iloc[:, :-1].values\n",
    "y_data = data.iloc[:, -1]\n",
    "\n",
    "# convert data to Pytorch Tensors\n",
    "x_tensor = torch.tensor(x_data, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_data, dtype=torch.float32)\n",
    "\n",
    "print('features (x_tensor):\\n', x_tensor)\n",
    "print('Targets (y_tensor):\\n', y_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the NN\n",
    "class MultiLayerRegNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiLayerRegNN, self).__init__()\n",
    "        # Define layers: Linear -> ReLU -> Linear\n",
    "        self.fc1 = nn.Linear(6, 10) # input: 6 features output: 10 neurons\n",
    "        self.fc2 = nn.Linear(10, 8) # input: 10 features, output: 20 neuron\n",
    "        self.fc3 = nn.Linear(8, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define the forward pass\n",
    "        x = torch.relu(self.fc1(x)) # Apply ReLU activation \n",
    "        x = torch.relu(self.fc2(x)) # Apply ReLU activation \n",
    "        x = self.fc3(x) # Output layer (no activation here)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intantiate the model\n",
    "model = MultiLayerRegNN()\n",
    "\n",
    "# Define the loss function (Mean Squared Error for Regression)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define the optimizer (Stochastic Gradient Descent with learning Rate)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataloarder for batch processing\n",
    "dataset = TensorDataset(x_tensor, y_tensor)\n",
    "loader = DataLoader(dataset, batch_size=20, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chidi\\anaconda3\\envs\\boot_DA\\lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([20])) that is different to the input size (torch.Size([20, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Loss: 0.0096\n",
      "Epoch 200, Loss: 0.0008\n",
      "Epoch 300, Loss: 0.0009\n",
      "Epoch 400, Loss: 0.0009\n",
      "Epoch 500, Loss: 0.0009\n",
      "Epoch 600, Loss: 0.0010\n",
      "Epoch 700, Loss: 0.0958\n",
      "Epoch 800, Loss: 0.0097\n",
      "Epoch 900, Loss: 0.0093\n",
      "Epoch 1000, Loss: 0.0007\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    for x_batch, y_batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(x_batch)\n",
    "        loss = criterion(predictions, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 6])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Disable gradient calculation\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 8\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_data\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Get predictions\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m predictions\u001b[38;5;241m.\u001b[39msqueeze()  \u001b[38;5;66;03m# Remove extra dimensions if necessary\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# Calculate regression metrics\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\chidi\\anaconda3\\envs\\boot_DA\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\chidi\\anaconda3\\envs\\boot_DA\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[42], line 12\u001b[0m, in \u001b[0;36mMultiLayerRegNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# Define the forward pass\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;66;03m# Apply ReLU activation \u001b[39;00m\n\u001b[0;32m     13\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)) \u001b[38;5;66;03m# Apply ReLU activation \u001b[39;00m\n\u001b[0;32m     14\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(x) \u001b[38;5;66;03m# Output layer (no activation here)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\chidi\\anaconda3\\envs\\boot_DA\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\chidi\\anaconda3\\envs\\boot_DA\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\chidi\\anaconda3\\envs\\boot_DA\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not numpy.ndarray"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Disable gradient calculation\n",
    "with torch.no_grad():\n",
    "    predictions = model(x_data)  # Get predictions\n",
    "    predictions = predictions.squeeze()  # Remove extra dimensions if necessary\n",
    "\n",
    "    # Calculate regression metrics\n",
    "    mse = mean_squared_error(y_data.cpu(), predictions.cpu())\n",
    "    mae = mean_absolute_error(y_data.cpu(), predictions.cpu())\n",
    "    r2 = r2_score(y_data.cpu(), predictions.cpu())\n",
    "\n",
    "    print(f\"MSE: {mse:.2f}\")\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    print(f\"R² Score: {r2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiclass classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_wine = pd.read_csv('data/winequality-red.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.08</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.090</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.45</td>\n",
       "      <td>0.58</td>\n",
       "      <td>10.5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.062</td>\n",
       "      <td>39.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.99512</td>\n",
       "      <td>3.52</td>\n",
       "      <td>0.76</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.076</td>\n",
       "      <td>29.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99574</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.75</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99547</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0.71</td>\n",
       "      <td>10.2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.47</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.067</td>\n",
       "      <td>18.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.99549</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.66</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1599 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               7.4             0.700         0.00             1.9      0.076   \n",
       "1               7.8             0.880         0.00             2.6      0.098   \n",
       "2               7.8             0.760         0.04             2.3      0.092   \n",
       "3              11.2             0.280         0.56             1.9      0.075   \n",
       "4               7.4             0.700         0.00             1.9      0.076   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "1594            6.2             0.600         0.08             2.0      0.090   \n",
       "1595            5.9             0.550         0.10             2.2      0.062   \n",
       "1596            6.3             0.510         0.13             2.3      0.076   \n",
       "1597            5.9             0.645         0.12             2.0      0.075   \n",
       "1598            6.0             0.310         0.47             3.6      0.067   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "1                    25.0                  67.0  0.99680  3.20       0.68   \n",
       "2                    15.0                  54.0  0.99700  3.26       0.65   \n",
       "3                    17.0                  60.0  0.99800  3.16       0.58   \n",
       "4                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "1594                 32.0                  44.0  0.99490  3.45       0.58   \n",
       "1595                 39.0                  51.0  0.99512  3.52       0.76   \n",
       "1596                 29.0                  40.0  0.99574  3.42       0.75   \n",
       "1597                 32.0                  44.0  0.99547  3.57       0.71   \n",
       "1598                 18.0                  42.0  0.99549  3.39       0.66   \n",
       "\n",
       "      alcohol  quality  \n",
       "0         9.4        5  \n",
       "1         9.8        5  \n",
       "2         9.8        5  \n",
       "3         9.8        6  \n",
       "4         9.4        5  \n",
       "...       ...      ...  \n",
       "1594     10.5        5  \n",
       "1595     11.2        6  \n",
       "1596     11.0        6  \n",
       "1597     10.2        5  \n",
       "1598     11.0        6  \n",
       "\n",
       "[1599 rows x 12 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 6, 7, 4, 8, 3])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_wine['quality'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features (Train): torch.Size([1279, 11])\n",
      "Targets (Train): torch.Size([1279])\n"
     ]
    }
   ],
   "source": [
    "x_data_wine = data_wine.iloc[:, :-1].values\n",
    "y_data_wine = data_wine.iloc[:, -1]\n",
    "\n",
    "# convert data to Pytorch Tensors\n",
    "x_tensor = torch.tensor(x_data_wine, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_data_wine, dtype=torch.long)\n",
    "\n",
    "# Step 4: Split the dataset into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x_data_wine, y_data_wine, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.to_numpy(), dtype=torch.long)  # Ensure Series is converted to numpy\n",
    "x_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.to_numpy(), dtype=torch.long)  # Ensure Series is converted to numpy\n",
    "\n",
    "# Print to verify\n",
    "print(\"Features (Train):\", x_train_tensor.shape)\n",
    "print(\"Targets (Train):\", y_train_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 6\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(y_train_tensor.unique())\n",
    "print(\"Number of classes:\", num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "\n",
    "# Encode labels to integers starting from 0\n",
    "le = LabelEncoder()\n",
    "y_data_wine = le.fit_transform(y_data_wine)\n",
    "\n",
    "# Split into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data_wine, y_data_wine, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "x_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Update model's output layer\n",
    "num_classes = len(le.classes_)  # Number of unique classes\n",
    "model_2 = nn.Sequential(\n",
    "    nn.Linear(11, 88),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(88, 55),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(55, 40),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(40, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 44),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(44, 22),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(22, num_classes)  # Ensure correct output size\n",
    ")\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # No need for one-hot encoding\n",
    "optimizer = torch.optim.Adam(model_2.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1000/10000], Loss: 0.8438\n",
      "Epoch [2000/10000], Loss: 0.7482\n",
      "Epoch [3000/10000], Loss: 0.6858\n",
      "Epoch [4000/10000], Loss: 0.6425\n",
      "Epoch [5000/10000], Loss: 0.6227\n",
      "Epoch [6000/10000], Loss: 0.5849\n",
      "Epoch [7000/10000], Loss: 0.5718\n",
      "Epoch [8000/10000], Loss: 0.5541\n",
      "Epoch [9000/10000], Loss: 0.6119\n",
      "Epoch [10000/10000], Loss: 0.5420\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "for epoch in range(10000):  # Train for 100 epochs\n",
    "    optimizer.zero_grad()           # Reset gradients\n",
    "    outputs = model_2(x_train_tensor)         # Forward pass\n",
    "    loss = criterion(outputs, y_train_tensor)  # Compute loss\n",
    "    loss.backward()                 # Backpropagation\n",
    "    optimizer.step()                # Update weights\n",
    "\n",
    "    if (epoch + 1) % 1000 == 0:  # Print every 10 epochs\n",
    "        print(f\"Epoch [{epoch+1}/10000], Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test the model\n",
    "test_data = x_test_tensor  # New input\n",
    "predictions = model_2(test_data)\n",
    "predicted_class = torch.argmax(predictions, dim=1)  # Get the predicted class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: Predicted class: 3\n",
      "Sample 1: Predicted class: 2\n",
      "Sample 2: Predicted class: 3\n",
      "Sample 3: Predicted class: 2\n",
      "Sample 4: Predicted class: 3\n",
      "Sample 5: Predicted class: 2\n",
      "Sample 6: Predicted class: 2\n",
      "Sample 7: Predicted class: 2\n",
      "Sample 8: Predicted class: 3\n",
      "Sample 9: Predicted class: 3\n",
      "Sample 10: Predicted class: 4\n",
      "Sample 11: Predicted class: 2\n",
      "Sample 12: Predicted class: 3\n",
      "Sample 13: Predicted class: 2\n",
      "Sample 14: Predicted class: 3\n",
      "Sample 15: Predicted class: 4\n",
      "Sample 16: Predicted class: 3\n",
      "Sample 17: Predicted class: 3\n",
      "Sample 18: Predicted class: 4\n",
      "Sample 19: Predicted class: 2\n",
      "Sample 20: Predicted class: 2\n",
      "Sample 21: Predicted class: 2\n",
      "Sample 22: Predicted class: 3\n",
      "Sample 23: Predicted class: 3\n",
      "Sample 24: Predicted class: 2\n",
      "Sample 25: Predicted class: 2\n",
      "Sample 26: Predicted class: 3\n",
      "Sample 27: Predicted class: 2\n",
      "Sample 28: Predicted class: 3\n",
      "Sample 29: Predicted class: 3\n",
      "Sample 30: Predicted class: 2\n",
      "Sample 31: Predicted class: 2\n",
      "Sample 32: Predicted class: 3\n",
      "Sample 33: Predicted class: 3\n",
      "Sample 34: Predicted class: 3\n",
      "Sample 35: Predicted class: 2\n",
      "Sample 36: Predicted class: 3\n",
      "Sample 37: Predicted class: 3\n",
      "Sample 38: Predicted class: 3\n",
      "Sample 39: Predicted class: 3\n",
      "Sample 40: Predicted class: 3\n",
      "Sample 41: Predicted class: 2\n",
      "Sample 42: Predicted class: 4\n",
      "Sample 43: Predicted class: 2\n",
      "Sample 44: Predicted class: 3\n",
      "Sample 45: Predicted class: 3\n",
      "Sample 46: Predicted class: 4\n",
      "Sample 47: Predicted class: 3\n",
      "Sample 48: Predicted class: 2\n",
      "Sample 49: Predicted class: 2\n",
      "Sample 50: Predicted class: 2\n",
      "Sample 51: Predicted class: 2\n",
      "Sample 52: Predicted class: 3\n",
      "Sample 53: Predicted class: 4\n",
      "Sample 54: Predicted class: 2\n",
      "Sample 55: Predicted class: 3\n",
      "Sample 56: Predicted class: 3\n",
      "Sample 57: Predicted class: 1\n",
      "Sample 58: Predicted class: 3\n",
      "Sample 59: Predicted class: 2\n",
      "Sample 60: Predicted class: 3\n",
      "Sample 61: Predicted class: 3\n",
      "Sample 62: Predicted class: 2\n",
      "Sample 63: Predicted class: 2\n",
      "Sample 64: Predicted class: 4\n",
      "Sample 65: Predicted class: 2\n",
      "Sample 66: Predicted class: 4\n",
      "Sample 67: Predicted class: 3\n",
      "Sample 68: Predicted class: 3\n",
      "Sample 69: Predicted class: 2\n",
      "Sample 70: Predicted class: 4\n",
      "Sample 71: Predicted class: 2\n",
      "Sample 72: Predicted class: 3\n",
      "Sample 73: Predicted class: 3\n",
      "Sample 74: Predicted class: 3\n",
      "Sample 75: Predicted class: 2\n",
      "Sample 76: Predicted class: 3\n",
      "Sample 77: Predicted class: 2\n",
      "Sample 78: Predicted class: 3\n",
      "Sample 79: Predicted class: 3\n",
      "Sample 80: Predicted class: 2\n",
      "Sample 81: Predicted class: 4\n",
      "Sample 82: Predicted class: 2\n",
      "Sample 83: Predicted class: 3\n",
      "Sample 84: Predicted class: 3\n",
      "Sample 85: Predicted class: 3\n",
      "Sample 86: Predicted class: 2\n",
      "Sample 87: Predicted class: 3\n",
      "Sample 88: Predicted class: 3\n",
      "Sample 89: Predicted class: 1\n",
      "Sample 90: Predicted class: 3\n",
      "Sample 91: Predicted class: 2\n",
      "Sample 92: Predicted class: 2\n",
      "Sample 93: Predicted class: 2\n",
      "Sample 94: Predicted class: 2\n",
      "Sample 95: Predicted class: 3\n",
      "Sample 96: Predicted class: 3\n",
      "Sample 97: Predicted class: 3\n",
      "Sample 98: Predicted class: 2\n",
      "Sample 99: Predicted class: 3\n",
      "Sample 100: Predicted class: 2\n",
      "Sample 101: Predicted class: 2\n",
      "Sample 102: Predicted class: 3\n",
      "Sample 103: Predicted class: 4\n",
      "Sample 104: Predicted class: 2\n",
      "Sample 105: Predicted class: 3\n",
      "Sample 106: Predicted class: 3\n",
      "Sample 107: Predicted class: 3\n",
      "Sample 108: Predicted class: 3\n",
      "Sample 109: Predicted class: 2\n",
      "Sample 110: Predicted class: 3\n",
      "Sample 111: Predicted class: 2\n",
      "Sample 112: Predicted class: 3\n",
      "Sample 113: Predicted class: 2\n",
      "Sample 114: Predicted class: 3\n",
      "Sample 115: Predicted class: 4\n",
      "Sample 116: Predicted class: 2\n",
      "Sample 117: Predicted class: 3\n",
      "Sample 118: Predicted class: 3\n",
      "Sample 119: Predicted class: 3\n",
      "Sample 120: Predicted class: 3\n",
      "Sample 121: Predicted class: 2\n",
      "Sample 122: Predicted class: 3\n",
      "Sample 123: Predicted class: 3\n",
      "Sample 124: Predicted class: 2\n",
      "Sample 125: Predicted class: 2\n",
      "Sample 126: Predicted class: 4\n",
      "Sample 127: Predicted class: 3\n",
      "Sample 128: Predicted class: 2\n",
      "Sample 129: Predicted class: 3\n",
      "Sample 130: Predicted class: 3\n",
      "Sample 131: Predicted class: 2\n",
      "Sample 132: Predicted class: 3\n",
      "Sample 133: Predicted class: 4\n",
      "Sample 134: Predicted class: 2\n",
      "Sample 135: Predicted class: 2\n",
      "Sample 136: Predicted class: 2\n",
      "Sample 137: Predicted class: 2\n",
      "Sample 138: Predicted class: 2\n",
      "Sample 139: Predicted class: 3\n",
      "Sample 140: Predicted class: 4\n",
      "Sample 141: Predicted class: 2\n",
      "Sample 142: Predicted class: 3\n",
      "Sample 143: Predicted class: 3\n",
      "Sample 144: Predicted class: 3\n",
      "Sample 145: Predicted class: 2\n",
      "Sample 146: Predicted class: 3\n",
      "Sample 147: Predicted class: 3\n",
      "Sample 148: Predicted class: 3\n",
      "Sample 149: Predicted class: 2\n",
      "Sample 150: Predicted class: 3\n",
      "Sample 151: Predicted class: 3\n",
      "Sample 152: Predicted class: 2\n",
      "Sample 153: Predicted class: 3\n",
      "Sample 154: Predicted class: 3\n",
      "Sample 155: Predicted class: 3\n",
      "Sample 156: Predicted class: 2\n",
      "Sample 157: Predicted class: 3\n",
      "Sample 158: Predicted class: 3\n",
      "Sample 159: Predicted class: 3\n",
      "Sample 160: Predicted class: 4\n",
      "Sample 161: Predicted class: 3\n",
      "Sample 162: Predicted class: 3\n",
      "Sample 163: Predicted class: 2\n",
      "Sample 164: Predicted class: 2\n",
      "Sample 165: Predicted class: 3\n",
      "Sample 166: Predicted class: 3\n",
      "Sample 167: Predicted class: 2\n",
      "Sample 168: Predicted class: 2\n",
      "Sample 169: Predicted class: 2\n",
      "Sample 170: Predicted class: 3\n",
      "Sample 171: Predicted class: 2\n",
      "Sample 172: Predicted class: 4\n",
      "Sample 173: Predicted class: 3\n",
      "Sample 174: Predicted class: 4\n",
      "Sample 175: Predicted class: 3\n",
      "Sample 176: Predicted class: 4\n",
      "Sample 177: Predicted class: 2\n",
      "Sample 178: Predicted class: 2\n",
      "Sample 179: Predicted class: 2\n",
      "Sample 180: Predicted class: 0\n",
      "Sample 181: Predicted class: 3\n",
      "Sample 182: Predicted class: 3\n",
      "Sample 183: Predicted class: 4\n",
      "Sample 184: Predicted class: 3\n",
      "Sample 185: Predicted class: 4\n",
      "Sample 186: Predicted class: 2\n",
      "Sample 187: Predicted class: 3\n",
      "Sample 188: Predicted class: 4\n",
      "Sample 189: Predicted class: 3\n",
      "Sample 190: Predicted class: 3\n",
      "Sample 191: Predicted class: 4\n",
      "Sample 192: Predicted class: 3\n",
      "Sample 193: Predicted class: 3\n",
      "Sample 194: Predicted class: 3\n",
      "Sample 195: Predicted class: 2\n",
      "Sample 196: Predicted class: 3\n",
      "Sample 197: Predicted class: 3\n",
      "Sample 198: Predicted class: 3\n",
      "Sample 199: Predicted class: 3\n",
      "Sample 200: Predicted class: 2\n",
      "Sample 201: Predicted class: 3\n",
      "Sample 202: Predicted class: 2\n",
      "Sample 203: Predicted class: 2\n",
      "Sample 204: Predicted class: 3\n",
      "Sample 205: Predicted class: 3\n",
      "Sample 206: Predicted class: 2\n",
      "Sample 207: Predicted class: 3\n",
      "Sample 208: Predicted class: 3\n",
      "Sample 209: Predicted class: 4\n",
      "Sample 210: Predicted class: 2\n",
      "Sample 211: Predicted class: 2\n",
      "Sample 212: Predicted class: 2\n",
      "Sample 213: Predicted class: 4\n",
      "Sample 214: Predicted class: 4\n",
      "Sample 215: Predicted class: 4\n",
      "Sample 216: Predicted class: 3\n",
      "Sample 217: Predicted class: 3\n",
      "Sample 218: Predicted class: 3\n",
      "Sample 219: Predicted class: 2\n",
      "Sample 220: Predicted class: 2\n",
      "Sample 221: Predicted class: 3\n",
      "Sample 222: Predicted class: 4\n",
      "Sample 223: Predicted class: 4\n",
      "Sample 224: Predicted class: 2\n",
      "Sample 225: Predicted class: 2\n",
      "Sample 226: Predicted class: 2\n",
      "Sample 227: Predicted class: 3\n",
      "Sample 228: Predicted class: 2\n",
      "Sample 229: Predicted class: 2\n",
      "Sample 230: Predicted class: 2\n",
      "Sample 231: Predicted class: 2\n",
      "Sample 232: Predicted class: 4\n",
      "Sample 233: Predicted class: 3\n",
      "Sample 234: Predicted class: 2\n",
      "Sample 235: Predicted class: 3\n",
      "Sample 236: Predicted class: 2\n",
      "Sample 237: Predicted class: 3\n",
      "Sample 238: Predicted class: 3\n",
      "Sample 239: Predicted class: 2\n",
      "Sample 240: Predicted class: 3\n",
      "Sample 241: Predicted class: 2\n",
      "Sample 242: Predicted class: 3\n",
      "Sample 243: Predicted class: 3\n",
      "Sample 244: Predicted class: 3\n",
      "Sample 245: Predicted class: 4\n",
      "Sample 246: Predicted class: 4\n",
      "Sample 247: Predicted class: 2\n",
      "Sample 248: Predicted class: 4\n",
      "Sample 249: Predicted class: 2\n",
      "Sample 250: Predicted class: 3\n",
      "Sample 251: Predicted class: 2\n",
      "Sample 252: Predicted class: 2\n",
      "Sample 253: Predicted class: 3\n",
      "Sample 254: Predicted class: 2\n",
      "Sample 255: Predicted class: 2\n",
      "Sample 256: Predicted class: 2\n",
      "Sample 257: Predicted class: 3\n",
      "Sample 258: Predicted class: 3\n",
      "Sample 259: Predicted class: 3\n",
      "Sample 260: Predicted class: 3\n",
      "Sample 261: Predicted class: 1\n",
      "Sample 262: Predicted class: 2\n",
      "Sample 263: Predicted class: 4\n",
      "Sample 264: Predicted class: 3\n",
      "Sample 265: Predicted class: 3\n",
      "Sample 266: Predicted class: 2\n",
      "Sample 267: Predicted class: 2\n",
      "Sample 268: Predicted class: 3\n",
      "Sample 269: Predicted class: 3\n",
      "Sample 270: Predicted class: 2\n",
      "Sample 271: Predicted class: 3\n",
      "Sample 272: Predicted class: 3\n",
      "Sample 273: Predicted class: 3\n",
      "Sample 274: Predicted class: 2\n",
      "Sample 275: Predicted class: 2\n",
      "Sample 276: Predicted class: 3\n",
      "Sample 277: Predicted class: 4\n",
      "Sample 278: Predicted class: 4\n",
      "Sample 279: Predicted class: 2\n",
      "Sample 280: Predicted class: 4\n",
      "Sample 281: Predicted class: 2\n",
      "Sample 282: Predicted class: 3\n",
      "Sample 283: Predicted class: 3\n",
      "Sample 284: Predicted class: 3\n",
      "Sample 285: Predicted class: 4\n",
      "Sample 286: Predicted class: 2\n",
      "Sample 287: Predicted class: 2\n",
      "Sample 288: Predicted class: 2\n",
      "Sample 289: Predicted class: 3\n",
      "Sample 290: Predicted class: 1\n",
      "Sample 291: Predicted class: 3\n",
      "Sample 292: Predicted class: 2\n",
      "Sample 293: Predicted class: 3\n",
      "Sample 294: Predicted class: 3\n",
      "Sample 295: Predicted class: 3\n",
      "Sample 296: Predicted class: 2\n",
      "Sample 297: Predicted class: 4\n",
      "Sample 298: Predicted class: 3\n",
      "Sample 299: Predicted class: 4\n",
      "Sample 300: Predicted class: 3\n",
      "Sample 301: Predicted class: 2\n",
      "Sample 302: Predicted class: 4\n",
      "Sample 303: Predicted class: 2\n",
      "Sample 304: Predicted class: 2\n",
      "Sample 305: Predicted class: 2\n",
      "Sample 306: Predicted class: 3\n",
      "Sample 307: Predicted class: 4\n",
      "Sample 308: Predicted class: 4\n",
      "Sample 309: Predicted class: 4\n",
      "Sample 310: Predicted class: 3\n",
      "Sample 311: Predicted class: 3\n",
      "Sample 312: Predicted class: 3\n",
      "Sample 313: Predicted class: 3\n",
      "Sample 314: Predicted class: 3\n",
      "Sample 315: Predicted class: 3\n",
      "Sample 316: Predicted class: 2\n",
      "Sample 317: Predicted class: 2\n",
      "Sample 318: Predicted class: 3\n",
      "Sample 319: Predicted class: 2\n"
     ]
    }
   ],
   "source": [
    "for i, pred in enumerate(predicted_class):\n",
    "    print(f\"Sample {i}: Predicted class: {pred.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 58.44%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       0.25      0.10      0.14        10\n",
      "           2       0.66      0.61      0.63       130\n",
      "           3       0.55      0.64      0.59       132\n",
      "           4       0.52      0.55      0.53        42\n",
      "           5       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.58       320\n",
      "   macro avg       0.33      0.32      0.32       320\n",
      "weighted avg       0.57      0.58      0.58       320\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chidi\\anaconda3\\envs\\boot_DA\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\chidi\\anaconda3\\envs\\boot_DA\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\chidi\\anaconda3\\envs\\boot_DA\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model_2.eval()\n",
    "\n",
    "# Disable gradient calculation\n",
    "with torch.no_grad():\n",
    "    # Get predictions\n",
    "    predictions = model_2(x_test_tensor)  # Pass test data through model\n",
    "    predicted_classes = torch.argmax(predictions, dim=1)  # Predicted labels\n",
    "    \n",
    "    # Evaluate accuracy\n",
    "    accuracy = accuracy_score(y_test_tensor.cpu(), predicted_classes.cpu())\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    # Print detailed metrics (optional)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test_tensor.cpu(), predicted_classes.cpu()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train_tensor: tensor([3, 3, 3, 2, 2])\n",
      "y_train_one_hot:\n",
      " tensor([[0, 0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 0],\n",
      "        [0, 0, 1, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Encode labels to integers starting from 0\n",
    "le = LabelEncoder()\n",
    "y_data_wine = le.fit_transform(y_data_wine)\n",
    "\n",
    "# Convert to tensors and one-hot encode\n",
    "y_tensor = torch.tensor(y_data_wine, dtype=torch.long)  # Convert to tensor\n",
    "y_one_hot = torch.nn.functional.one_hot(y_tensor, num_classes=len(le.classes_))\n",
    "\n",
    "# Split into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data_wine, y_data_wine, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "x_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# One-hot encode target tensors for training/testing\n",
    "y_train_one_hot = torch.nn.functional.one_hot(y_train_tensor, num_classes=len(le.classes_))\n",
    "y_test_one_hot = torch.nn.functional.one_hot(y_test_tensor, num_classes=len(le.classes_))\n",
    "\n",
    "# Print for verification\n",
    "print(\"y_train_tensor:\", y_train_tensor[:5])\n",
    "print(\"y_train_one_hot:\\n\", y_train_one_hot[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the model\n",
    "model_3 = nn.Sequential(\n",
    "    nn.Linear(11, 88),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(88, 55),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(55, 40),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(40, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, len(le.classes_))  # Output layer matches the number of classes\n",
    ")\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.MSELoss()  # Use Mean Squared Error for one-hot labels\n",
    "optimizer = torch.optim.Adam(model_3.parameters(), lr=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/5000], Loss: 0.1069\n",
      "Epoch [200/5000], Loss: 0.1069\n",
      "Epoch [300/5000], Loss: 0.1069\n",
      "Epoch [400/5000], Loss: 0.1069\n",
      "Epoch [500/5000], Loss: 0.1069\n",
      "Epoch [600/5000], Loss: 0.1069\n",
      "Epoch [700/5000], Loss: 0.1069\n",
      "Epoch [800/5000], Loss: 0.1069\n",
      "Epoch [900/5000], Loss: 0.1069\n",
      "Epoch [1000/5000], Loss: 0.1069\n",
      "Epoch [1100/5000], Loss: 0.1069\n",
      "Epoch [1200/5000], Loss: 0.1069\n",
      "Epoch [1300/5000], Loss: 0.1069\n",
      "Epoch [1400/5000], Loss: 0.1069\n",
      "Epoch [1500/5000], Loss: 0.1069\n",
      "Epoch [1600/5000], Loss: 0.1069\n",
      "Epoch [1700/5000], Loss: 0.1069\n",
      "Epoch [1800/5000], Loss: 0.1069\n",
      "Epoch [1900/5000], Loss: 0.1069\n",
      "Epoch [2000/5000], Loss: 0.1069\n",
      "Epoch [2100/5000], Loss: 0.1069\n",
      "Epoch [2200/5000], Loss: 0.1069\n",
      "Epoch [2300/5000], Loss: 0.1069\n",
      "Epoch [2400/5000], Loss: 0.1069\n",
      "Epoch [2500/5000], Loss: 0.1069\n",
      "Epoch [2600/5000], Loss: 0.1069\n",
      "Epoch [2700/5000], Loss: 0.1069\n",
      "Epoch [2800/5000], Loss: 0.1069\n",
      "Epoch [2900/5000], Loss: 0.1069\n",
      "Epoch [3000/5000], Loss: 0.1069\n",
      "Epoch [3100/5000], Loss: 0.1069\n",
      "Epoch [3200/5000], Loss: 0.1069\n",
      "Epoch [3300/5000], Loss: 0.1069\n",
      "Epoch [3400/5000], Loss: 0.1069\n",
      "Epoch [3500/5000], Loss: 0.1069\n",
      "Epoch [3600/5000], Loss: 0.1069\n",
      "Epoch [3700/5000], Loss: 0.1069\n",
      "Epoch [3800/5000], Loss: 0.1069\n",
      "Epoch [3900/5000], Loss: 0.1069\n",
      "Epoch [4000/5000], Loss: 0.1069\n",
      "Epoch [4100/5000], Loss: 0.1069\n",
      "Epoch [4200/5000], Loss: 0.1069\n",
      "Epoch [4300/5000], Loss: 0.1069\n",
      "Epoch [4400/5000], Loss: 0.1069\n",
      "Epoch [4500/5000], Loss: 0.1069\n",
      "Epoch [4600/5000], Loss: 0.1069\n",
      "Epoch [4700/5000], Loss: 0.1069\n",
      "Epoch [4800/5000], Loss: 0.1069\n",
      "Epoch [4900/5000], Loss: 0.1069\n",
      "Epoch [5000/5000], Loss: 0.1069\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 5000\n",
    "for epoch in range(epochs):\n",
    "    model_3.train()\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model_3(x_train_tensor)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = criterion(outputs, y_train_one_hot.float())  # Ensure targets are float\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:  # Print every 10 epochs\n",
    "        print(f\"Epoch [{epoch+1}/5000], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.4062\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "model_3.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model_3(x_test_tensor)\n",
    "    predictions = torch.argmax(test_outputs, dim=1)  # Convert probabilities to class indices\n",
    "    actuals = torch.argmax(y_test_one_hot, dim=1)    # Convert one-hot labels to class indices\n",
    "\n",
    "    accuracy = (predictions == actuals).float().mean()\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "students = [\"Alice\", \"Bob\", \"Charlie\"]\n",
    "subjects = [\"Math\", \"History\", \"Biology\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alice', 'Bob', 'Charlie']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Math', 'History', 'Biology']\n"
     ]
    }
   ],
   "source": [
    "print(subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice Math\n",
      "Bob History\n",
      "Charlie Biology\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for stud, sub in zip(students, subjects):\n",
    "    print(stud, sub)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factorial(num: int) -> int:\n",
    "    n = num\n",
    "    if n < 0:\n",
    "        return 'reached the boundary'\n",
    "    result = 1\n",
    "\n",
    "    for i in range(1, n+1):\n",
    "        result *= i\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    }
   ],
   "source": [
    "print(factorial(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "boot_DA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

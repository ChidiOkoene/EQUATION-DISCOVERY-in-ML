{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy.io import loadmat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath):\n",
    "    \"\"\"\n",
    "    Load Burgers' equation data or similar PDE dataset.\n",
    "    \"\"\"\n",
    "    data = loadmat(filepath)\n",
    "    t = np.real(data['t'].flatten())[:, None]  # Time points\n",
    "    x = np.real(data['x'].flatten())[:, None]  # Spatial points\n",
    "    Exact = np.real(data['usol']).T           # Solution matrix\n",
    "\n",
    "    # Create space-time grid\n",
    "    X, T = np.meshgrid(x, t)\n",
    "    X_star = np.hstack((X.flatten()[:, None], T.flatten()[:, None]))\n",
    "    u_star = Exact.flatten()[:, None]\n",
    "\n",
    "    # Bounds for normalization\n",
    "    lb = X_star.min(0)\n",
    "    ub = X_star.max(0)\n",
    "\n",
    "    return X_star, u_star, lb, ub, Exact, X, T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(X_star, u_star, collocation_points, noise=0.01, N_u_s=100, split=0.8):\n",
    "    \"\"\"\n",
    "    Prepare measurement data and collocation points.\n",
    "    \"\"\"\n",
    "    # Randomly sample measurement points\n",
    "    idx_s = np.random.choice(X_star.shape[0], N_u_s, replace=False)\n",
    "    X_u_meas = X_star[idx_s, :]\n",
    "    u_meas = u_star[idx_s, :]\n",
    "\n",
    "    # Add noise to measurements\n",
    "    u_meas += noise * np.std(u_meas) * np.random.randn(*u_meas.shape)\n",
    "\n",
    "    # Split measurement data into training and validation\n",
    "    N_u_train = int(split * N_u_s)\n",
    "    idx_train = np.random.choice(N_u_s, N_u_train, replace=False)\n",
    "\n",
    "    X_u_train = X_u_meas[idx_train, :]\n",
    "    u_train = u_meas[idx_train, :]\n",
    "\n",
    "    idx_val = np.setdiff1d(np.arange(N_u_s), idx_train)\n",
    "    X_u_val = X_u_meas[idx_val, :]\n",
    "    u_val = u_meas[idx_val, :]\n",
    "\n",
    "    # Use provided collocation points\n",
    "    X_f_train = collocation_points\n",
    "\n",
    "    return X_u_train, u_train, X_u_val, u_val, X_f_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicsInformedNN(nn.Module):\n",
    "    def __init__(self, X, u, X_f, X_val, u_val, layers, lb, ub):\n",
    "        super(PhysicsInformedNN, self).__init__()\n",
    "        self.lb = torch.tensor(lb, dtype=torch.float32)\n",
    "        self.ub = torch.tensor(ub, dtype=torch.float32)\n",
    "        self.model = self._initialize_nn(layers)\n",
    "\n",
    "        self.lambda1 = nn.Parameter(torch.zeros(16, 1, dtype=torch.float32), requires_grad=True)\n",
    "\n",
    "        self.x = torch.tensor(X[:, 0:1], dtype=torch.float32)\n",
    "        self.t = torch.tensor(X[:, 1:2], dtype=torch.float32)\n",
    "        self.u = torch.tensor(u, dtype=torch.float32)\n",
    "        self.x_f = torch.tensor(X_f[:, 0:1], dtype=torch.float32)\n",
    "        self.t_f = torch.tensor(X_f[:, 1:2], dtype=torch.float32)\n",
    "\n",
    "        self.x_val = torch.tensor(X_val[:, 0:1], dtype=torch.float32)\n",
    "        self.t_val = torch.tensor(X_val[:, 1:2], dtype=torch.float32)\n",
    "        self.u_val = torch.tensor(u_val, dtype=torch.float32)\n",
    "\n",
    "    def _initialize_nn(self, layers):\n",
    "        modules = []\n",
    "        for i in range(len(layers) - 1):\n",
    "            modules.append(nn.Linear(layers[i], layers[i + 1]))\n",
    "            if i < len(layers) - 2:\n",
    "                modules.append(nn.Tanh())\n",
    "\n",
    "        model = nn.Sequential(*modules)\n",
    "\n",
    "        for layer in model:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_normal_(layer.weight)\n",
    "                nn.init.zeros_(layer.bias)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        inputs = torch.cat([x, t], dim=1)\n",
    "        inputs = 2.0 * (inputs - self.lb) / (self.ub - self.lb) - 1.0\n",
    "        return self.model(inputs)\n",
    "\n",
    "    def net_u(self, x, t):\n",
    "        return self.forward(x, t)\n",
    "\n",
    "    def net_f(self, x, t):\n",
    "        u = self.net_u(x, t)\n",
    "        u_t = torch.autograd.grad(u, t, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "        u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "        u_xx = torch.autograd.grad(u_x, x, grad_outputs=torch.ones_like(u_x), create_graph=True)[0]\n",
    "\n",
    "        Phi = torch.cat([torch.ones_like(u), u, u_x, u_xx, u ** 2], dim=1)\n",
    "\n",
    "        if self.training:\n",
    "            self.lambda1.data = STRidge(u_t, Phi, delta=1e-3, max_iter=100)\n",
    "\n",
    "        f = u_t + Phi @ self.lambda1\n",
    "        return f, Phi, u_t\n",
    "\n",
    "    def compute_loss(self):\n",
    "        u_pred = self.net_u(self.x, self.t)\n",
    "        loss_u = torch.mean((self.u - u_pred) ** 2)\n",
    "\n",
    "        f_pred, Phi_pred, u_t_pred = self.net_f(self.x_f, self.t_f)\n",
    "        loss_f = torch.mean(f_pred ** 2)\n",
    "\n",
    "        loss_lambda = 1e-7 * torch.norm(self.lambda1, p=1)\n",
    "\n",
    "        return torch.log(loss_u + loss_f + loss_lambda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def STRidge(U_dot, Phi, delta, max_iter=1000, ridge_param=1e-5):\n",
    "    M = Phi.shape[1]\n",
    "    lambda_ = torch.linalg.solve(Phi.T @ Phi + ridge_param * torch.eye(M), Phi.T @ U_dot)\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        small_idx = torch.abs(lambda_) < delta\n",
    "        large_idx = ~small_idx\n",
    "\n",
    "        lambda_[small_idx] = 0.0\n",
    "\n",
    "        if torch.any(large_idx):\n",
    "            Phi_large = Phi[:, large_idx]\n",
    "            lambda_large = torch.linalg.solve(\n",
    "                Phi_large.T @ Phi_large + ridge_param * torch.eye(Phi_large.shape[1]),\n",
    "                Phi_large.T @ U_dot\n",
    "            )\n",
    "            lambda_[large_idx] = lambda_large\n",
    "\n",
    "    return lambda_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_ADO(model, optimizers, epochs, X_u_train, u_train, X_f_train):\n",
    "    history = []\n",
    "    adam_optimizer = optimizers[\"adam\"]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        def closure():\n",
    "            adam_optimizer.zero_grad()\n",
    "            loss = model.compute_loss()\n",
    "            loss.backward()\n",
    "            return loss\n",
    "\n",
    "        adam_optimizer.step(closure)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            f_pred, Phi, u_t_pred = model.net_f(model.x_f, model.t_f)\n",
    "            model.lambda1.data = STRidge(u_t_pred, Phi, delta=1e-3, max_iter=100)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            loss = model.compute_loss()\n",
    "            history.append(loss.item())\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}/{epochs}: Loss={loss.item()}\")\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pde(lambda1, terms):\n",
    "    lambda_values = lambda1.detach().numpy().flatten()\n",
    "    discovered_terms = [\n",
    "        f\"{coef:.6f}*{term}\" for coef, term in zip(lambda_values, terms) if abs(coef) > 1e-5\n",
    "    ]\n",
    "    return \" + \".join(discovered_terms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "One of the differentiated Tensors does not require grad",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m optimizers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m\"\u001b[39m: optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)}\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Train model with ADO and STRidge\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[43mtrain_with_ADO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_u_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_u_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mu_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_f_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_f_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Extract discovered PDE\u001b[39;00m\n\u001b[0;32m     18\u001b[0m terms \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mu_x\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mu_xx\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mu_t\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[1;32mIn[73], line 12\u001b[0m, in \u001b[0;36mtrain_with_ADO\u001b[1;34m(model, optimizers, epochs, X_u_train, u_train, X_f_train)\u001b[0m\n\u001b[0;32m      9\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[1;32m---> 12\u001b[0m \u001b[43madam_optimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     15\u001b[0m     f_pred, Phi, u_t_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mnet_f(model\u001b[38;5;241m.\u001b[39mx_f, model\u001b[38;5;241m.\u001b[39mt_f)\n",
      "File \u001b[1;32mc:\\Users\\chidi\\anaconda3\\envs\\boot_DA\\lib\\site-packages\\torch\\optim\\optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    485\u001b[0m             )\n\u001b[1;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\chidi\\anaconda3\\envs\\boot_DA\\lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\chidi\\anaconda3\\envs\\boot_DA\\lib\\site-packages\\torch\\optim\\adam.py:202\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[1;32m--> 202\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[0;32m    205\u001b[0m     params_with_grad: List[Tensor] \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[73], line 8\u001b[0m, in \u001b[0;36mtrain_with_ADO.<locals>.closure\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclosure\u001b[39m():\n\u001b[0;32m      7\u001b[0m     adam_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m----> 8\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "Cell \u001b[1;32mIn[71], line 62\u001b[0m, in \u001b[0;36mPhysicsInformedNN.compute_loss\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     59\u001b[0m u_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet_u(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt)\n\u001b[0;32m     60\u001b[0m loss_u \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mu \u001b[38;5;241m-\u001b[39m u_pred) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 62\u001b[0m f_pred, Phi_pred, u_t_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet_f\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_f\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt_f\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m loss_f \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(f_pred \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     65\u001b[0m loss_lambda \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-7\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mnorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlambda1, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[71], line 46\u001b[0m, in \u001b[0;36mPhysicsInformedNN.net_f\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnet_f\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[0;32m     45\u001b[0m     u \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet_u(x, t)\n\u001b[1;32m---> 46\u001b[0m     u_t \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     47\u001b[0m     u_x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(u, x, grad_outputs\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mones_like(u), create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     48\u001b[0m     u_xx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(u_x, x, grad_outputs\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mones_like(u_x), create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\chidi\\anaconda3\\envs\\boot_DA\\lib\\site-packages\\torch\\autograd\\__init__.py:496\u001b[0m, in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[0;32m    492\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[0;32m    493\u001b[0m         grad_outputs_\n\u001b[0;32m    494\u001b[0m     )\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m    507\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[0;32m    508\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[0;32m    509\u001b[0m     ):\n",
      "File \u001b[1;32mc:\\Users\\chidi\\anaconda3\\envs\\boot_DA\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: One of the differentiated Tensors does not require grad"
     ]
    }
   ],
   "source": [
    "# Load and prepare data\n",
    "filepath = \"data/burgers.mat\"\n",
    "X_star, u_star, lb, ub, Exact, X, T = load_data(filepath)\n",
    "collocation_points = np.random.rand(1000, 2)\n",
    "X_u_train, u_train, X_u_val, u_val, X_f_train = prepare_data(X_star, u_star, collocation_points)\n",
    "\n",
    "# Initialize model\n",
    "layers = [2, 50, 50, 1]\n",
    "model = PhysicsInformedNN(X_u_train, u_train, X_f_train, X_u_val, u_val, layers, lb, ub)\n",
    "\n",
    "# Initialize optimizers\n",
    "optimizers = {\"adam\": optim.Adam(model.parameters(), lr=1e-3)}\n",
    "\n",
    "# Train model with ADO and STRidge\n",
    "train_with_ADO(model, optimizers, epochs=1000, X_u_train=X_u_train, u_train=u_train, X_f_train=X_f_train)\n",
    "\n",
    "# Extract discovered PDE\n",
    "terms = [\"1\", \"u\", \"u_x\", \"u_xx\", \"u_t\"]\n",
    "discovered_pde = extract_pde(model.lambda1, terms)\n",
    "print(\"Discovered PDE:\", discovered_pde)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(1234)\n",
    "X_u_train = np.random.rand(100, 2)  # Input data (x, t)\n",
    "u_train = np.sin(X_u_train[:, 0:1]) * np.cos(X_u_train[:, 1:2])  # Target data\n",
    "\n",
    "X_f_train = np.random.rand(200, 2)  # Collocation points for physics constraints\n",
    "X_u_val = np.random.rand(50, 2)  # Validation points\n",
    "u_val = np.sin(X_u_val[:, 0:1]) * np.cos(X_u_val[:, 1:2])  # Validation data\n",
    "\n",
    "lb = np.array([0, 0])  # Lower bounds of the domain\n",
    "ub = np.array([1, 1])  # Upper bounds of the domain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class PhysicsInformedNN:\n",
    "    def __init__(self, X, u, X_f, layers, lb, ub):\n",
    "        \"\"\"\n",
    "        Initialize the Physics-Informed Neural Network.\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): Training inputs (space and time).\n",
    "            u (np.ndarray): Training outputs.\n",
    "            X_f (np.ndarray): Collocation points for physics-based constraints.\n",
    "            layers (list): Network architecture.\n",
    "            lb (np.ndarray): Lower bound of the domain.\n",
    "            ub (np.ndarray): Upper bound of the domain.\n",
    "        \"\"\"\n",
    "        self.lb = tf.constant(lb, dtype=tf.float32)\n",
    "        self.ub = tf.constant(ub, dtype=tf.float32)\n",
    "        self.layers = layers\n",
    "\n",
    "        # Build the neural network\n",
    "        self.model = self.build_model(layers)\n",
    "        self.lambda1 = tf.Variable(tf.zeros([16, 1], dtype=tf.float32), name=\"lambda\")\n",
    "\n",
    "        # Store data\n",
    "        self.X = tf.convert_to_tensor(X, dtype=tf.float32)  # Ensure input data is converted to tensors\n",
    "        self.u = tf.convert_to_tensor(u, dtype=tf.float32)  # Convert to Tensor\n",
    "        self.X_f = tf.convert_to_tensor(X_f, dtype=tf.float32)  # Collocation points\n",
    "\n",
    "        # Optimizer\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "        self.loss_history = []\n",
    "\n",
    "    def build_model(self, layers):\n",
    "        \"\"\"\n",
    "        Build a feedforward neural network using Keras layers.\n",
    "        \"\"\"\n",
    "        model = tf.keras.Sequential()\n",
    "        for i in range(len(layers) - 1):\n",
    "            model.add(tf.keras.layers.Dense(\n",
    "                layers[i + 1],\n",
    "                activation='tanh' if i < len(layers) - 2 else None,\n",
    "                kernel_initializer='glorot_normal'\n",
    "            ))\n",
    "        return model\n",
    "\n",
    "    def net_u(self, x, t):\n",
    "        \"\"\"\n",
    "        Compute the neural network output (u) based on spatial and temporal inputs.\n",
    "        \"\"\"\n",
    "        X_scaled = 2.0 * (tf.concat([x, t], axis=1) - self.lb) / (self.ub - self.lb) - 1.0\n",
    "        return self.model(X_scaled)\n",
    "\n",
    "    def net_f(self, x, t):\n",
    "        \"\"\"\n",
    "        Compute the physics-based loss function (f) based on the network output.\n",
    "        \"\"\"\n",
    "        with tf.GradientTape(persistent=True) as tape1:\n",
    "            tape1.watch([x, t])\n",
    "            u = self.net_u(x, t)\n",
    "\n",
    "            # First-order derivatives\n",
    "            u_t = tape1.gradient(u, t)\n",
    "            u_x = tape1.gradient(u, x)\n",
    "\n",
    "        with tf.GradientTape() as tape2:\n",
    "            tape2.watch(x)\n",
    "            u_xx = tape2.gradient(u_x, x)\n",
    "\n",
    "        # Example PDE: u_t - λ₁ * u - λ₂ * u_xx = 0\n",
    "        f = u_t - self.lambda1[0] * u - self.lambda1[1] * u_xx\n",
    "        return f\n",
    "\n",
    "    def compute_loss(self):\n",
    "        \"\"\"\n",
    "        Compute the total loss.\n",
    "        \"\"\"\n",
    "        # Data loss\n",
    "        with tf.GradientTape() as tape:\n",
    "            u_pred = self.net_u(self.X[:, 0:1], self.X[:, 1:2])\n",
    "        loss_u = tf.reduce_mean(tf.square(self.u - u_pred))\n",
    "\n",
    "        # Physics loss\n",
    "        f_pred = self.net_f(self.X_f[:, 0:1], self.X_f[:, 1:2])\n",
    "        loss_f = tf.reduce_mean(tf.square(f_pred))\n",
    "\n",
    "        # Regularization term for lambda\n",
    "        loss_lambda = 1e-7 * tf.reduce_sum(tf.abs(self.lambda1))\n",
    "\n",
    "        return loss_u + loss_f + loss_lambda\n",
    "\n",
    "    def train(self, epochs):\n",
    "        \"\"\"\n",
    "        Train the PINN using a custom training loop.\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss = self.compute_loss()\n",
    "\n",
    "            # Compute gradients\n",
    "            grads = tape.gradient(loss, self.model.trainable_variables + [self.lambda1])\n",
    "\n",
    "            # Apply gradients\n",
    "            self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables + [self.lambda1]))\n",
    "\n",
    "            self.loss_history.append(loss.numpy())\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss.numpy()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def STRidge(U, Phi, delta, max_iter=10):\n",
    "    \"\"\"\n",
    "    Perform Sparse Threshold Ridge Regression.\n",
    "\n",
    "    Args:\n",
    "        U (np.ndarray): Target vector (e.g., time derivatives).\n",
    "        Phi (np.ndarray): Candidate function library.\n",
    "        delta (float): Threshold for sparsity.\n",
    "        max_iter (int): Maximum iterations.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Coefficients of the sparse solution.\n",
    "    \"\"\"\n",
    "    Lambda = np.linalg.lstsq(Phi, U, rcond=None)[0]  # Initial Ridge Regression\n",
    "    for _ in range(max_iter):\n",
    "        small_idx = np.abs(Lambda) < delta\n",
    "        Lambda[small_idx] = 0\n",
    "        large_idx = ~small_idx\n",
    "        if np.sum(large_idx) == 0:\n",
    "            break\n",
    "        Phi_reduced = Phi[:, large_idx]\n",
    "        Lambda[large_idx] = np.linalg.lstsq(Phi_reduced, U, rcond=None)[0]\n",
    "    return Lambda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ADO(pinn, tol, delta, n_iter):\n",
    "    \"\"\"\n",
    "    Alternating Direction Optimization (ADO) framework.\n",
    "\n",
    "    Args:\n",
    "        pinn (PhysicsInformedNN): The PINN model instance.\n",
    "        tol (float): Convergence tolerance.\n",
    "        delta (float): Threshold for STRidge sparsity.\n",
    "        n_iter (int): Number of iterations.\n",
    "    \"\"\"\n",
    "    for iteration in range(n_iter):\n",
    "        # Train the PINN\n",
    "        pinn.train(epochs=100)\n",
    "\n",
    "        # Extract candidate terms\n",
    "        X_f = pinn.X_f.numpy()\n",
    "        derivatives = pinn.compute_derivatives(tf.convert_to_tensor(X_f))\n",
    "        Phi = np.column_stack([derivatives[\"u\"], derivatives[\"u_x\"], derivatives[\"u_xx\"]])\n",
    "        U = derivatives[\"u_t\"].numpy()\n",
    "\n",
    "        # Apply STRidge\n",
    "        Lambda = STRidge(U, Phi, delta)\n",
    "        print(f\"Iteration {iteration}, Lambda: {Lambda}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Initialize and train the model\u001b[39;00m\n\u001b[0;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m PhysicsInformedNN(X_u_train, u_train, X_f_train, layers, lb, ub)\n\u001b[1;32m---> 11\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[18], line 98\u001b[0m, in \u001b[0;36mPhysicsInformedNN.train\u001b[1;34m(self, epochs)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m---> 98\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;66;03m# Compute gradients\u001b[39;00m\n\u001b[0;32m    101\u001b[0m     grads \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrainable_variables \u001b[38;5;241m+\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlambda1])\n",
      "Cell \u001b[1;32mIn[18], line 84\u001b[0m, in \u001b[0;36mPhysicsInformedNN.compute_loss\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     81\u001b[0m loss_u \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_mean(tf\u001b[38;5;241m.\u001b[39msquare(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mu \u001b[38;5;241m-\u001b[39m u_pred))\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# Physics loss\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m f_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet_f\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX_f\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX_f\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m loss_f \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_mean(tf\u001b[38;5;241m.\u001b[39msquare(f_pred))\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# Regularization term for lambda\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[18], line 71\u001b[0m, in \u001b[0;36mPhysicsInformedNN.net_f\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     68\u001b[0m     u_xx \u001b[38;5;241m=\u001b[39m tape2\u001b[38;5;241m.\u001b[39mgradient(u_x, x)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Example PDE: u_t - λ₁ * u - λ₂ * u_xx = 0\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m f \u001b[38;5;241m=\u001b[39m u_t \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlambda1[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m u \u001b[38;5;241m-\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlambda1\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mu_xx\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f\n",
      "File \u001b[1;32mc:\\Users\\chidi\\anaconda3\\envs\\boot_DA\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\chidi\\anaconda3\\envs\\boot_DA\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:108\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    106\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    107\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor."
     ]
    }
   ],
   "source": [
    "# Example data\n",
    "X_u_train = np.random.rand(100, 2)  # Spatial and temporal coordinates\n",
    "u_train = np.sin(X_u_train[:, 0]) * np.cos(X_u_train[:, 1])  # Example u\n",
    "X_f_train = np.random.rand(100, 2)  # Collocation points\n",
    "layers = [2, 20, 20, 1]  # Example network layers\n",
    "lb = np.array([0, 0])  # Lower bound of the domain\n",
    "ub = np.array([1, 1])  # Upper bound of the domain\n",
    "\n",
    "# Initialize and train the model\n",
    "model = PhysicsInformedNN(X_u_train, u_train, X_f_train, layers, lb, ub)\n",
    "model.train(epochs=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Argument `target` should be a list or nested structure of Tensors, Variables or CompositeTensors to be differentiated, but received None.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m PhysicsInformedNN(X_u_train, u_train, X_f_train, layers, lb, ub)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Use ADO and STRidge for PDE discovery\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mADO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 13\u001b[0m, in \u001b[0;36mADO\u001b[1;34m(pinn, tol, delta, n_iter)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mAlternating Direction Optimization (ADO) framework.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m    n_iter (int): Number of iterations.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iteration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_iter):\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# Train the PINN\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m     \u001b[43mpinn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# Extract candidate terms\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     X_f \u001b[38;5;241m=\u001b[39m pinn\u001b[38;5;241m.\u001b[39mX_f\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "Cell \u001b[1;32mIn[11], line 89\u001b[0m, in \u001b[0;36mPhysicsInformedNN.train\u001b[1;34m(self, epochs)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m---> 89\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m     grads \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrainable_variables \u001b[38;5;241m+\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlambda1])\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(grads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrainable_variables \u001b[38;5;241m+\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlambda1]))\n",
      "Cell \u001b[1;32mIn[11], line 73\u001b[0m, in \u001b[0;36mPhysicsInformedNN.compute_loss\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     70\u001b[0m loss_u \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_mean(tf\u001b[38;5;241m.\u001b[39msquare(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mu \u001b[38;5;241m-\u001b[39m u_pred))\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Physics loss\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m derivatives \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_derivatives\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX_f\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m u_t \u001b[38;5;241m=\u001b[39m derivatives[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mu_t\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     75\u001b[0m f \u001b[38;5;241m=\u001b[39m u_t \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlambda1[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m derivatives[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mu\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlambda1[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m derivatives[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mu_xx\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[1;32mIn[11], line 61\u001b[0m, in \u001b[0;36mPhysicsInformedNN.compute_derivatives\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     59\u001b[0m     u_t \u001b[38;5;241m=\u001b[39m tape1\u001b[38;5;241m.\u001b[39mgradient(u, X[:, \u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m2\u001b[39m])\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape2:\n\u001b[1;32m---> 61\u001b[0m     u_xx \u001b[38;5;241m=\u001b[39m \u001b[43mtape2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mu\u001b[39m\u001b[38;5;124m\"\u001b[39m: u, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mu_x\u001b[39m\u001b[38;5;124m\"\u001b[39m: u_x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mu_t\u001b[39m\u001b[38;5;124m\"\u001b[39m: u_t, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mu_xx\u001b[39m\u001b[38;5;124m\"\u001b[39m: u_xx}\n",
      "File \u001b[1;32mc:\\Users\\chidi\\anaconda3\\envs\\boot_DA\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py:1023\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1011\u001b[0m     logging\u001b[38;5;241m.\u001b[39mlog_first_n(\n\u001b[0;32m   1012\u001b[0m         logging\u001b[38;5;241m.\u001b[39mWARN, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling GradientTape.gradient on a persistent \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1013\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtape inside its context is significantly less \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1019\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgradient in order to compute higher order \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1020\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mderivatives.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1023\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `target` should be a list or nested structure\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1024\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m of Tensors, Variables or CompositeTensors to be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1025\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiated, but received None.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1027\u001b[0m flat_targets \u001b[38;5;241m=\u001b[39m composite_tensor_gradient\u001b[38;5;241m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[0;32m   1028\u001b[0m     nest\u001b[38;5;241m.\u001b[39mflatten(target))\n\u001b[0;32m   1029\u001b[0m \u001b[38;5;66;03m# TODO(b/246997907): Remove this once\u001b[39;00m\n\u001b[0;32m   1030\u001b[0m \u001b[38;5;66;03m# ResourceVariableGradient.get_gradient_components returns the handle.\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: Argument `target` should be a list or nested structure of Tensors, Variables or CompositeTensors to be differentiated, but received None."
     ]
    }
   ],
   "source": [
    "# Define the PINN model\n",
    "layers = [2, 64, 64, 1]\n",
    "model = PhysicsInformedNN(X_u_train, u_train, X_f_train, layers, lb, ub)\n",
    "\n",
    "# Use ADO and STRidge for PDE discovery\n",
    "ADO(model, tol=1e-3, delta=1e-4, n_iter=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "from scipy.interpolate import griddata\n",
    "from scipy.spatial import distance\n",
    "from matplotlib import cm\n",
    "import time\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from pyDOE import lhs  # Latin Hypercube Sampling for collocation points\n",
    "import sobol_seq  # Sobol sequence generator for collocation points\n",
    "import os\n",
    "\n",
    "# Set device to CPU explicitly\n",
    "with tf.device('/CPU:0'):\n",
    "    pass  # Replace with computations later if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "#  Define loss histories to record convergence\n",
    "# =============================================================================\n",
    "\n",
    "# Adam loss history\n",
    "loss_history_Adam = np.array([0])\n",
    "loss_u_history_Adam = np.array([0])\n",
    "loss_f_history_Adam = np.array([0])\n",
    "loss_lambda_history_Adam = np.array([0])\n",
    "lambda_history_Adam = np.zeros((16, 1))\n",
    "loss_history_Adam_val = np.array([0])\n",
    "loss_u_history_Adam_val = np.array([0])\n",
    "loss_f_history_Adam_val = np.array([0])\n",
    "\n",
    "# STRidge loss history\n",
    "loss_history_STRidge = np.array([0])\n",
    "loss_f_history_STRidge = np.array([0])\n",
    "loss_lambda_history_STRidge = np.array([0])\n",
    "optimaltol_history = np.array([0])\n",
    "tol_history_STRidge = np.array([0])\n",
    "lambda_normalized_history_STRidge = np.zeros((16, 1))\n",
    "\n",
    "lambda_history_STRidge = np.zeros((16, 1))\n",
    "ridge_append_counter_STRidge = np.array([0])\n",
    "\n",
    "# Loss histories for pretraining\n",
    "loss_history_Pretrain = np.array([0])\n",
    "loss_u_history_Pretrain = np.array([0])\n",
    "loss_f_history_Pretrain = np.array([0])\n",
    "loss_lambda_history_Pretrain = np.array([0])\n",
    "loss_history_val_Pretrain = np.array([0])\n",
    "loss_u_history_val_Pretrain = np.array([0])\n",
    "loss_f_history_val_Pretrain = np.array([0])\n",
    "step_Pretrain = 0\n",
    "\n",
    "lambda_history_Pretrain = np.zeros((16, 1))\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(1234)\n",
    "tf.random.set_seed(1234)  # Updated for TensorFlow 2.x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import os\n",
    "from scipy.interpolate import griddata\n",
    "from scipy.spatial import distance\n",
    "import time\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from pyDOE import lhs\n",
    "\n",
    "class PhysicsInformedNN:\n",
    "    def __init__(self, X, u, X_f, X_val, u_val, layers, lb, ub):\n",
    "        # Store domain bounds\n",
    "        self.lb = lb\n",
    "        self.ub = ub\n",
    "        self.layers = layers\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.weights, self.biases = self.initialize_NN(layers)\n",
    "\n",
    "        # Eager execution\n",
    "        tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "        # Initialize model parameters\n",
    "        self.lambda1 = tf.Variable(tf.zeros([16, 1], dtype=tf.float32), dtype=tf.float32, name='lambda')\n",
    "\n",
    "\n",
    "        # Training data\n",
    "        self.x = X[:, 0:1]\n",
    "        self.t = X[:, 1:2]\n",
    "        self.u = u\n",
    "        self.x_f = X_f[:, 0:1]\n",
    "        self.t_f = X_f[:, 1:2]\n",
    "\n",
    "        # Use TensorFlow Variables for inputs and labels\n",
    "        self.x_tf = tf.Variable(self.x)\n",
    "        self.t_tf = tf.Variable(self.t)\n",
    "        self.u_tf = tf.Variable(self.u)\n",
    "        self.x_f_tf = tf.Variable(self.x_f)\n",
    "        self.t_f_tf = tf.Variable(self.t_f)\n",
    "\n",
    "        self.u_pred = self.net_u(self.x_tf, self.t_tf)\n",
    "        self.f_pred, self.Phi_pred, self.u_t_pred = self.net_f(self.x_f_tf, self.t_f_tf, self.x_f.shape[0])\n",
    "\n",
    "        # Losses\n",
    "        self.loss_u = tf.reduce_mean(tf.square(self.u_tf - self.u_pred))\n",
    "        self.loss_f_coeff_tf = tf.Variable(1.0, dtype=tf.float32)\n",
    "        self.loss_f = self.loss_f_coeff_tf * tf.reduce_mean(tf.square(self.f_pred))\n",
    "\n",
    "        self.loss_lambda = 1e-7 * tf.norm(self.lambda1, ord=1)\n",
    "        self.loss = tf.math.log(self.loss_u + self.loss_f + self.loss_lambda)  # log loss\n",
    "\n",
    "        # Validation data\n",
    "        self.x_val = X_val[:, 0:1]\n",
    "        self.t_val = X_val[:, 1:2]\n",
    "        self.u_val = u_val\n",
    "        self.x_val_tf = tf.Variable(self.x_val)\n",
    "        self.t_val_tf = tf.Variable(self.t_val)\n",
    "        self.u_val_tf = tf.Variable(self.u_val)\n",
    "\n",
    "        self.u_val_pred = self.net_u(self.x_val_tf, self.t_val_tf)\n",
    "        self.f_val_pred, _, _ = self.net_f(self.x_val_tf, self.t_val_tf, self.x_val.shape[0])\n",
    "\n",
    "        self.loss_u_val = tf.reduce_mean(tf.square(self.u_val_tf - self.u_val_pred))\n",
    "        self.loss_f_val = tf.reduce_mean(tf.square(self.f_val_pred))\n",
    "        self.loss_val = tf.math.log(self.loss_u_val + self.loss_f_val)  # log loss\n",
    "\n",
    "        # Optimizer\n",
    "        self.optimizer = tf.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "    def initialize_NN(self, layers):\n",
    "        # Initialize neural network weights and biases\n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(layers)\n",
    "        for l in range(num_layers - 1):\n",
    "            W = self.xavier_init(size=[layers[l], layers[l+1]])\n",
    "            b = tf.Variable(tf.zeros([1, layers[l+1]], dtype=tf.float32), dtype=tf.float32, name='b')\n",
    "            weights.append(W)\n",
    "            biases.append(b)\n",
    "        return weights, biases\n",
    "\n",
    "    def xavier_init(self, size):\n",
    "        # Xavier initialization\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]\n",
    "        xavier_stddev = np.sqrt(2 / (in_dim + out_dim))\n",
    "        return tf.Variable(tf.random.normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32, name='W')\n",
    "\n",
    "    def neural_net(self, X, weights, biases):\n",
    "        # Feedforward neural network\n",
    "        num_layers = len(weights) + 1\n",
    "        H = 2.0 * (X - self.lb) / (self.ub - self.lb) - 1.0\n",
    "        H = tf.cast(H, dtype=tf.float32)  # Ensure H is float32\n",
    "\n",
    "        for l in range(num_layers - 2):\n",
    "            W = weights[l]\n",
    "            b = biases[l]\n",
    "            W = tf.cast(W, dtype=tf.float32)  # Ensure W is float32\n",
    "            b = tf.cast(b, dtype=tf.float32)  # Ensure b is float32\n",
    "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "\n",
    "        W = weights[-1]\n",
    "        b = biases[-1]\n",
    "        W = tf.cast(W, dtype=tf.float32)  # Ensure W is float32\n",
    "        b = tf.cast(b, dtype=tf.float32)  # Ensure b is float32\n",
    "        Y = tf.add(tf.matmul(H, W), b)\n",
    "        return Y\n",
    "\n",
    "\n",
    "    def net_u(self, x, t):\n",
    "        # u(x,t) prediction\n",
    "        u = self.neural_net(tf.concat([x, t], axis=1), self.weights, self.biases)\n",
    "        return u\n",
    "\n",
    "    def net_f(self, x, t, N_f):\n",
    "        # Physics-based loss computation\n",
    "        u = self.net_u(x, t)  # Compute network output\n",
    "\n",
    "        # Use GradientTape to compute derivatives\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            tape.watch(x)\n",
    "            tape.watch(t)\n",
    "            u = self.net_u(x, t)  # Ensure forward pass is recorded\n",
    "\n",
    "        # First derivative w.r.t t\n",
    "        u_t = tape.gradient(u, t)\n",
    "\n",
    "        # First derivative w.r.t x\n",
    "        u_x = tape.gradient(u, x)\n",
    "\n",
    "        # Second derivative w.r.t x\n",
    "        u_xx = tape.gradient(u_x, x)\n",
    "\n",
    "        # Third derivative w.r.t x\n",
    "        u_xxx = tape.gradient(u_xx, x)\n",
    "\n",
    "        # Check if any derivative is None\n",
    "        if u_x is None or u_xx is None or u_xxx is None:\n",
    "            raise ValueError(\"One of the derivatives (u_x, u_xx, u_xxx) is None. Ensure the forward pass is correctly defined.\")\n",
    "\n",
    "        # Construct the candidate library matrix (Phi)\n",
    "        Phi = tf.concat([tf.ones((N_f, 1), dtype=tf.float32), u, u**2, u**3, u_x, u*u_x, u**2*u_x,\n",
    "                        u**3*u_x, u_xx, u*u_xx, u**2*u_xx, u**3*u_xx, u_xxx, u*u_xxx, u**2*u_xxx, u**3*u_xxx], axis=1)\n",
    "\n",
    "        # PDE residual: Using the library matrix and the gradient of u\n",
    "        f = tf.matmul(Phi, self.lambda1) - u_t  # PDE residual\n",
    "        \n",
    "        return f, Phi, u_t\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def lbfgs_optimizer(self, loss_fn, var_list):\n",
    "        \"\"\"\n",
    "        Performs L-BFGS-B optimization using SciPy for training the PINN.\n",
    "        \"\"\"\n",
    "        def get_params():\n",
    "            params = []\n",
    "            for var in var_list:\n",
    "                params.append(var.numpy().flatten())\n",
    "            return np.concatenate(params)\n",
    "\n",
    "        def loss_and_grads(params):\n",
    "            offset = 0\n",
    "            for var in var_list:\n",
    "                param_size = np.prod(var.shape)\n",
    "                var.assign(tf.convert_to_tensor(params[offset:offset + param_size].reshape(var.shape), dtype=tf.float32))\n",
    "                offset += param_size\n",
    "            with tf.GradientTape() as tape:\n",
    "                tape.watch(var_list)\n",
    "                loss_value = loss_fn()\n",
    "            grads = tape.gradient(loss_value, var_list)\n",
    "            grad_params = np.concatenate([grad.numpy().flatten() for grad in grads])\n",
    "            return loss_value.numpy().astype(np.float64), grad_params\n",
    "\n",
    "        initial_params = get_params()\n",
    "        result = opt.minimize(loss_and_grads, initial_params, jac=True, method='L-BFGS-B', \n",
    "                              options={'maxiter': 1000, 'maxfun': 1000, 'maxcor': 50, 'maxls': 50, \n",
    "                                       'ftol': 1e-10, 'disp': True})\n",
    "        return result\n",
    "\n",
    "    def train_with_lbfgs(self, epochs=1000):\n",
    "        \"\"\"\n",
    "        Train the Physics-Informed Neural Network using L-BFGS-B optimization.\n",
    "        \"\"\"\n",
    "        def loss_fn():\n",
    "            self.u_pred = self.net_u(self.x_tf, self.t_tf)\n",
    "            self.f_pred, _, _ = self.net_f(self.x_f_tf, self.t_f_tf, self.x_f.shape[0])\n",
    "            loss_u = tf.reduce_mean(tf.square(self.u_tf - self.u_pred))\n",
    "            loss_f = tf.reduce_mean(tf.square(self.f_pred))\n",
    "            loss_lambda = 1e-7 * tf.norm(self.lambda1, ord=1)\n",
    "            return tf.math.log(loss_u + loss_f + loss_lambda)\n",
    "        \n",
    "        self.lbfgs_result = self.lbfgs_optimizer(loss_fn, self.weights + self.biases + [self.lambda1])\n",
    "\n",
    "    def adam_optimizer(self, loss_fn, var_list):\n",
    "        \"\"\"\n",
    "        Adam optimizer used for training the PINN with the loss function.\n",
    "        \"\"\"\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(var_list)\n",
    "            loss_value = loss_fn()\n",
    "\n",
    "        grads = tape.gradient(loss_value, var_list)\n",
    "        optimizer.apply_gradients(zip(grads, var_list))\n",
    "        return loss_value\n",
    "\n",
    "    def train(self, epochs=1000):\n",
    "        \"\"\"\n",
    "        Train the model using both Adam and L-BFGS optimizers.\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            loss_value = self.adam_optimizer(self.loss_fn, self.weights + self.biases + [self.lambda1])\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss_value.numpy()}\")\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                self.train_with_lbfgs()\n",
    "        \n",
    "    def loss_fn(self):\n",
    "        \"\"\"\n",
    "        Compute the loss for the training process.\n",
    "        \"\"\"\n",
    "        self.u_pred = self.net_u(self.x_tf, self.t_tf)\n",
    "        self.f_pred, _, _ = self.net_f(self.x_f_tf, self.t_f_tf, self.x_f.shape[0])\n",
    "        loss_u = tf.reduce_mean(tf.square(self.u_tf - self.u_pred))\n",
    "        loss_f = tf.reduce_mean(tf.square(self.f_pred))\n",
    "        loss_lambda = 1e-7 * tf.norm(self.lambda1, ord=1)\n",
    "        return tf.math.log(loss_u + loss_f + loss_lambda)\n",
    "    \n",
    "    # Callback function for Pretraining\n",
    "    def callback_Pretrain(self, loss, loss_u, loss_f, loss_lambda, loss_val, loss_u_val, loss_f_val, lamu):\n",
    "        global step_Pretrain\n",
    "        step_Pretrain += 1\n",
    "        if step_Pretrain % 10 == 0:\n",
    "            print('Step: %d, log Loss: %e, loss_u: %e, loss_f: %e, loss_lambda: %e' % (step_Pretrain, loss, loss_u, loss_f,\n",
    "                                                                                   loss_lambda))\n",
    "            \n",
    "            # Update loss history\n",
    "            global loss_history_Pretrain\n",
    "            global loss_u_history_Pretrain\n",
    "            global loss_f_history_Pretrain\n",
    "            global loss_lambda_history_Pretrain\n",
    "            \n",
    "            global loss_history_val_Pretrain\n",
    "            global loss_u_history_val_Pretrain\n",
    "            global loss_f_history_val_Pretrain\n",
    "            \n",
    "            global lambda_history_Pretrain\n",
    "            \n",
    "            loss_history_Pretrain = np.append(loss_history_Pretrain, loss)\n",
    "            loss_u_history_Pretrain = np.append(loss_u_history_Pretrain, loss_u)\n",
    "            loss_f_history_Pretrain = np.append(loss_f_history_Pretrain, loss_f)\n",
    "            loss_lambda_history_Pretrain = np.append(loss_lambda_history_Pretrain, loss_lambda)\n",
    "            \n",
    "            loss_history_val_Pretrain = np.append(loss_history_val_Pretrain, loss_val)\n",
    "            loss_u_history_val_Pretrain = np.append(loss_u_history_val_Pretrain, loss_u_val)\n",
    "            loss_f_history_val_Pretrain = np.append(loss_f_history_val_Pretrain, loss_f_val)\n",
    "            \n",
    "            # Update lambda history\n",
    "            lambda_history_Pretrain = np.append(lambda_history_Pretrain, lamu, axis=1)\n",
    "\n",
    "    def train(self, nIter):  # nIter is the number of ADO loop\n",
    "        self.tf_dict = {\n",
    "            self.x_tf: self.x, \n",
    "            self.t_tf: self.t, \n",
    "            self.u_tf: self.u, \n",
    "            self.x_f_tf: self.x_f, \n",
    "            self.t_f_tf: self.t_f,\n",
    "            self.x_val_tf: self.x_val, \n",
    "            self.t_val_tf: self.t_val, \n",
    "            self.u_val_tf: self.u_val,\n",
    "            self.loss_f_coeff_tf: 1\n",
    "        }\n",
    "\n",
    "        # Pretraining, as a form of a good initialization\n",
    "        print('L-BFGS-B pretraining begins')\n",
    "        self.optimizer_Pretrain.minimize(\n",
    "            self.loss,\n",
    "            var_list=self.model.trainable_variables,\n",
    "            feed_dict=self.tf_dict,\n",
    "            loss_callback=self.callback_Pretrain\n",
    "        )\n",
    "\n",
    "        self.tf_dict[self.loss_f_coeff_tf] = 2\n",
    "        for self.it in range(nIter):\n",
    "            \n",
    "            # Loop of STRidge optimization\n",
    "            print('STRidge begins')\n",
    "            self.callTrainSTRidge()\n",
    "\n",
    "            # Loop of Adam optimization\n",
    "            print('Adam begins')\n",
    "            start_time = time.time()\n",
    "            for it_Adam in range(1000):\n",
    "\n",
    "                # Run the Adam optimization step\n",
    "                self.optimizer_Adam.minimize(self.loss, var_list=self.model.trainable_variables)\n",
    "\n",
    "                # Print every 10 steps\n",
    "                if it_Adam % 10 == 0:\n",
    "                    elapsed = time.time() - start_time\n",
    "                    loss, loss_u, loss_f, loss_lambda, lambda1_value, loss_val, loss_u_val, loss_f_val = self.sess.run([\n",
    "                        self.loss, self.loss_u, self.loss_f, self.loss_lambda, self.lambda1, self.loss_val, \n",
    "                        self.loss_u_val, self.loss_f_val\n",
    "                    ], self.tf_dict)\n",
    "                    print('It: %d, Log Loss: %.3e, loss_u: %e, loss_f: %e, loss_lambda: %e, Time: %.2f' \n",
    "                        % (it_Adam, loss, loss_u, loss_f, loss_lambda, elapsed))\n",
    "\n",
    "                    lamu = self.sess.run(self.lambda1)\n",
    "\n",
    "                    global loss_history_Adam\n",
    "                    global lambda_history_Adam\n",
    "                    global loss_u_history_Adam\n",
    "                    global loss_f_history_Adam\n",
    "                    global loss_lambda_history_Adam\n",
    "                    \n",
    "                    global loss_history_Adam_val\n",
    "                    global loss_u_history_Adam_val\n",
    "                    global loss_f_history_Adam_val\n",
    "\n",
    "                    # Record the losses and parameters\n",
    "                    loss_history_Adam = np.append(loss_history_Adam, loss)\n",
    "                    lambda_history_Adam = np.append(lambda_history_Adam, lambda1_value, axis=1)\n",
    "                    loss_u_history_Adam = np.append(loss_u_history_Adam, loss_u)\n",
    "                    loss_f_history_Adam = np.append(loss_f_history_Adam, loss_f)\n",
    "                    loss_lambda_history_Adam = np.append(loss_lambda_history_Adam, loss_lambda)\n",
    "\n",
    "                    loss_history_Adam_val = np.append(loss_history_Adam_val, loss_val)\n",
    "                    loss_u_history_Adam_val = np.append(loss_u_history_Adam_val, loss_u_val)\n",
    "                    loss_f_history_Adam_val = np.append(loss_f_history_Adam_val, loss_f_val)\n",
    "\n",
    "                    lambda_history_Adam = np.append(lambda_history_Adam, lamu, axis=1)\n",
    "\n",
    "                    start_time = time.time()\n",
    "\n",
    "            # Loop of L-BFGS-B optimization\n",
    "            # print('L-BFGS-B begins')\n",
    "            # self.optimizer.minimize(self.sess,\n",
    "            #                         feed_dict = self.tf_dict,\n",
    "            #                         fetches = [self.loss, self.loss_u, self.loss_f, self.loss_lambda,\n",
    "            #                                    self.loss_val, self.loss_u_val, self.loss_f_val],\n",
    "            #                         loss_callback = self.callback)\n",
    "            \n",
    "        # One more time of STRidge optimization\n",
    "        print('STRidge begins')\n",
    "        self.callTrainSTRidge()\n",
    "\n",
    "    \n",
    "    def predict(self, X_star):\n",
    "        tf_dict = {self.x_tf: X_star[:, 0:1], self.t_tf: X_star[:, 1:2]}\n",
    "        u_star = self.sess.run(self.u_pred, tf_dict)\n",
    "        return u_star\n",
    "\n",
    "    def callTrainSTRidge(self):\n",
    "        lam = 1e-5\n",
    "        d_tol = 1\n",
    "        maxit = 100\n",
    "        STR_iters = 10\n",
    "\n",
    "        l0_penalty = None\n",
    "\n",
    "        normalize = 2\n",
    "        split = 0.8\n",
    "        print_best_tol = False\n",
    "        Phi_pred, u_t_pred = self.sess.run([self.Phi_pred, self.u_t_pred], self.tf_dict)\n",
    "\n",
    "        lambda2 = self.TrainSTRidge(Phi_pred, u_t_pred, lam, d_tol, maxit, STR_iters, l0_penalty, normalize, split,\n",
    "                                     print_best_tol)\n",
    "\n",
    "        self.lambda1.assign(tf.convert_to_tensor(lambda2, dtype=tf.float32))\n",
    "\n",
    "    def TrainSTRidge(self, R0, Ut, lam, d_tol, maxit, STR_iters=10, l0_penalty=None, normalize=2, split=0.8,\n",
    "                     print_best_tol=False):\n",
    "        # Normalization logic as in the original code\n",
    "        n, d = R0.shape\n",
    "        R = np.zeros((n, d), dtype=np.float32)\n",
    "        if normalize != 0:\n",
    "            Mreg = np.zeros((d, 1))\n",
    "            for i in range(0, d):\n",
    "                Mreg[i] = 1.0 / (np.linalg.norm(R0[:, i], normalize))\n",
    "                R[:, i] = Mreg[i] * R0[:, i]\n",
    "            normalize_inner = 0\n",
    "        else:\n",
    "            R = R0\n",
    "            Mreg = np.ones((d, 1)) * d\n",
    "            normalize_inner = 2\n",
    "\n",
    "        # Split data\n",
    "        np.random.seed(0)\n",
    "        n, _ = R.shape\n",
    "        train = np.random.choice(n, int(n * split), replace=False)\n",
    "        test = [i for i in np.arange(n) if i not in train]\n",
    "        TrainR = R[train, :]\n",
    "        TestR = R[test, :]\n",
    "        TrainY = Ut[train, :]\n",
    "        TestY = Ut[test, :]\n",
    "\n",
    "        # Setup tolerance\n",
    "        d_tol = float(d_tol)\n",
    "        if self.it == 0:\n",
    "            self.tol = d_tol\n",
    "\n",
    "        # Or inherit Lambda\n",
    "        w_best = self.sess.run(self.lambda1) / Mreg\n",
    "        err_f = np.mean((TestY - TestR.dot(w_best)) ** 2)\n",
    "\n",
    "        if l0_penalty is None and self.it == 0:\n",
    "            self.l0_penalty_0 = err_f\n",
    "            l0_penalty = self.l0_penalty_0\n",
    "        elif l0_penalty is None:\n",
    "            l0_penalty = self.l0_penalty_0\n",
    "\n",
    "        err_lambda = l0_penalty * np.count_nonzero(w_best)\n",
    "        err_best = err_f + err_lambda\n",
    "        tol_best = 0\n",
    "\n",
    "        # Update loss history\n",
    "        loss_history_STRidge = np.append(loss_history_STRidge, err_best)\n",
    "        loss_f_history_STRidge = np.append(loss_f_history_STRidge, err_f)\n",
    "        loss_lambda_history_STRidge = np.append(loss_lambda_history_STRidge, err_lambda)\n",
    "        tol_history_STRidge = np.append(tol_history_STRidge, tol_best)\n",
    "\n",
    "        # Increase tolerance until test performance decreases\n",
    "        for iter in range(maxit):\n",
    "            # Get coefficients and error\n",
    "            w = self.STRidge(TrainR, TrainY, lam, STR_iters, self.tol, Mreg, normalize=normalize_inner)\n",
    "\n",
    "            err_f = np.mean((TestY - TestR.dot(w)) ** 2)\n",
    "            err_lambda = l0_penalty * np.count_nonzero(w)\n",
    "            err = err_f + err_lambda\n",
    "\n",
    "            # If accuracy improves, update\n",
    "            if err <= err_best:\n",
    "                err_best = err\n",
    "                w_best = w\n",
    "                tol_best = self.tol\n",
    "                self.tol = self.tol + d_tol\n",
    "\n",
    "                loss_history_STRidge = np.append(loss_history_STRidge, err_best)\n",
    "                loss_f_history_STRidge = np.append(loss_f_history_STRidge, err_f)\n",
    "                loss_lambda_history_STRidge = np.append(loss_lambda_history_STRidge, err_lambda)\n",
    "                tol_history_STRidge = np.append(tol_history_STRidge, tol_best)\n",
    "            else:\n",
    "                self.tol = max([0, self.tol - 2 * d_tol])\n",
    "                d_tol = d_tol / 1.618\n",
    "                self.tol = self.tol + d_tol\n",
    "\n",
    "        if print_best_tol: print(\"Optimal tolerance:\", tol_best)\n",
    "\n",
    "        optimaltol_history = np.append(optimaltol_history, tol_best)\n",
    "\n",
    "        return np.real(np.multiply(Mreg, w_best))\n",
    "    \n",
    "\n",
    "    def STRidge(self, X0, y, lam, maxit, tol, Mreg, normalize=2, print_results=False):\n",
    "        n, d = X0.shape\n",
    "        X = np.zeros((n, d), dtype=np.complex64)\n",
    "        \n",
    "        # First normalize data\n",
    "        if normalize != 0:\n",
    "            Mreg = np.zeros((d, 1))\n",
    "            for i in range(d):\n",
    "                Mreg[i] = 1.0 / (np.linalg.norm(X0[:, i], normalize))\n",
    "                X[:, i] = Mreg[i] * X0[:, i]                \n",
    "        else: \n",
    "            X = X0\n",
    "        \n",
    "        # Inherit lambda\n",
    "        w = self.lambda1.numpy() / Mreg  # TensorFlow 2.x, use .numpy() to extract values from tensors\n",
    "\n",
    "        biginds = np.where(abs(w) > tol)[0]\n",
    "        num_relevant = d            \n",
    "        \n",
    "        # Global variable initialization\n",
    "        global ridge_append_counter_STRidge\n",
    "        ridge_append_counter = 0\n",
    "        \n",
    "        global lambda_history_STRidge\n",
    "        lambda_history_STRidge = np.append(lambda_history_STRidge, np.multiply(Mreg, w), axis=1)\n",
    "        ridge_append_counter += 1\n",
    "        \n",
    "        # Threshold and continue\n",
    "        for j in range(maxit):\n",
    "            # Figure out which items to cut out\n",
    "            smallinds = np.where(abs(w) < tol)[0]\n",
    "            new_biginds = [i for i in range(d) if i not in smallinds]\n",
    "                    \n",
    "            # If nothing changes then stop\n",
    "            if num_relevant == len(new_biginds): break\n",
    "            else: num_relevant = len(new_biginds)\n",
    "                    \n",
    "            if len(new_biginds) == 0:\n",
    "                if j == 0: \n",
    "                    if normalize != 0:\n",
    "                        lambda_history_STRidge = np.append(lambda_history_STRidge, w * Mreg, axis=1)\n",
    "                        ridge_append_counter += 1\n",
    "                        ridge_append_counter_STRidge = np.append(ridge_append_counter_STRidge, ridge_append_counter)\n",
    "                        return np.multiply(Mreg, w)\n",
    "                    else:\n",
    "                        lambda_history_STRidge = np.append(lambda_history_STRidge, w * Mreg, axis=1)\n",
    "                        ridge_append_counter += 1\n",
    "                        ridge_append_counter_STRidge = np.append(ridge_append_counter_STRidge, ridge_append_counter)\n",
    "                        return w\n",
    "                else: break\n",
    "            biginds = new_biginds\n",
    "            \n",
    "            # Otherwise get a new guess\n",
    "            w[smallinds] = 0\n",
    "            \n",
    "            if lam != 0: \n",
    "                # Solve using least squares with regularization\n",
    "                X_biginds = X[:, biginds]\n",
    "                w[biginds] = np.linalg.lstsq(X_biginds.T.dot(X_biginds) + lam * np.eye(len(biginds)), X_biginds.T.dot(y))[0]\n",
    "                lambda_history_STRidge = np.append(lambda_history_STRidge, np.multiply(Mreg, w), axis=1)\n",
    "                ridge_append_counter += 1\n",
    "            else: \n",
    "                # Solve without regularization\n",
    "                w[biginds] = np.linalg.lstsq(X[:, biginds], y)[0]\n",
    "                lambda_history_STRidge = np.append(lambda_history_STRidge, np.multiply(Mreg, w), axis=1)\n",
    "                ridge_append_counter += 1\n",
    "\n",
    "        # Now that we have the sparsity pattern, use standard least squares to get w\n",
    "        if biginds != []:\n",
    "            w[biginds] = np.linalg.lstsq(X[:, biginds], y)[0]\n",
    "            \n",
    "        if normalize != 0:\n",
    "            lambda_history_STRidge = np.append(lambda_history_STRidge, w * Mreg, axis=1)\n",
    "            ridge_append_counter += 1\n",
    "            ridge_append_counter_STRidge = np.append(ridge_append_counter_STRidge, ridge_append_counter)\n",
    "            return np.multiply(Mreg, w)\n",
    "        else:\n",
    "            lambda_history_STRidge = np.append(lambda_history_STRidge, w * Mreg, axis=1)\n",
    "            ridge_append_counter += 1\n",
    "            ridge_append_counter_STRidge = np.append(ridge_append_counter_STRidge, ridge_append_counter)\n",
    "            return w\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Argument `target` should be a list or nested structure of Tensors, Variables or CompositeTensors to be differentiated, but received None.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 70\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;124;03m# Option: Add noise\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03mnoise = 0.1\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;124;03mu_train = u_train + noise * np.std(u_train) * np.random.randn(u_train.shape[0], u_train.shape[1])\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03mu_val = u_val + noise * np.std(u_val) * np.random.randn(u_val.shape[0], u_val.shape[1])\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Model initialization\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mPhysicsInformedNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_u_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_f_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_u_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mub\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;241m6\u001b[39m)\n",
      "Cell \u001b[1;32mIn[52], line 43\u001b[0m, in \u001b[0;36mPhysicsInformedNN.__init__\u001b[1;34m(self, X, u, X_f, X_val, u_val, layers, lb, ub)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_f_tf \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mVariable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_f)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mu_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet_u(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_tf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_tf)\n\u001b[1;32m---> 43\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_pred, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mPhi_pred, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mu_t_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet_f\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_f_tf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt_f_tf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Losses\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_u \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_mean(tf\u001b[38;5;241m.\u001b[39msquare(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mu_tf \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mu_pred))\n",
      "Cell \u001b[1;32mIn[52], line 136\u001b[0m, in \u001b[0;36mPhysicsInformedNN.net_f\u001b[1;34m(self, x, t, N_f)\u001b[0m\n\u001b[0;32m    133\u001b[0m u_xx \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(u_x, x)\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# Third derivative w.r.t x\u001b[39;00m\n\u001b[1;32m--> 136\u001b[0m u_xxx \u001b[38;5;241m=\u001b[39m \u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu_xx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# Check if any derivative is None\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m u_x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m u_xx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m u_xxx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\chidi\\anaconda3\\envs\\boot_DA\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py:1023\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1011\u001b[0m     logging\u001b[38;5;241m.\u001b[39mlog_first_n(\n\u001b[0;32m   1012\u001b[0m         logging\u001b[38;5;241m.\u001b[39mWARN, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling GradientTape.gradient on a persistent \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1013\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtape inside its context is significantly less \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1019\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgradient in order to compute higher order \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1020\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mderivatives.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1023\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `target` should be a list or nested structure\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1024\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m of Tensors, Variables or CompositeTensors to be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1025\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiated, but received None.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1027\u001b[0m flat_targets \u001b[38;5;241m=\u001b[39m composite_tensor_gradient\u001b[38;5;241m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[0;32m   1028\u001b[0m     nest\u001b[38;5;241m.\u001b[39mflatten(target))\n\u001b[0;32m   1029\u001b[0m \u001b[38;5;66;03m# TODO(b/246997907): Remove this once\u001b[39;00m\n\u001b[0;32m   1030\u001b[0m \u001b[38;5;66;03m# ResourceVariableGradient.get_gradient_components returns the handle.\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: Argument `target` should be a list or nested structure of Tensors, Variables or CompositeTensors to be differentiated, but received None."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import os\n",
    "from scipy.interpolate import griddata\n",
    "from scipy.spatial import distance\n",
    "import time\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from pyDOE import lhs\n",
    "\n",
    "# Define layers for the neural network\n",
    "layers = [2, 20, 20, 20, 20, 20, 20, 20, 20, 1]\n",
    "\n",
    "# Load the data\n",
    "data = scipy.io.loadmat(\"C:/Users/chidi/Downloads/burgers.mat\")\n",
    "\n",
    "t = np.real(data['t'].flatten()[:, None])\n",
    "x = np.real(data['x'].flatten()[:, None])\n",
    "Exact = np.real(data['usol']).T\n",
    "\n",
    "X, T = np.meshgrid(x, t)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:, None], T.flatten()[:, None]))\n",
    "u_star = Exact.flatten()[:, None]\n",
    "\n",
    "# Domain bounds\n",
    "lb = X_star.min(0)\n",
    "ub = X_star.max(0)\n",
    "\n",
    "# Measurement data\n",
    "N_u_s = 10\n",
    "idx_s = np.random.choice(x.shape[0], N_u_s, replace=False)\n",
    "X0 = X[:, idx_s]\n",
    "T0 = T[:, idx_s]\n",
    "Exact0 = Exact[:, idx_s]\n",
    "\n",
    "N_u_t = int(t.shape[0] * 1)\n",
    "idx_t = np.random.choice(t.shape[0], N_u_t, replace=False)\n",
    "X0 = X0[idx_t, :]\n",
    "T0 = T0[idx_t, :]\n",
    "Exact0 = Exact0[idx_t, :]\n",
    "\n",
    "X_u_meas = np.hstack((X0.flatten()[:, None], T0.flatten()[:, None]))\n",
    "u_meas = Exact0.flatten()[:, None]\n",
    "\n",
    "# Training measurements, randomly sampled spatio-temporally\n",
    "Split_TrainVal = 0.8\n",
    "N_u_train = int(X_u_meas.shape[0] * Split_TrainVal)\n",
    "idx_train = np.random.choice(X_u_meas.shape[0], N_u_train, replace=False)\n",
    "X_u_train = X_u_meas[idx_train, :]\n",
    "u_train = u_meas[idx_train, :]\n",
    "\n",
    "# Validation Measurements\n",
    "idx_val = np.setdiff1d(np.arange(X_u_meas.shape[0]), idx_train, assume_unique=True)\n",
    "X_u_val = X_u_meas[idx_val, :]\n",
    "u_val = u_meas[idx_val, :]\n",
    "\n",
    "# Collocation points\n",
    "N_f = 50000\n",
    "X_f_train = lb + (ub - lb) * lhs(2, N_f)\n",
    "X_f_train = np.vstack((X_f_train, X_u_train))\n",
    "\n",
    "\"\"\"\n",
    "# Option: Add noise\n",
    "noise = 0.1\n",
    "u_train = u_train + noise * np.std(u_train) * np.random.randn(u_train.shape[0], u_train.shape[1])\n",
    "u_val = u_val + noise * np.std(u_val) * np.random.randn(u_val.shape[0], u_val.shape[1])\n",
    "\"\"\"\n",
    "# Model initialization\n",
    "model = PhysicsInformedNN(X_u_train, u_train, X_f_train, X_u_val, u_val, layers, lb, ub)\n",
    "model.train(6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\chidi\\AppData\\Local\\Temp\\ipykernel_5856\\4053053540.py:52: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'u' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 129\u001b[0m\n\u001b[0;32m    127\u001b[0m layers \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    128\u001b[0m model \u001b[38;5;241m=\u001b[39m PhysicsInformedNN(X_u_train, u_train, X_f_train, X_u_train, u_train, layers, lb, ub)\n\u001b[1;32m--> 129\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;66;03m# Predict and visualize results\u001b[39;00m\n\u001b[0;32m    133\u001b[0m u_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39msess\u001b[38;5;241m.\u001b[39mrun(model\u001b[38;5;241m.\u001b[39mu_pred, feed_dict\u001b[38;5;241m=\u001b[39m{model\u001b[38;5;241m.\u001b[39mx_tf: X_star})\n",
      "Cell \u001b[1;32mIn[54], line 95\u001b[0m, in \u001b[0;36mPhysicsInformedNN.train\u001b[1;34m(self, nIter)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;124;03mTraining loop for the PINN.\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m it \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nIter):\n\u001b[1;32m---> 95\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msess\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_op, feed_dict\u001b[38;5;241m=\u001b[39m{\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_tf: X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mu_tf: \u001b[43mu\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_f_tf: X_f})\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m it \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     97\u001b[0m         loss_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msess\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss, feed_dict\u001b[38;5;241m=\u001b[39m{\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_tf: X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mu_tf: u, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_f_tf: X_f})\n",
      "\u001b[1;31mNameError\u001b[0m: name 'u' is not defined"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## Physics-Informed Neural Networks for Burgers' Equation\n",
    "# This notebook demonstrates the discovery of the Burgers' equation using a physics-informed neural network (PINN).\n",
    "\n",
    "# %%\n",
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "from scipy.interpolate import griddata\n",
    "from scipy.spatial import distance\n",
    "from matplotlib import cm\n",
    "from pyDOE import lhs\n",
    "import os\n",
    "import time\n",
    "\n",
    "# %%\n",
    "# Set TensorFlow to execute eagerly\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# %%\n",
    "# Define the PhysicsInformedNN class\n",
    "class PhysicsInformedNN:\n",
    "    def __init__(self, X, u, X_f, X_val, u_val, layers, lb, ub):\n",
    "        \"\"\"\n",
    "        Initializes the Physics-Informed Neural Network (PINN).\n",
    "        \"\"\"\n",
    "        self.lb = lb\n",
    "        self.ub = ub\n",
    "        self.layers = layers\n",
    "\n",
    "        # Initialize NN weights and biases\n",
    "        self.weights, self.biases = self.initialize_NN(layers)\n",
    "\n",
    "        # Input placeholders\n",
    "        self.x_tf = tf.compat.v1.placeholder(tf.float32, shape=[None, X.shape[1]])\n",
    "        self.u_tf = tf.compat.v1.placeholder(tf.float32, shape=[None, u.shape[1]])\n",
    "        self.x_f_tf = tf.compat.v1.placeholder(tf.float32, shape=[None, X_f.shape[1]])\n",
    "        self.u_val_tf = tf.compat.v1.placeholder(tf.float32, shape=[None, u_val.shape[1]])\n",
    "\n",
    "        # Prediction for u\n",
    "        self.u_pred = self.neural_net(self.x_tf)\n",
    "\n",
    "        # Physics loss components\n",
    "        self.f_pred = self.physics_loss(self.x_f_tf)\n",
    "\n",
    "        # Losses\n",
    "        self.loss = tf.reduce_mean(tf.square(self.u_tf - self.u_pred)) + tf.reduce_mean(tf.square(self.f_pred))\n",
    "\n",
    "        # Optimizers\n",
    "        self.optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=0.001)\n",
    "        self.train_op = self.optimizer.minimize(self.loss)\n",
    "\n",
    "        # TensorFlow session\n",
    "        self.sess = tf.compat.v1.Session()\n",
    "        self.sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "    def initialize_NN(self, layers):\n",
    "        \"\"\"\n",
    "        Xavier initialization of weights and biases for the neural network.\n",
    "        \"\"\"\n",
    "        weights = []\n",
    "        biases = []\n",
    "        for i in range(len(layers) - 1):\n",
    "            W = tf.Variable(tf.random.truncated_normal([layers[i], layers[i + 1]], stddev=0.1), dtype=tf.float32)\n",
    "            b = tf.Variable(tf.zeros([1, layers[i + 1]], dtype=tf.float32), dtype=tf.float32)\n",
    "            weights.append(W)\n",
    "            biases.append(b)\n",
    "        return weights, biases\n",
    "\n",
    "    def neural_net(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the neural network.\n",
    "        \"\"\"\n",
    "        H = 2.0 * (x - self.lb) / (self.ub - self.lb) - 1.0\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            H = tf.nn.tanh(tf.add(tf.matmul(H, self.weights[i]), self.biases[i]))\n",
    "        return tf.add(tf.matmul(H, self.weights[-1]), self.biases[-1])\n",
    "\n",
    "    def physics_loss(self, x):\n",
    "        \"\"\"\n",
    "        Computes the physics-based loss.\n",
    "        \"\"\"\n",
    "        u = self.neural_net(x)\n",
    "        u_x = tf.gradients(u, x)[0]\n",
    "        u_xx = tf.gradients(u_x, x)[0]\n",
    "        return -0.01 * u_xx + u * u_x\n",
    "\n",
    "    def train(self, nIter):\n",
    "        \"\"\"\n",
    "        Training loop for the PINN.\n",
    "        \"\"\"\n",
    "        for it in range(nIter):\n",
    "            u = self.neural_net(x)\n",
    "            self.sess.run(self.train_op, feed_dict={self.x_tf: X, self.u_tf: u, self.x_f_tf: X_f})\n",
    "            if it % 100 == 0:\n",
    "                loss_value = self.sess.run(self.loss, feed_dict={self.x_tf: X, self.u_tf: u, self.x_f_tf: X_f})\n",
    "                print(f\"Iteration {it}: Loss = {loss_value}\")\n",
    "\n",
    "# %%\n",
    "# Load data\n",
    "data = scipy.io.loadmat(\"data/burgers.mat\")\n",
    "t = data[\"t\"].flatten()[:, None]\n",
    "x = data[\"x\"].flatten()[:, None]\n",
    "Exact = np.real(data[\"usol\"]).T\n",
    "\n",
    "# Domain bounds\n",
    "X, T = np.meshgrid(x, t)\n",
    "X_star = np.hstack((X.flatten()[:, None], T.flatten()[:, None]))\n",
    "u_star = Exact.flatten()[:, None]\n",
    "lb = X_star.min(0)\n",
    "ub = X_star.max(0)\n",
    "\n",
    "# %%\n",
    "# Generate training data\n",
    "N_u = 2000\n",
    "idx = np.random.choice(X_star.shape[0], N_u, replace=False)\n",
    "X_u_train = X_star[idx, :]\n",
    "u_train = u_star[idx, :]\n",
    "\n",
    "# Generate collocation points\n",
    "N_f = 10000\n",
    "X_f_train = lb + (ub - lb) * lhs(2, N_f)\n",
    "\n",
    "# %%\n",
    "# Define and train the model\n",
    "layers = [2, 50, 50, 50, 1]\n",
    "model = PhysicsInformedNN(X_u_train, u_train, X_f_train, X_u_train, u_train, layers, lb, ub)\n",
    "model.train(1000)\n",
    "\n",
    "# %%\n",
    "# Predict and visualize results\n",
    "u_pred = model.sess.run(model.u_pred, feed_dict={model.x_tf: X_star})\n",
    "u_pred = griddata(X_star, u_pred.flatten(), (X, T), method=\"cubic\")\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.pcolor(T, X, u_pred, cmap=cm.coolwarm)\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"x\")\n",
    "plt.title(\"Predicted Solution\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Physics-Informed Neural Network (PINN) for PDE Discovery\n",
    "# This notebook implements a PINN for discovering governing equations such as the Burgers' equation.\n",
    "\n",
    "# %%\n",
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "from pyDOE import lhs\n",
    "import time\n",
    "import os\n",
    "\n",
    "# %%\n",
    "# Disable eager execution for compatibility with TensorFlow v1 code style\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# %%\n",
    "# Define the PhysicsInformedNN class\n",
    "# Define the PhysicsInformedNN class\n",
    "class PhysicsInformedNN:\n",
    "    def __init__(self, X, u, X_f, X_val, u_val, layers, lb, ub):\n",
    "        \"\"\"\n",
    "        Initializes the Physics-Informed Neural Network (PINN).\n",
    "        \"\"\"\n",
    "        self.lb = tf.constant(lb.reshape(1, -1), dtype=tf.float32)  # Reshape to [1, n_features]\n",
    "        self.ub = tf.constant(ub.reshape(1, -1), dtype=tf.float32)  # Reshape to [1, n_features]\n",
    "        self.layers = layers\n",
    "\n",
    "        # Initialize NN weights and biases\n",
    "        self.weights, self.biases = self.initialize_NN(layers)\n",
    "\n",
    "        # TensorFlow placeholders for inputs and outputs\n",
    "        self.x_tf = tf.compat.v1.placeholder(tf.float32, shape=[None, X.shape[1]])\n",
    "        self.t_tf = tf.compat.v1.placeholder(tf.float32, shape=[None, 1])\n",
    "        self.u_tf = tf.compat.v1.placeholder(tf.float32, shape=[None, 1])\n",
    "        self.x_f_tf = tf.compat.v1.placeholder(tf.float32, shape=[None, X_f.shape[1]])\n",
    "\n",
    "        # Neural network predictions\n",
    "        self.u_pred = self.net_u(self.x_tf, self.t_tf)\n",
    "        self.f_pred, self.Phi_pred, self.u_t_pred = self.net_f(self.x_f_tf)\n",
    "\n",
    "        # Loss function\n",
    "        self.loss_u = tf.reduce_mean(tf.square(self.u_tf - self.u_pred))\n",
    "        self.loss_f = tf.reduce_mean(tf.square(self.f_pred))\n",
    "        self.loss = self.loss_u + self.loss_f\n",
    "\n",
    "        # Optimizer\n",
    "        self.optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=0.001)\n",
    "        self.train_op = self.optimizer.minimize(self.loss)\n",
    "\n",
    "        # TensorFlow session\n",
    "        self.sess = tf.compat.v1.Session()\n",
    "        self.sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "    def initialize_NN(self, layers):\n",
    "        \"\"\"\n",
    "        Xavier initialization of weights and biases for the neural network.\n",
    "        \"\"\"\n",
    "        weights = []\n",
    "        biases = []\n",
    "        for i in range(len(layers) - 1):\n",
    "            W = tf.Variable(tf.random.truncated_normal([layers[i], layers[i + 1]], stddev=0.1), dtype=tf.float32)\n",
    "            b = tf.Variable(tf.zeros([1, layers[i + 1]], dtype=tf.float32), dtype=tf.float32)\n",
    "            weights.append(W)\n",
    "            biases.append(b)\n",
    "        return weights, biases\n",
    "\n",
    "    def neural_net(self, X, weights, biases):\n",
    "        \"\"\"\n",
    "        Forward pass through the neural network.\n",
    "        \"\"\"\n",
    "        # Debugging input shapes\n",
    "        tf.print(\"Shape of X:\", tf.shape(X))\n",
    "        tf.print(\"Shape of lb:\", tf.shape(self.lb))\n",
    "        tf.print(\"Shape of ub:\", tf.shape(self.ub))\n",
    "\n",
    "        # Reshape lb and ub to be broadcastable\n",
    "        self.lb = tf.reshape(self.lb, [1, -1])\n",
    "        self.ub = tf.reshape(self.ub, [1, -1])\n",
    "\n",
    "        # Normalize input\n",
    "        H = 2.0 * (X - self.lb) / (self.ub - self.lb) - 1.0\n",
    "\n",
    "        # Forward pass\n",
    "        for i in range(len(weights) - 1):\n",
    "            W = weights[i]\n",
    "            b = biases[i]\n",
    "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "        W = weights[-1]\n",
    "        b = biases[-1]\n",
    "        return tf.add(tf.matmul(H, W), b)\n",
    "\n",
    "\n",
    "    def net_u(self, x, t):\n",
    "        \"\"\"\n",
    "        Forward pass for the solution u(x,t).\n",
    "        \"\"\"\n",
    "        return self.neural_net(tf.concat([x, t], axis=1), self.weights, self.biases)\n",
    "\n",
    "    def net_f(self, X_f):\n",
    "        \"\"\"\n",
    "        Physics-based loss for the PDE residual and candidate library construction.\n",
    "        \"\"\"\n",
    "        x, t = X_f[:, 0:1], X_f[:, 1:2]\n",
    "        u = self.net_u(x, t)\n",
    "\n",
    "        # Derivatives\n",
    "        u_t = tf.gradients(u, t)[0]\n",
    "        u_x = tf.gradients(u, x)[0]\n",
    "        u_xx = tf.gradients(u_x, x)[0]\n",
    "\n",
    "        # Candidate library Phi\n",
    "        Phi = tf.concat([tf.ones_like(u), u, u**2, u_x, u**3, u_xx], axis=1)\n",
    "\n",
    "        # Residual\n",
    "        f = u_t + u * u_x - (0.01 / np.pi) * u_xx\n",
    "        return f, Phi, u_t\n",
    "\n",
    "    def train(self, nIter, X_u_train, u_train, X_f_train):\n",
    "        \"\"\"\n",
    "        Training loop for the PINN.\n",
    "        \"\"\"\n",
    "        tf_dict = {self.x_tf: X_u_train[:, 0:1], self.t_tf: X_u_train[:, 1:2], self.u_tf: u_train,\n",
    "                   self.x_f_tf: X_f_train}\n",
    "\n",
    "        for it in range(nIter):\n",
    "            self.sess.run(self.train_op, tf_dict)\n",
    "\n",
    "            if it % 10 == 0:\n",
    "                loss_value = self.sess.run(self.loss, tf_dict)\n",
    "                print(f\"Iteration {it}, Loss: {loss_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimensions must be equal, but are 3 and 2 for '{{node sub_8}} = Sub[T=DT_FLOAT](concat_3, Reshape)' with input shapes: [?,3], [1,2].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Define the PINN model\u001b[39;00m\n\u001b[0;32m     29\u001b[0m layers \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 30\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mPhysicsInformedNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_u_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_f_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_u_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mub\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m     33\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;241m1000\u001b[39m, X_u_train, u_train, X_f_train)\n",
      "Cell \u001b[1;32mIn[61], line 41\u001b[0m, in \u001b[0;36mPhysicsInformedNN.__init__\u001b[1;34m(self, X, u, X_f, X_val, u_val, layers, lb, ub)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_f_tf \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mplaceholder(tf\u001b[38;5;241m.\u001b[39mfloat32, shape\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28;01mNone\u001b[39;00m, X_f\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]])\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Neural network predictions\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mu_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet_u\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_tf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt_tf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_pred, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mPhi_pred, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mu_t_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet_f(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_f_tf)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Loss function\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[61], line 100\u001b[0m, in \u001b[0;36mPhysicsInformedNN.net_u\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnet_u\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[0;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03m    Forward pass for the solution u(x,t).\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mneural_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbiases\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[61], line 84\u001b[0m, in \u001b[0;36mPhysicsInformedNN.neural_net\u001b[1;34m(self, X, weights, biases)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mub \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mub, [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# Normalize input\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m H \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2.0\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[43mX\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlb\u001b[49m) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mub \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlb) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(weights) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\chidi\\anaconda3\\envs\\boot_DA\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\chidi\\anaconda3\\envs\\boot_DA\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1056\u001b[0m, in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs, op_def, extract_traceback)\u001b[0m\n\u001b[0;32m   1053\u001b[0m   c_op \u001b[38;5;241m=\u001b[39m pywrap_tf_session\u001b[38;5;241m.\u001b[39mTF_FinishOperation(op_desc)\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mInvalidArgumentError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1055\u001b[0m   \u001b[38;5;66;03m# Convert to ValueError for backwards compatibility.\u001b[39;00m\n\u001b[1;32m-> 1056\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(e\u001b[38;5;241m.\u001b[39mmessage)\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;66;03m# Record the current Python stack trace as the creating stacktrace of this\u001b[39;00m\n\u001b[0;32m   1059\u001b[0m \u001b[38;5;66;03m# TF_Operation.\u001b[39;00m\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extract_traceback:\n",
      "\u001b[1;31mValueError\u001b[0m: Dimensions must be equal, but are 3 and 2 for '{{node sub_8}} = Sub[T=DT_FLOAT](concat_3, Reshape)' with input shapes: [?,3], [1,2]."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Load and preprocess data\n",
    "data = scipy.io.loadmat(\"data/burgers.mat\")\n",
    "t = data[\"t\"].flatten()[:, None]\n",
    "x = data[\"x\"].flatten()[:, None]\n",
    "Exact = np.real(data[\"usol\"]).T\n",
    "\n",
    "X, T = np.meshgrid(x, t)\n",
    "X_star = np.hstack((X.flatten()[:, None], T.flatten()[:, None]))\n",
    "u_star = Exact.flatten()[:, None]\n",
    "\n",
    "# Domain bounds\n",
    "lb = X_star.min(0)\n",
    "ub = X_star.max(0)\n",
    "\n",
    "# %%\n",
    "# Training data\n",
    "N_u = 2000\n",
    "idx = np.random.choice(X_star.shape[0], N_u, replace=False)\n",
    "X_u_train = X_star[idx, :]\n",
    "u_train = u_star[idx, :]\n",
    "\n",
    "# Collocation points\n",
    "N_f = 10000\n",
    "X_f_train = lb + (ub - lb) * lhs(2, N_f)\n",
    "\n",
    "# %%\n",
    "# Define the PINN model\n",
    "layers = [2, 50, 50, 50, 50, 1]\n",
    "model = PhysicsInformedNN(X_u_train, u_train, X_f_train, X_u_train, u_train, layers, lb, ub)\n",
    "\n",
    "# Train the model\n",
    "model.train(1000, X_u_train, u_train, X_f_train)\n",
    "\n",
    "# Discover PDE\n",
    "lambda_discovered = model.callTrainSTRidge()\n",
    "print(\"Discovered PDE coefficients:\", lambda_discovered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2d6ae026f70>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "from scipy.interpolate import griddata\n",
    "from scipy.spatial import distance\n",
    "from matplotlib import cm\n",
    "import time\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from pyDOE import lhs\n",
    "#    import sobol_seq\n",
    "import os\n",
    "\n",
    "# Set device to GPU if available, otherwise CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define loss histories to record convergence\n",
    "# Adam loss history\n",
    "loss_history_Adam = np.array([0])\n",
    "loss_u_history_Adam = np.array([0])\n",
    "loss_f_history_Adam = np.array([0])\n",
    "loss_lambda_history_Adam = np.array([0])\n",
    "lambda_history_Adam = np.zeros((16, 1))\n",
    "loss_history_Adam_val = np.array([0])\n",
    "loss_u_history_Adam_val = np.array([0])\n",
    "loss_f_history_Adam_val = np.array([0])\n",
    "\n",
    "# STRidge loss history\n",
    "loss_history_STRidge = np.array([0])\n",
    "loss_f_history_STRidge = np.array([0])\n",
    "loss_lambda_history_STRidge = np.array([0])\n",
    "optimaltol_history = np.array([0])\n",
    "tol_history_STRidge = np.array([0])\n",
    "lambda_normalized_history_STRidge = np.zeros((16, 1))\n",
    "\n",
    "lambda_history_STRidge = np.zeros((16, 1))\n",
    "ridge_append_counter_STRidge = np.array([0])\n",
    "\n",
    "# Loss histories for pretraining\n",
    "loss_history_Pretrain = np.array([0])\n",
    "loss_u_history_Pretrain = np.array([0])\n",
    "loss_f_history_Pretrain = np.array([0])\n",
    "loss_lambda_history_Pretrain = np.array([0])\n",
    "loss_history_val_Pretrain = np.array([0])\n",
    "loss_u_history_val_Pretrain = np.array([0])\n",
    "loss_f_history_val_Pretrain = np.array([0])\n",
    "step_Pretrain = 0\n",
    "\n",
    "lambda_history_Pretrain = np.zeros((16, 1))\n",
    "\n",
    "# Set the random seeds for reproducibility\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Add other PyTorch-specific functionality as needed\n",
    "\n",
    "# Example of creating a tensor and moving it to GPU\n",
    "# x = torch.tensor(np.random.randn(10, 1), dtype=torch.float32, requires_grad=True).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device to GPU if available, otherwise CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# =============================================================================\n",
    "# Define the Physics-Informed Neural Network class (PINN)\n",
    "# =============================================================================\n",
    "class PhysicsInformedNN:\n",
    "    def __init__(self, X, u, X_f, X_val, u_val, layers, lb, ub):\n",
    "        self.lb = lb\n",
    "        self.ub = ub\n",
    "        self.layers = layers\n",
    "        \n",
    "        # Initialize NNs\n",
    "        self.weights, self.biases = self.initialize_NN(layers)\n",
    "        \n",
    "        # Training data\n",
    "        self.x = torch.tensor(X[:, 0:1], dtype=torch.float32, requires_grad=True).to(device)\n",
    "        self.t = torch.tensor(X[:, 1:2], dtype=torch.float32, requires_grad=True).to(device)\n",
    "        self.u = torch.tensor(u, dtype=torch.float32).to(device)\n",
    "        \n",
    "        # Collocation data\n",
    "        self.x_f = torch.tensor(X_f[:, 0:1], dtype=torch.float32, requires_grad=True).to(device)\n",
    "        self.t_f = torch.tensor(X_f[:, 1:2], dtype=torch.float32, requires_grad=True).to(device)\n",
    "        \n",
    "        # Validation data\n",
    "        self.x_val = torch.tensor(X_val[:, 0:1], dtype=torch.float32).to(device)\n",
    "        self.t_val = torch.tensor(X_val[:, 1:2], dtype=torch.float32).to(device)\n",
    "        self.u_val = torch.tensor(u_val, dtype=torch.float32).to(device)\n",
    "        \n",
    "        # Initializing lambda\n",
    "        self.lambda1 = torch.zeros((16, 1), dtype=torch.float32, requires_grad=True).to(device)\n",
    "        \n",
    "        # Loss function placeholders\n",
    "        self.loss_u = torch.nn.MSELoss()\n",
    "        \n",
    "        # Placeholder for physics-based loss\n",
    "        self.loss_f_coeff = torch.tensor(1.0, dtype=torch.float32, requires_grad=False).to(device)\n",
    "        \n",
    "        self.optimizer_Adam = torch.optim.Adam([{'params': self.weights}, {'params': self.biases}, {'params': self.lambda1}], lr=1e-3)\n",
    "        \n",
    "        # Model forward pass and loss computation\n",
    "        self.u_pred = self.net_u(self.x, self.t)\n",
    "        self.f_pred, self.Phi_pred, self.u_t_pred = self.net_f(self.x_f, self.t_f)\n",
    "        \n",
    "        self.loss_u_val = self.loss_u(self.u_pred, self.u)\n",
    "        self.loss_f = self.loss_f_coeff * torch.mean(torch.square(self.f_pred))\n",
    "        self.loss_lambda = 1e-7 * torch.norm(self.lambda1, p=1)\n",
    "        \n",
    "        # Total loss\n",
    "        self.loss = torch.log(self.loss_u_val + self.loss_f + self.loss_lambda)\n",
    "        \n",
    "        # Validation loss\n",
    "        self.u_val_pred = self.net_u(self.x_val, self.t_val)\n",
    "        self.loss_u_val = self.loss_u(self.u_val_pred, self.u_val)\n",
    "\n",
    "    def initialize_NN(self, layers):\n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(layers) \n",
    "        for l in range(0, num_layers - 1):\n",
    "            W = self.xavier_init(layers[l], layers[l + 1])\n",
    "            b = torch.zeros(1, layers[l + 1], dtype=torch.float32).to(device)\n",
    "            weights.append(W)\n",
    "            biases.append(b)\n",
    "        return weights, biases\n",
    "        \n",
    "    def xavier_init(self, in_dim, out_dim):\n",
    "        xavier_stddev = np.sqrt(2.0 / (in_dim + out_dim))\n",
    "        return torch.randn((in_dim, out_dim), dtype=torch.float32).to(device) * xavier_stddev\n",
    "    \n",
    "    def neural_net(self, X, weights, biases):\n",
    "        H = 2.0 * (X - self.lb) / (self.ub - self.lb) - 1.0\n",
    "        for l in range(len(weights) - 1):\n",
    "            W = weights[l]\n",
    "            b = biases[l]\n",
    "            H = torch.tanh(torch.add(torch.matmul(H, W), b))\n",
    "        W = weights[-1]\n",
    "        b = biases[-1]\n",
    "        Y = torch.add(torch.matmul(H, W), b)\n",
    "        return Y\n",
    "        \n",
    "    def net_u(self, x, t):\n",
    "        return self.neural_net(torch.cat([x, t], dim=1), self.weights, self.biases)\n",
    "    \n",
    "    def net_f(self, x, t):\n",
    "        u = self.net_u(x, t)\n",
    "        u_t = torch.autograd.grad(u, t, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "        u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "        u_xx = torch.autograd.grad(u_x, x, grad_outputs=torch.ones_like(u_x), create_graph=True)[0]\n",
    "        u_xxx = torch.autograd.grad(u_xx, x, grad_outputs=torch.ones_like(u_xx), create_graph=True)[0]\n",
    "        \n",
    "        Phi = torch.cat([torch.ones_like(u), u, u**2, u**3, u_x, u*u_x, u**2*u_x, u**3*u_x, u_xx, u*u_xx, u**2*u_xx, u**3*u_xx, u_xxx, u*u_xxx, u**2*u_xxx, u**3*u_xxx], dim=1)\n",
    "        \n",
    "        f = u_t - self.lambda1[0] * u - self.lambda1[1] * u_xx\n",
    "        \n",
    "        return f, Phi, u_t\n",
    "\n",
    "    def train(self, nIter, X_u_train, u_train, X_f_train):\n",
    "        for epoch in range(nIter):\n",
    "            self.optimizer_Adam.zero_grad()\n",
    "            loss = self.loss\n",
    "            loss.backward()\n",
    "            self.optimizer_Adam.step()\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "    def callback_Pretrain(self, loss, loss_u, loss_f, loss_lambda, loss_val, loss_u_val, loss_f_val, lamu):\n",
    "        step_Pretrain = 0\n",
    "        step_Pretrain += 1\n",
    "        if step_Pretrain % 10 == 0:\n",
    "            print(f'Step: {step_Pretrain}, log Loss: {loss:.3e}, loss_u: {loss_u:.3e}, loss_f: {loss_f:.3e}, loss_lambda: {loss_lambda:.3e}')\n",
    "            \n",
    "            # Save losses and lambda values in lists (history)\n",
    "            loss_history_Pretrain\n",
    "            loss_u_history_Pretrain\n",
    "            loss_f_history_Pretrain\n",
    "            loss_lambda_history_Pretrain\n",
    "            loss_history_val_Pretrain\n",
    "            loss_u_history_val_Pretrain\n",
    "            loss_f_history_val_Pretrain\n",
    "            lambda_history_Pretrain\n",
    "            \n",
    "            loss_history_Pretrain.append(loss)\n",
    "            loss_u_history_Pretrain.append(loss_u)\n",
    "            loss_f_history_Pretrain.append(loss_f)\n",
    "            loss_lambda_history_Pretrain.append(loss_lambda)\n",
    "            loss_history_val_Pretrain.append(loss_val)\n",
    "            loss_u_history_val_Pretrain.append(loss_u_val)\n",
    "            loss_f_history_val_Pretrain.append(loss_f_val)\n",
    "            lambda_history_Pretrain.append(lamu)\n",
    "\n",
    "    def train(self, nIter):  # nIter is the number of iterations\n",
    "        self.loss_f_coeff = torch.tensor(1.0, dtype=torch.float32, requires_grad=False)\n",
    "        \n",
    "        # Pretraining phase using L-BFGS-B optimizer equivalent in PyTorch (via scipy)\n",
    "        print('L-BFGS-B pretraining begins')\n",
    "        self.optimizer_Adam.zero_grad()  # Zero gradients\n",
    "        \n",
    "        for epoch in range(nIter):\n",
    "            # Perform STRidge optimization (custom method)\n",
    "            print('STRidge begins')\n",
    "            self.callTrainSTRidge()\n",
    "    \n",
    "            # Adam optimization loop\n",
    "            print('Adam begins')\n",
    "            start_time = time.time()\n",
    "            for it_Adam in range(1000):\n",
    "                self.optimizer_Adam.zero_grad()  # Zero gradients before backward pass\n",
    "                loss = self.loss  # Your loss function here\n",
    "                loss.backward()  # Backpropagate gradients\n",
    "                self.optimizer_Adam.step()  # Update weights\n",
    "\n",
    "                # Print progress every 10 iterations\n",
    "                if it_Adam % 10 == 0:\n",
    "                    elapsed = time.time() - start_time\n",
    "                    print(f'It: {it_Adam}, Log Loss: {loss:.3e}, Time: {elapsed:.2f}')\n",
    "                    \n",
    "                    lamu = self.lambda1.detach().cpu().numpy()  # Get current lambda values\n",
    "                    \n",
    "                    # Save history (loss, lambda, etc.)\n",
    "                    loss_history_Adam\n",
    "                    lambda_history_Adam\n",
    "                    loss_u_history_Adam\n",
    "                    loss_f_history_Adam\n",
    "                    loss_lambda_history_Adam\n",
    "                    loss_history_Adam_val\n",
    "                    loss_u_history_Adam_val\n",
    "                    loss_f_history_Adam_val\n",
    "                    \n",
    "                    loss_history_Adam.append(loss.item())\n",
    "                    lambda_history_Adam.append(lamu)\n",
    "                    # Add other losses to history\n",
    "                    loss_history_Adam_val.append(loss_val.item())\n",
    "                    loss_u_history_Adam_val.append(loss_u_val.item())\n",
    "                    loss_f_history_Adam_val.append(loss_f_val.item())\n",
    "            \n",
    "            start_time = time.time()\n",
    "\n",
    "        # One more round of STRidge optimization after Adam\n",
    "        print('STRidge begins')\n",
    "        self.callTrainSTRidge()\n",
    "\n",
    "\n",
    "    def predict(self, X_star):\n",
    "        x_star = torch.tensor(X_star[:, 0:1], dtype=torch.float32, requires_grad=True)\n",
    "        t_star = torch.tensor(X_star[:, 1:2], dtype=torch.float32, requires_grad=True)\n",
    "        u_star = self.net_u(x_star, t_star)\n",
    "        return u_star.detach().numpy()\n",
    "\n",
    "    def callTrainSTRidge(self):\n",
    "        lam = 1e-5\n",
    "        d_tol = 1\n",
    "        maxit = 100\n",
    "        STR_iters = 10\n",
    "        l0_penalty = None\n",
    "        normalize = 2\n",
    "        split = 0.8\n",
    "        print_best_tol = False\n",
    "\n",
    "        with torch.no_grad():\n",
    "            Phi_pred, u_t_pred = self.net_f(self.x_f, self.t_f)\n",
    "\n",
    "        lambda2 = self.TrainSTRidge(Phi_pred, u_t_pred, lam, d_tol, maxit, STR_iters, l0_penalty, normalize, split, print_best_tol)\n",
    "        self.lambda1.data = torch.tensor(lambda2, dtype=torch.float32)\n",
    "\n",
    "    def TrainSTRidge(self, R0, Ut, lam, d_tol, maxit, STR_iters=10, l0_penalty=None, normalize=2, split=0.8, print_best_tol=False):\n",
    "        n, d = R0.shape\n",
    "        R = torch.zeros((n, d), dtype=torch.float32)\n",
    "\n",
    "        if normalize != 0:\n",
    "            Mreg = torch.zeros((d, 1), dtype=torch.float32)\n",
    "            for i in range(d):\n",
    "                Mreg[i] = 1.0 / torch.norm(R0[:, i], p=normalize)\n",
    "                R[:, i] = Mreg[i] * R0[:, i]\n",
    "            normalize_inner = 0\n",
    "        else:\n",
    "            R = R0.clone()\n",
    "            Mreg = torch.ones((d, 1), dtype=torch.float32) * d\n",
    "            normalize_inner = 2\n",
    "\n",
    "        lambda_normalized_history_STRidge\n",
    "        lambda_normalized_history_STRidge = np.append(lambda_normalized_history_STRidge, Mreg.numpy(), axis=1)\n",
    "\n",
    "        # Split data into training and test sets\n",
    "        np.random.seed(0)\n",
    "        indices = np.random.permutation(n)\n",
    "        train_size = int(n * split)\n",
    "        train_idx = indices[:train_size]\n",
    "        test_idx = indices[train_size:]\n",
    "\n",
    "        TrainR = R[train_idx, :]\n",
    "        TestR = R[test_idx, :]\n",
    "        TrainY = Ut[train_idx, :]\n",
    "        TestY = Ut[test_idx, :]\n",
    "\n",
    "        # Set up the initial tolerance and l0 penalty\n",
    "        d_tol = float(d_tol)\n",
    "        if not hasattr(self, \"tol\"):\n",
    "            self.tol = d_tol\n",
    "\n",
    "        w_best = (self.lambda1 / Mreg).detach()\n",
    "        err_f = torch.mean((TestY - TestR @ w_best) ** 2)\n",
    "\n",
    "        if l0_penalty is None and not hasattr(self, \"l0_penalty_0\"):\n",
    "            self.l0_penalty_0 = err_f\n",
    "            l0_penalty = self.l0_penalty_0\n",
    "        elif l0_penalty is None:\n",
    "            l0_penalty = self.l0_penalty_0\n",
    "\n",
    "        err_lambda = l0_penalty * (w_best != 0).sum().item()\n",
    "        err_best = err_f + err_lambda\n",
    "        tol_best = 0\n",
    "\n",
    "        loss_history_STRidge\n",
    "        loss_f_history_STRidge\n",
    "        loss_lambda_history_STRidge\n",
    "        tol_history_STRidge\n",
    "\n",
    "        loss_history_STRidge = np.append(loss_history_STRidge, err_best.numpy())\n",
    "        loss_f_history_STRidge = np.append(loss_f_history_STRidge, err_f.numpy())\n",
    "        loss_lambda_history_STRidge = np.append(loss_lambda_history_STRidge, err_lambda)\n",
    "        tol_history_STRidge = np.append(tol_history_STRidge, tol_best)\n",
    "\n",
    "        # Increase tolerance until test performance decreases\n",
    "        for _ in range(maxit):\n",
    "            w = self.STRidge(TrainR, TrainY, lam, STR_iters, self.tol, Mreg, normalize_inner)\n",
    "            err_f = torch.mean((TestY - TestR @ w) ** 2)\n",
    "            err_lambda = l0_penalty * (w != 0).sum().item()\n",
    "            err = err_f + err_lambda\n",
    "\n",
    "            if err <= err_best:\n",
    "                err_best = err\n",
    "                w_best = w\n",
    "                tol_best = self.tol\n",
    "                self.tol += d_tol\n",
    "\n",
    "                loss_history_STRidge = np.append(loss_history_STRidge, err_best.numpy())\n",
    "                loss_f_history_STRidge = np.append(loss_f_history_STRidge, err_f.numpy())\n",
    "                loss_lambda_history_STRidge = np.append(loss_lambda_history_STRidge, err_lambda)\n",
    "                tol_history_STRidge = np.append(tol_history_STRidge, tol_best)\n",
    "            else:\n",
    "                self.tol = max(0, self.tol - 2 * d_tol)\n",
    "                d_tol /= 1.618\n",
    "                self.tol += d_tol\n",
    "\n",
    "        if print_best_tol:\n",
    "            print(\"Optimal tolerance:\", tol_best)\n",
    "\n",
    "        optimaltol_history\n",
    "        optimaltol_history = np.append(optimaltol_history, tol_best)\n",
    "\n",
    "        return (w_best * Mreg).numpy()\n",
    "\n",
    "    \n",
    "    def STRidge(self, X0, y, lam, maxit, tol, Mreg, normalize=2, print_results=False):\n",
    "        \"\"\"\n",
    "        Sequential Threshold Ridge Regression (STRidge)\n",
    "        Converted to PyTorch from TensorFlow/Numpy.\n",
    "        \"\"\"\n",
    "        n, d = X0.shape\n",
    "        X = torch.zeros((n, d), dtype=torch.complex64)\n",
    "\n",
    "        # Normalize data\n",
    "        if normalize != 0:\n",
    "            Mreg = torch.zeros((d, 1), dtype=torch.float32)\n",
    "            for i in range(d):\n",
    "                Mreg[i] = 1.0 / torch.norm(X0[:, i], p=normalize)\n",
    "                X[:, i] = Mreg[i] * X0[:, i]\n",
    "        else:\n",
    "            X = X0.clone()\n",
    "\n",
    "        # Inherit lambda\n",
    "        w = self.lambda1.detach() / Mreg\n",
    "\n",
    "        # Select indices for big weights\n",
    "        biginds = torch.where(torch.abs(w) > tol)[0]\n",
    "        num_relevant = d\n",
    "\n",
    "        # variables for logging\n",
    "        ridge_append_counter_STRidge\n",
    "        ridge_append_counter = 0\n",
    "\n",
    "        lambda_history_STRidge\n",
    "        lambda_history_STRidge = np.append(lambda_history_STRidge, (Mreg * w).numpy(), axis=1)\n",
    "        ridge_append_counter += 1\n",
    "\n",
    "        # Threshold and iterate\n",
    "        for _ in range(maxit):\n",
    "            # Determine small indices and new big indices\n",
    "            smallinds = torch.where(torch.abs(w) < tol)[0]\n",
    "            new_biginds = [i for i in range(d) if i not in smallinds]\n",
    "\n",
    "            # Stop if no changes\n",
    "            if num_relevant == len(new_biginds):\n",
    "                break\n",
    "            else:\n",
    "                num_relevant = len(new_biginds)\n",
    "\n",
    "            if len(new_biginds) == 0:\n",
    "                if _ == 0:\n",
    "                    if normalize != 0:\n",
    "                        lambda_history_STRidge = np.append(lambda_history_STRidge, (w * Mreg).numpy(), axis=1)\n",
    "                        ridge_append_counter += 1\n",
    "                        ridge_append_counter_STRidge = np.append(ridge_append_counter_STRidge, ridge_append_counter)\n",
    "                        return (Mreg * w).numpy()\n",
    "                    else:\n",
    "                        lambda_history_STRidge = np.append(lambda_history_STRidge, (w * Mreg).numpy(), axis=1)\n",
    "                        ridge_append_counter += 1\n",
    "                        ridge_append_counter_STRidge = np.append(ridge_append_counter_STRidge, ridge_append_counter)\n",
    "                        return w.numpy()\n",
    "                else:\n",
    "                    break\n",
    "            biginds = new_biginds\n",
    "\n",
    "            # Update weights\n",
    "            w[smallinds] = 0\n",
    "\n",
    "            if lam != 0:\n",
    "                w[biginds] = torch.linalg.lstsq(\n",
    "                    X[:, biginds].T @ X[:, biginds] + lam * torch.eye(len(biginds), dtype=torch.float32),\n",
    "                    X[:, biginds].T @ y,\n",
    "                )[0]\n",
    "                lambda_history_STRidge = np.append(lambda_history_STRidge, (Mreg * w).numpy(), axis=1)\n",
    "                ridge_append_counter += 1\n",
    "            else:\n",
    "                w[biginds] = torch.linalg.lstsq(X[:, biginds], y)[0]\n",
    "                lambda_history_STRidge = np.append(lambda_history_STRidge, (Mreg * w).numpy(), axis=1)\n",
    "                ridge_append_counter += 1\n",
    "\n",
    "        # Final least squares with sparsity pattern\n",
    "        if len(biginds) > 0:\n",
    "            w[biginds] = torch.linalg.lstsq(X[:, biginds], y)[0]\n",
    "\n",
    "        if normalize != 0:\n",
    "            lambda_history_STRidge = np.append(lambda_history_STRidge, (w * Mreg).numpy(), axis=1)\n",
    "            ridge_append_counter += 1\n",
    "            ridge_append_counter_STRidge = np.append(ridge_append_counter_STRidge, ridge_append_counter)\n",
    "            return (Mreg * w).numpy()\n",
    "        else:\n",
    "            lambda_history_STRidge = np.append(lambda_history_STRidge, (w * Mreg).numpy(), axis=1)\n",
    "            ridge_append_counter += 1\n",
    "            ridge_append_counter_STRidge = np.append(ridge_append_counter_STRidge, ridge_append_counter)\n",
    "            return w.numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chidi\\AppData\\Local\\Temp\\ipykernel_5856\\218216156.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.x = torch.tensor(X[:, 0:1], dtype=torch.float32, requires_grad=True).to(device)\n",
      "C:\\Users\\chidi\\AppData\\Local\\Temp\\ipykernel_5856\\218216156.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t = torch.tensor(X[:, 1:2], dtype=torch.float32, requires_grad=True).to(device)\n",
      "C:\\Users\\chidi\\AppData\\Local\\Temp\\ipykernel_5856\\218216156.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u = torch.tensor(u, dtype=torch.float32).to(device)\n",
      "C:\\Users\\chidi\\AppData\\Local\\Temp\\ipykernel_5856\\218216156.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.x_f = torch.tensor(X_f[:, 0:1], dtype=torch.float32, requires_grad=True).to(device)\n",
      "C:\\Users\\chidi\\AppData\\Local\\Temp\\ipykernel_5856\\218216156.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_f = torch.tensor(X_f[:, 1:2], dtype=torch.float32, requires_grad=True).to(device)\n",
      "C:\\Users\\chidi\\AppData\\Local\\Temp\\ipykernel_5856\\218216156.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.x_val = torch.tensor(X_val[:, 0:1], dtype=torch.float32).to(device)\n",
      "C:\\Users\\chidi\\AppData\\Local\\Temp\\ipykernel_5856\\218216156.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_val = torch.tensor(X_val[:, 1:2], dtype=torch.float32).to(device)\n",
      "C:\\Users\\chidi\\AppData\\Local\\Temp\\ipykernel_5856\\218216156.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.u_val = torch.tensor(u_val, dtype=torch.float32).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L-BFGS-B pretraining begins\n",
      "STRidge begins\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 80\u001b[0m\n\u001b[0;32m     68\u001b[0m model \u001b[38;5;241m=\u001b[39m PhysicsInformedNN(\n\u001b[0;32m     69\u001b[0m     torch\u001b[38;5;241m.\u001b[39mtensor(X_u_train, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[0;32m     70\u001b[0m     u_train,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     76\u001b[0m     ub\n\u001b[0;32m     77\u001b[0m )\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 80\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[66], line 143\u001b[0m, in \u001b[0;36mPhysicsInformedNN.train\u001b[1;34m(self, nIter)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nIter):\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;66;03m# Perform STRidge optimization (custom method)\u001b[39;00m\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSTRidge begins\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 143\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallTrainSTRidge\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;66;03m# Adam optimization loop\u001b[39;00m\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdam begins\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[66], line 202\u001b[0m, in \u001b[0;36mPhysicsInformedNN.callTrainSTRidge\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    199\u001b[0m print_best_tol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 202\u001b[0m     Phi_pred, u_t_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet_f\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_f\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt_f\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    204\u001b[0m lambda2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mTrainSTRidge(Phi_pred, u_t_pred, lam, d_tol, maxit, STR_iters, l0_penalty, normalize, split, print_best_tol)\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlambda1\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(lambda2, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "Cell \u001b[1;32mIn[66], line 87\u001b[0m, in \u001b[0;36mPhysicsInformedNN.net_f\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnet_f\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[0;32m     86\u001b[0m     u \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet_u(x, t)\n\u001b[1;32m---> 87\u001b[0m     u_t \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     88\u001b[0m     u_x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(u, x, grad_outputs\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mones_like(u), create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     89\u001b[0m     u_xx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(u_x, x, grad_outputs\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mones_like(u_x), create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\chidi\\anaconda3\\envs\\boot_DA\\lib\\site-packages\\torch\\autograd\\__init__.py:496\u001b[0m, in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[0;32m    492\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[0;32m    493\u001b[0m         grad_outputs_\n\u001b[0;32m    494\u001b[0m     )\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m    507\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[0;32m    508\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[0;32m    509\u001b[0m     ):\n",
      "File \u001b[1;32mc:\\Users\\chidi\\anaconda3\\envs\\boot_DA\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "from pyDOE import lhs\n",
    "\n",
    "# Define the model architecture\n",
    "layers = [2, 20, 20, 20, 20, 20, 20, 20, 20, 1]\n",
    "\n",
    "# =============================================================================\n",
    "# Load data\n",
    "# =============================================================================\n",
    "data = scipy.io.loadmat(\"C:/Users/chidi/Downloads/burgers.mat\")\n",
    "\n",
    "t = np.real(data['t'].flatten()[:, None])\n",
    "x = np.real(data['x'].flatten()[:, None])\n",
    "Exact = np.real(data['usol']).T\n",
    "\n",
    "X, T = np.meshgrid(x, t)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:, None], T.flatten()[:, None]))\n",
    "u_star = Exact.flatten()[:, None]\n",
    "\n",
    "# Domain bounds\n",
    "lb = torch.tensor(X_star.min(0), dtype=torch.float32)\n",
    "ub = torch.tensor(X_star.max(0), dtype=torch.float32)\n",
    "\n",
    "# Measurement data\n",
    "N_u_s = 10  # Number of spatial points\n",
    "idx_s = np.random.choice(x.shape[0], N_u_s, replace=False)\n",
    "X0 = X[:, idx_s]\n",
    "T0 = T[:, idx_s]\n",
    "Exact0 = Exact[:, idx_s]\n",
    "\n",
    "N_u_t = int(t.shape[0] * 1)  # Number of temporal points\n",
    "idx_t = np.random.choice(t.shape[0], N_u_t, replace=False)\n",
    "X0 = X0[idx_t, :]\n",
    "T0 = T0[idx_t, :]\n",
    "Exact0 = Exact0[idx_t, :]\n",
    "\n",
    "X_u_meas = np.hstack((X0.flatten()[:, None], T0.flatten()[:, None]))\n",
    "u_meas = Exact0.flatten()[:, None]\n",
    "\n",
    "# Split data into training and validation\n",
    "Split_TrainVal = 0.8\n",
    "N_u_train = int(X_u_meas.shape[0] * Split_TrainVal)\n",
    "idx_train = np.random.choice(X_u_meas.shape[0], N_u_train, replace=False)\n",
    "X_u_train = X_u_meas[idx_train, :]\n",
    "u_train = u_meas[idx_train, :]\n",
    "\n",
    "idx_val = np.setdiff1d(np.arange(X_u_meas.shape[0]), idx_train, assume_unique=True)\n",
    "X_u_val = X_u_meas[idx_val, :]\n",
    "u_val = u_meas[idx_val, :]\n",
    "\n",
    "# Collocation points\n",
    "N_f = 50000\n",
    "X_f_train = lb + (ub - lb) * torch.tensor(lhs(2, N_f), dtype=torch.float32)\n",
    "X_f_train = torch.cat((X_f_train, torch.tensor(X_u_train, dtype=torch.float32)), dim=0)\n",
    "\n",
    "# Option: Add noise\n",
    "noise = 0.1\n",
    "u_train = torch.tensor(u_train, dtype=torch.float32) + noise * torch.std(torch.tensor(u_train, dtype=torch.float32)) * torch.randn_like(torch.tensor(u_train, dtype=torch.float32))\n",
    "u_val = torch.tensor(u_val, dtype=torch.float32) + noise * torch.std(torch.tensor(u_val, dtype=torch.float32)) * torch.randn_like(torch.tensor(u_val, dtype=torch.float32))\n",
    "\n",
    "# =============================================================================\n",
    "# Model training\n",
    "# =============================================================================\n",
    "# Initialize model (requires the PyTorch version of PhysicsInformedNN)\n",
    "model = PhysicsInformedNN(\n",
    "    torch.tensor(X_u_train, dtype=torch.float32),\n",
    "    u_train,\n",
    "    X_f_train,\n",
    "    torch.tensor(X_u_val, dtype=torch.float32),\n",
    "    u_val,\n",
    "    layers,\n",
    "    lb,\n",
    "    ub\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.train(6)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "boot_DA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

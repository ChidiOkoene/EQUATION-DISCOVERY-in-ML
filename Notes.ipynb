{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Note on Equation Discovery in Machine Learning"]},{"cell_type":"markdown","metadata":{},"source":["In machine learning and neural networks, equation discovery typically involves deriving or identifying mathematical formulations that govern how models learn from data. Here are some key equations and concepts commonly used in these fields:\n","\n","### 1. **Loss Functions**\n","The loss function quantifies the difference between predicted values and actual values. Common loss functions include:\n","\n","- **Mean Squared Error (MSE)** for regression:\n","  \\[\n","  L(y, \\hat{y}) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n","  \\]\n","\n","- **Cross-Entropy Loss** for classification:\n","  \\[\n","  L(y, \\hat{y}) = -\\frac{1}{n} \\sum_{i=1}^{n} [y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)]\n","  \\]\n","\n","### 2. **Gradient Descent**\n","Gradient descent is an optimization algorithm used to minimize the loss function by updating the model parameters (weights). The update rule is:\n","\\[\n","w \\leftarrow w - \\eta \\nabla L(w)\n","\\]\n","where \\(w\\) are the model parameters, \\(\\eta\\) is the learning rate, and \\(\\nabla L(w)\\) is the gradient of the loss with respect to the parameters.\n","\n","### 3. **Activation Functions**\n","Activation functions introduce non-linearity into the model. Common activation functions include:\n","\n","- **Sigmoid**:\n","  \\[\n","  \\sigma(x) = \\frac{1}{1 + e^{-x}}\n","  \\]\n","\n","- **ReLU (Rectified Linear Unit)**:\n","  \\[\n","  \\text{ReLU}(x) = \\max(0, x)\n","  \\]\n","\n","- **Softmax** (for multi-class classification):\n","  \\[\n","  \\text{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}}\n","  \\]\n","\n","### 4. **Backpropagation**\n","The backpropagation algorithm is used to compute the gradients for updating the weights in neural networks. The chain rule is applied as follows:\n","\\[\n","\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial a} \\cdot \\frac{\\partial a}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w}\n","\\]\n","where \\(a\\) is the activation from the current layer, and \\(z\\) is the linear combination of inputs and weights.\n","\n","### 5. **Convolutional Neural Networks (CNNs)**\n","For CNNs, the convolution operation is defined as:\n","\\[\n","(S * K)(i, j) = \\sum_m \\sum_n S(m, n) K(i - m, j - n)\n","\\]\n","where \\(S\\) is the input feature map, \\(K\\) is the filter/kernel, and \\((i, j)\\) are the spatial dimensions.\n","\n","### 6. **Recurrent Neural Networks (RNNs)**\n","In RNNs, the hidden state at time \\(t\\) is computed as:\n","\\[\n","h_t = f(W_h h_{t-1} + W_x x_t + b)\n","\\]\n","where \\(h_t\\) is the hidden state, \\(x_t\\) is the input at time \\(t\\), \\(W_h\\) and \\(W_x\\) are weight matrices, and \\(b\\) is the bias.\n","\n","### Conclusion\n","Discovering and deriving equations in machine learning often involves understanding these foundational concepts and how they interconnect. "]},{"cell_type":"markdown","metadata":{},"source":["\n","## 1. Overview of Equation Discovery\n","\n","Equation discovery, or symbolic regression, is a powerful computational approach aimed at identifying the mathematical relationships that govern datasets. Unlike traditional regression methods, which rely on predefined models to fit data, equation discovery seeks to derive both the structure (the form of the equation) and the parameters (the coefficients or constants) from the data itself. This method is particularly valuable in various scientific and engineering fields, as it allows researchers to uncover underlying relationships without making strong assumptions about the system being studied.\n","\n","### Significance:\n","- **Flexibility**: Equation discovery is model-agnostic, meaning it can uncover a wide variety of relationships without being limited to specific forms, such as linear or polynomial equations.\n","- **Discovery of New Insights**: This approach can lead to the identification of novel relationships or laws that may not have been previously considered, fostering innovation and new hypotheses in scientific research.\n","- **Interpretability**: The equations discovered are often simpler and more interpretable than complex black-box models, making it easier for researchers to communicate findings and apply them in practical scenarios.\n","\n","### Applications:\n","- **Physical Sciences**: Uncovering fundamental physical laws from experimental data.\n","- **Biological Research**: Identifying relationships between variables in complex biological systems.\n","- **Economics and Social Sciences**: Analyzing relationships between economic indicators and social factors.\n","\n","---\n","\n","## 2. Methods Used for Equation Discovery\n","\n","### a. Symbolic Regression\n","\n","Symbolic regression employs evolutionary algorithms to explore the vast space of possible mathematical expressions, searching for those that best describe the data. Unlike standard regression methods, it does not require prior assumptions about the functional form of the relationship.\n","\n","#### Genetic Programming (GP):\n","- **Mechanism**: GP mimics the process of natural selection by evolving a population of candidate equations over generations. Each candidate is evaluated based on a fitness function, such as the mean squared error between predicted and actual values.\n","- **Operations**: Selection, mutation, and crossover are used to create new generations of equations, leading to the discovery of high-performing models.\n","\n","#### Tree-based Representation:\n","- Equations are represented as tree structures, where nodes are mathematical operators (e.g., +, -, *, /, sin) and leaves are the variables or constants. This representation allows for the manipulation and combination of various mathematical expressions.\n","\n","### Impact:\n","- **Comprehensive Exploration**: The flexibility of GP allows for thorough exploration of potential models, leading to the identification of complex relationships.\n","- **Applicability**: Symbolic regression can be applied across various domains, from physics to finance, to discover relationships that are not apparent through traditional methods.\n","\n","---\n","\n","### b. Sparse Identification of Nonlinear Dynamics (SINDy)\n","\n","SINDy is a technique that aims to discover governing equations of dynamical systems from data by leveraging sparsity. It operates on the assumption that only a few terms are relevant to the dynamics, which allows it to effectively model complex systems with fewer parameters.\n","\n","#### Methodology:\n","- **Library Creation**: A library of potential terms (polynomials, trigonometric functions, etc.) is generated from the data.\n","- **Sparse Regression**: Techniques like LASSO (Least Absolute Shrinkage and Selection Operator) are used to promote sparsity in the regression, identifying only the most significant terms in the governing equation.\n","\n","### Impact:\n","- **Efficiency**: SINDy reduces the dimensionality of the problem by focusing on the most relevant terms, leading to simpler and more interpretable models.\n","- **Real-world Applications**: It is particularly useful in fluid dynamics, biological systems, and other complex systems where understanding the underlying dynamics is crucial.\n","\n","---\n","\n","### c. Bayesian Methods\n","\n","Bayesian approaches to equation discovery utilize probabilistic models to search for equations that best describe the data. By incorporating prior knowledge about potential relationships, these methods can provide a more nuanced understanding of uncertainty in the discovered equations.\n","\n","#### Process:\n","- **Prior Distribution**: Researchers define a prior distribution over possible equations based on their beliefs or prior knowledge.\n","- **Likelihood Calculation**: The likelihood of observing the data given each candidate equation is calculated.\n","- **Posterior Distribution**: Bayes' theorem is applied to update the beliefs about the equations, resulting in a posterior distribution that reflects the most probable candidates.\n","\n","### Impact:\n","- **Uncertainty Quantification**: Bayesian methods provide not just a single equation but a range of plausible models, allowing researchers to account for uncertainty in their findings.\n","- **Adaptability**: These methods can be adapted to various types of data and prior knowledge, making them versatile tools for equation discovery.\n","\n","---\n","\n","### d. Deep Learning Approaches\n","\n","Recent advancements in deep learning have led to the development of methods that can automatically learn equations from data, particularly through physics-informed neural networks (PINNs). These networks are designed to respect the physical laws governing a system while fitting the data.\n","\n","#### Mechanism:\n","- **Integration of Physics**: PINNs incorporate the known physics of the problem into the learning process, constraining the neural network to respect these laws during training.\n","- **Differential Equations**: By embedding the governing differential equations directly into the loss function, these networks can effectively learn both the model and the underlying dynamics.\n","\n","### Impact:\n","- **Complex Systems**: Deep learning approaches can handle high-dimensional data and complex relationships that traditional methods may struggle with.\n","- **Innovative Solutions**: These techniques can lead to breakthroughs in fields such as fluid dynamics, materials science, and other areas where traditional modeling approaches may fail.\n","\n","---\n","\n","## 3. Tools and Libraries for Equation Discovery\n","\n","Several tools and libraries have been developed to facilitate equation discovery and symbolic regression:\n","\n","### gplearn\n","- **Description**: A Python library that implements genetic programming for symbolic regression.\n","- **Features**: It allows users to easily set up and run symbolic regression experiments, with options for customizing the population size, mutation rates, and selection methods.\n","\n","### PySR (Python Symbolic Regression)\n","- **Description**: An efficient symbolic regression library based on evolutionary algorithms and genetic programming.\n","- **Features**: PySR offers a user-friendly interface and optimizations that make it suitable for large datasets and complex equations.\n","\n","### DataRobot Eureqa\n","- **Description**: A commercial tool designed for automated equation discovery using genetic programming techniques.\n","- **Features**: Eureqa provides a robust platform for exploring and validating discovered equations, making it accessible to practitioners without deep programming expertise.\n","\n","### Impact of Tools:\n","- **Accessibility**: These tools democratize access to advanced equation discovery techniques, enabling researchers and practitioners to explore data-driven insights without needing extensive coding skills.\n","- **Integration**: Many of these libraries can be integrated with existing data science workflows, facilitating broader adoption and application of equation discovery methods.\n","\n","---\n","\n","## 4. Applications of Equation Discovery\n","\n","Equation discovery has a wide range of applications across various disciplines:\n","\n","### Physics\n","- **Governing Equations**: Discovering fundamental laws, such as Newton’s laws of motion or thermodynamic principles, from experimental data can validate or refine existing theories.\n","\n","### Biology\n","- **Modeling Dynamics**: In areas like population dynamics or enzyme kinetics, equation discovery can help model complex biological processes and predict outcomes based on varying conditions.\n","\n","### Economics\n","- **Economic Modeling**: Identifying relationships between economic indicators, such as unemployment and inflation rates, can provide insights into economic behavior and policy implications.\n","\n","### Engineering\n","- **Process Optimization**: In engineering, developing empirical models to describe system behavior can lead to more efficient designs and improved control processes in manufacturing or robotics.\n","\n","### Impact:\n","- **Interdisciplinary Collaboration**: The applicability of equation discovery across disciplines fosters collaboration between fields, leading to richer insights and innovative solutions to complex problems.\n","\n","---\n","\n","## 5. Challenges in Equation Discovery\n","\n","While equation discovery presents numerous advantages, it is not without its challenges:\n","\n","### Complexity of Equations\n","- **Interpretability**: Discovered equations may be overly complex, making it difficult to extract meaningful insights or communicate findings effectively to a broader audience.\n","- **Model Selection**: Determining the simplest and most effective model can be challenging, as more complex models may fit the data better but lack generalizability.\n","\n","### Noise Sensitivity\n","- **Overfitting**: Noisy data can lead to overfitting, where the model captures random fluctuations rather than true underlying patterns, compromising the model's predictive capabilities.\n","\n","### Computation-intensive\n","- **Resource Requirements**: Techniques like genetic programming can be computationally expensive, particularly for large datasets or complex systems, requiring significant time and resources for exploration and validation.\n","\n","### Strategies to Overcome Challenges:\n","- **Regularization Techniques**: Employing regularization methods can help manage complexity and reduce overfitting by penalizing overly complex models.\n","- **Cross-Validation**: Implementing robust validation techniques, such as cross-validation, can help ensure that discovered equations generalize well to unseen data.\n","\n","---\n","\n","## Example Workflow for Symbolic Regression\n","\n","The process of symbolic regression through equation discovery can be broken down into a structured workflow:\n","\n","1. **Input**: Begin with a dataset containing one or more input variables (features) and a target variable (output).\n","2. **Search**: Utilize the selected algorithm (e.g., genetic programming) to search for mathematical expressions that best fit the data, exploring the space of possible equations.\n","3. **Optimization**: Optimize the parameters of the discovered equation using methods to minimize error (such as mean squared error) or maximize a defined fitness function.\n","4. **Validation**: Validate the discovered equation against a separate test dataset to ensure its effectiveness and generalizability to unseen data.\n","\n","### Importance of the Workflow:\n","- **Systematic Approach**: This structured workflow ensures a comprehensive exploration of the data while maintaining rigor in model validation, leading to reliable and actionable insights.\n","- **Facilitation of Discovery**: By following a methodical process, researchers can effectively uncover hidden relationships in data, advancing scientific understanding and driving innovation.\n","\n","---\n"]},{"attachments":{"image.png":{"image/png":"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"}},"cell_type":"markdown","metadata":{},"source":["\n","\n","### **Symbolic Regression Overview**\n","\n","Traditional regression, like linear or polynomial regression, requires a predefined model or equation structure. For example, with linear regression, you assume the model has the form \\( y = ax + b \\) and then adjust \\( a \\) and \\( b \\) to minimize the error with the observed data. Symbolic regression, however, does not assume any particular form for the underlying equation. It uses computational techniques to automatically **discover both the equation structure and parameters** that best fit the data.\n","\n","Symbolic regression is advantageous because it can reveal **underlying relationships in data** that aren’t obvious or may not fit common regression types. This makes it especially useful in fields like physics, biology, and engineering, where finding an empirical formula can simplify complex phenomena and give insight into natural laws.\n","\n","Symbolic regression (SR) is a type of regression analysis that searches the space of mathematical expressions to find the model that best fits a given dataset, both in terms of accuracy and simplicity.\n","\n","No particular model is provided as a starting point for symbolic regression. Instead, initial expressions are formed by randomly combining mathematical building blocks such as mathematical operators, analytic functions, constants, and state variables. Usually, a subset of these primitives will be specified by the person operating it, but that's not a requirement of the technique. The symbolic regression problem for mathematical functions has been tackled with a variety of methods, including recombining equations most commonly using genetic programming,[1] as well as more recent methods utilizing Bayesian methods[2] and neural networks.[3] Another non-classical alternative method to SR is called Universal Functions Originator (UFO), which has a different mechanism, search-space, and building strategy.[4] Further methods such as Exact Learning attempt to transform the fitting problem into a moments problem in a natural function space, usually built around generalizations of the Meijer-G function.[5]\n","\n","By not requiring a priori specification of a model, symbolic regression isn't affected by human bias, or unknown gaps in domain knowledge. It attempts to uncover the intrinsic relationships of the dataset, by letting the patterns in the data itself reveal the appropriate models, rather than imposing a model structure that is deemed mathematically tractable from a human perspective. The fitness function that drives the evolution of the models takes into account not only error metrics (to ensure the models accurately predict the data), but also special complexity measures,[6] thus ensuring that the resulting models reveal the data's underlying structure in a way that's understandable from a human perspective. This facilitates reasoning and favors the odds of getting insights about the data-generating system, as well as improving generalisability and extrapolation behaviour by preventing overfitting. Accuracy and simplicity may be left as two separate objectives of the regression—in which case the optimum solutions form a Pareto front—or they may be combined into a single objective by means of a model selection principle such as minimum description length.\n","\n","It has been proven that symbolic regression is an NP-hard problem, in the sense that one cannot always find the best possible mathematical expression to fit to a given dataset in polynomial time.[7] Nevertheless, if the sought-for equation is not too complex it is possible to solve the symbolic regression problem exactly by generating every possible function (built from some predefined set of operators) and evaluating them on the dataset in question.[8]\n","\n","Difference from classical regression\n","While conventional regression techniques seek to optimize the parameters for a pre-specified model structure, symbolic regression avoids imposing prior assumptions, and instead infers the model from the data. In other words, it attempts to discover both model structures and model parameters.\n","\n","This approach has the disadvantage of having a much larger space to search, because not only the search space in symbolic regression is infinite, but there are an infinite number of models which will perfectly fit a finite data set (provided that the model complexity isn't artificially limited). This means that it will possibly take a symbolic regression algorithm longer to find an appropriate model and parametrization, than traditional regression techniques. This can be attenuated by limiting the set of building blocks provided to the algorithm, based on existing knowledge of the system that produced the data; but in the end, using symbolic regression is a decision that has to be balanced with how much is known about the underlying system.\n","\n","Nevertheless, this characteristic of symbolic regression also has advantages: because the evolutionary algorithm requires diversity in order to effectively explore the search space, the result is likely to be a selection of high-scoring models (and their corresponding set of parameters). Examining this collection could provide better insight into the underlying process, and allows the user to identify an approximation that better fits their needs in terms of accuracy and simplicity.\n","\n","\n","#### Key Techniques in Symbolic Regression\n","\n","Symbolic regression often employs techniques like:\n","1. **Genetic Programming (GP)**\n","2. **Tree-based Representations**\n","3. **Evolutionary Computation**\n","\n","Let’s break these down one by one.\n","\n","---\n","\n","### Genetic Programming (GP)\n","\n","Genetic programming is the backbone of symbolic regression. It’s an optimization technique inspired by biological evolution that evolves a population of candidate solutions (in this case, equations) over multiple generations. \n","\n","In symbolic regression, GP starts with a **population of random equations** and applies genetic operations such as **selection, crossover (recombination), and mutation** to evolve the equations over time. Each equation has a fitness score based on how well it fits the data, and the best equations are more likely to be selected for reproduction in the next generation. \n","\n","\n","![image.png](attachment:image.png)\n","\n","In artificial intelligence, genetic programming (GP) is a technique of evolving programs, starting from a population of unfit (usually random) programs, fit for a particular task by applying operations analogous to natural genetic processes to the population of programs.\n","\n","The operations are: selection of the fittest programs for reproduction (crossover), replication and/or mutation according to a predefined fitness measure, usually proficiency at the desired task. The crossover operation involves swapping specified parts of selected pairs (parents) to produce new and different offspring that become part of the new generation of programs. Some programs not selected for reproduction are copied from the current generation to the new generation. Mutation involves substitution of some random part of a program with some other random part of a program. Then the selection and other operations are recursively applied to the new generation of programs.\n","\n","Typically, members of each new generation are on average more fit than the members of the previous generation, and the best-of-generation program is often better than the best-of-generation programs from previous generations. Termination of the evolution usually occurs when some individual program reaches a predefined proficiency or fitness level.\n","\n","It may and often does happen that a particular run of the algorithm results in premature convergence to some local maximum which is not a globally optimal or even good solution. Multiple runs (dozens to hundreds) are usually necessary to produce a very good result. It may also be necessary to have a large starting population size and variability of the individuals to avoid pathologies.\n","\n","History\n","The first record of the proposal to evolve programs is probably that of Alan Turing in 1950.[1] There was a gap of 25 years before the publication of John Holland's 'Adaptation in Natural and Artificial Systems' laid out the theoretical and empirical foundations of the science. In 1981, Richard Forsyth demonstrated the successful evolution of small programs, represented as trees, to perform classification of crime scene evidence for the UK Home Office.[2]\n","\n","Although the idea of evolving programs, initially in the computer language Lisp, was current amongst John Holland's students,[3] it was not until they organised the first Genetic Algorithms (GA) conference in Pittsburgh that Nichael Cramer[4] published evolved programs in two specially designed languages, which included the first statement of modern \"tree-based\" Genetic Programming (that is, procedural languages organized in tree-based structures and operated on by suitably defined GA-operators). In 1988, John Koza (also a PhD student of John Holland) patented his invention of a GA for program evolution.[5] This was followed by publication in the International Joint Conference on Artificial Intelligence IJCAI-89.[6]\n","\n","Koza followed this with 205 publications on “Genetic Programming” (GP), name coined by David Goldberg, also a PhD student of John Holland.[7] However, it is the series of 4 books by Koza, starting in 1992[8] with accompanying videos,[9] that really established GP. Subsequently, there was an enormous expansion of the number of publications with the Genetic Programming Bibliography, surpassing 10,000 entries.[10] In 2010, Koza[11] listed 77 results where Genetic Programming was human competitive.\n","\n","In 1996, Koza started the annual Genetic Programming conference[12] which was followed in 1998 by the annual EuroGP conference,[13] and the first book[14] in a GP series edited by Koza. 1998 also saw the first GP textbook.[15] GP continued to flourish, leading to the first specialist GP journal[16] and three years later (2003) the annual Genetic Programming Theory and Practice (GPTP) workshop was established by Rick Riolo.[17][18] Genetic Programming papers continue to be published at a diversity of conferences and associated journals. Today there are nineteen GP books including several for students.[15]\n","\n","Foundational work in GP\n","Early work that set the stage for current genetic programming research topics and applications is diverse, and includes software synthesis and repair, predictive modeling, data mining,[19] financial modeling,[20] soft sensors,[21] design,[22] and image processing.[23] Applications in some areas, such as design, often make use of intermediate representations,[24] such as Fred Gruau's cellular encoding.[25] Industrial uptake has been significant in several areas including finance, the chemical industry, bioinformatics[26][27] and the steel industry.[28]\n","\n","Methods\n","Program representation\n","\n","A function represented as a tree structure\n","Main article: genetic representation\n","GP evolves computer programs, traditionally represented in memory as tree structures.[29] Trees can be easily evaluated in a recursive manner. Every internal node has an operator function and every terminal node has an operand, making mathematical expressions easy to evolve and evaluate. Thus traditionally GP favors the use of programming languages that naturally embody tree structures (for example, Lisp; other functional programming languages are also suitable).\n","\n","Non-tree representations have been suggested and successfully implemented, such as linear genetic programming which perhaps suits the more traditional imperative languages.[30][31] The commercial GP software Discipulus uses automatic induction of binary machine code (\"AIM\")[32] to achieve better performance. μGP[33] uses directed multigraphs to generate programs that fully exploit the syntax of a given assembly language. Multi expression programming uses Three-address code for encoding solutions. Other program representations on which significant research and development have been conducted include programs for stack-based virtual machines,[34][35][36] and sequences of integers that are mapped to arbitrary programming languages via grammars.[37][38] Cartesian genetic programming is another form of GP, which uses a graph representation instead of the usual tree based representation to encode computer programs.\n","\n","Most representations have structurally noneffective code (introns). Such non-coding genes may seem to be useless because they have no effect on the performance of any one individual. However, they alter the probabilities of generating different offspring under the variation operators, and thus alter the individual's variational properties. Experiments seem to show faster convergence when using program representations that allow such non-coding genes, compared to program representations that do not have any non-coding genes.[39][40] Instantiations may have both trees with introns and those without; the latter are called canonical trees. Special canonical crossover operators are introduced that maintain the canonical structure of parents in their children.\n","\n","Selection\n","Selection is a process whereby certain individuals are selected from the current generation that would serve as parents for the next generation. The individuals are selected probabilistically such that the better performing individuals have a higher chance of getting selected.[18] The most commonly used selection method in GP is tournament selection, although other methods such as fitness proportionate selection, lexicase selection,[41] and others have been demonstrated to perform better for many GP problems.\n","\n","Elitism, which involves seeding the next generation with the best individual (or best n individuals) from the current generation, is a technique sometimes employed to avoid regression.\n","\n","Crossover\n","In Genetic Programming two fit individuals are chosen from the population to be parents for one or two children. In tree genetic programming, these parents are represented as inverted lisp like trees, with their root nodes at the top. In subtree crossover in each parent a subtree is randomly chosen. (Highlighted with yellow in the animation.) In the root donating parent (in the animation on the left) the chosen subtree is removed and replaced with a copy of the randomly chosen subtree from the other parent, to give a new child tree.\n","\n","Sometimes two child crossover is used, in which case the removed subtree (in the animation on the left) is not simply deleted but is copied to a copy of the second parent (here on the right) replacing (in the copy) its randomly chosen subtree. Thus this type of subtree crossover takes two fit trees and generates two child trees. Genetic programming subtree crossover\n","\n","Replication\n","Some individuals selected according to fitness criteria do not participate in crossover, but are copied into the next generation, akin to asexual reproduction in the natural world. They may be further subject to mutation.\n","\n","Mutation\n","There are many types of mutation in genetic programming. They start from a fit syntactically correct parent and aim to randomly create a syntactically correct child. In the animation a subtree is randomly chosen (highlighted by yellow). It is removed and replaced by a randomly generated subtree.\n","\n","Other mutation operators select a leaf (external node) of the tree and replace it with a randomly chosen leaf. Another mutation is to select at random a function (internal node) and replace it with another function with the same arity (number of inputs). Hoist mutation randomly chooses a subtree and replaces it with a subtree within itself. Thus hoist mutation is guaranteed to make the child smaller. Leaf and same arity function replacement ensure the child is the same size as the parent. Whereas subtree mutation (in the animation) may, depending upon the function and terminal sets, have a bias to either increase or decrease the tree size. Other subtree based mutations try to carefully control the size of the replacement subtree and thus the size of the child tree.\n","\n","\n","Animation of creating genetic programing child by mutating parent removing subtree and replacing with random code\n","Similarly there are many types of linear genetic programming mutation, each of which tries to ensure the mutated child is still syntactically correct.\n","\n","Applications\n","GP has been successfully used as an automatic programming tool, a machine learning tool and an automatic problem-solving engine.[18] GP is especially useful in the domains where the exact form of the solution is not known in advance or an approximate solution is acceptable (possibly because finding the exact solution is very difficult). Some of the applications of GP are curve fitting, data modeling, symbolic regression, feature selection, classification, etc. John R. Koza mentions 76 instances where Genetic Programming has been able to produce results that are competitive with human-produced results (called Human-competitive results).[42] Since 2004, the annual Genetic and Evolutionary Computation Conference (GECCO) holds Human Competitive Awards (called Humies) competition,[43] where cash awards are presented to human-competitive results produced by any form of genetic and evolutionary computation. GP has won many awards in this competition over the years.\n","\n","Meta-genetic programming\n","Meta-genetic programming is the proposed meta-learning technique of evolving a genetic programming system using genetic programming itself. It suggests that chromosomes, crossover, and mutation were themselves evolved, therefore like their real life counterparts should be allowed to change on their own rather than being determined by a human programmer. Meta-GP was formally proposed by Jürgen Schmidhuber in 1987.[44] Doug Lenat's Eurisko is an earlier effort that may be the same technique. It is a recursive but terminating algorithm, allowing it to avoid infinite recursion. In the \"autoconstructive evolution\" approach to meta-genetic programming, the methods for the production and variation of offspring are encoded within the evolving programs themselves, and programs are executed to produce new programs to be added to the population.[35][45]\n","\n","Critics of this idea often say this approach is overly broad in scope. However, it might be possible to constrain the fitness criterion onto a general class of results, and so obtain an evolved GP that would more efficiently produce results for sub-classes. This might take the form of a meta evolved GP for producing human walking algorithms which is then used to evolve human running, jumping, etc. The fitness criterion applied to the meta GP would simply be one of efficiency.\n","\n","#### Steps in Genetic Programming:\n","\n","1. **Initialization**: A population of random equations is generated. Each equation is randomly structured and might include a mix of mathematical operations (like addition, multiplication, sine, etc.), constants, and variables.\n","\n","2. **Evaluation (Fitness Scoring)**: Each candidate equation is tested against the dataset, and its fitness is scored, usually based on how well it predicts the output values. A common metric for fitness is **mean squared error (MSE)**.\n","\n","3. **Selection**: Equations with higher fitness scores are more likely to be selected for reproduction. This is analogous to “survival of the fittest,” where equations that fit the data well are more likely to pass on their structures to the next generation.\n","\n","4. **Crossover (Recombination)**: Selected equations can be recombined to create new “offspring” equations. This process mimics biological recombination, where parts of two “parent” equations are combined. For example, if one parent equation has a multiplication operation in the middle and the other has an addition operation, the offspring might have a mix of both substructures.\n","\n","5. **Mutation**: Random changes are introduced into some equations to maintain diversity in the population and allow exploration of new solution spaces. For instance, a “+” operation might be randomly changed to a “-” operation, or a constant might be modified.\n","\n","6. **Termination**: This process of evaluation, selection, crossover, and mutation continues until a stopping criterion is met. Common stopping criteria include reaching a maximum number of generations or achieving a satisfactory fitness score.\n","\n","**Why Genetic Programming Works Well**:\n","- GP can search across a **vast space of possible equations** without needing assumptions about the equation form.\n","- By exploring this space through generations, GP can often converge on simpler, interpretable equations that generalize well to new data.\n","- It is adaptable to various problems, allowing for custom constraints and adjustments depending on the domain or complexity needed.\n","\n","---\n","\n","### Tree-Based Representation in Genetic Programming\n","\n","In symbolic regression, equations are typically represented as **trees**, where:\n","- **Internal nodes** are mathematical operations (e.g., +, -, *, /, sin, cos).\n","- **Leaf nodes** are constants or variables (e.g., \\( x \\), \\( y \\), 3.14).\n","\n","This tree-based structure provides flexibility to represent complex mathematical expressions. \n","\n","#### Example of Tree-Based Representation\n","\n","Consider the equation \\( y = 3x + 4 \\).\n","\n","The tree representation would look like this:\n","\n","```\n","      +\n","     / \\\n","    *   4\n","   / \\\n","  3   x\n","```\n","\n","- The root node is “+,” indicating that the final operation is addition.\n","- The left branch of the root node has a multiplication operation with children nodes “3” and “x.”\n","- The right branch of the root node is the constant “4.”\n","\n","By representing equations as trees, GP can apply genetic operations like crossover and mutation more effectively. For instance:\n","- **Crossover** can exchange entire subtrees between two parent trees, creating new equation structures.\n","- **Mutation** can replace a subtree or modify nodes, allowing new mathematical expressions to be explored.\n","\n","### Example: Applying GP to Symbolic Regression\n","\n","Suppose we have data points showing a pattern we believe could be described by a mathematical equation. Using GP, the algorithm might proceed as follows:\n","\n","1. **Start with a random set of equations**:\n","   - Equation 1: \\( y = x^2 + 3 \\)\n","   - Equation 2: \\( y = 4 \\sin(x) - x \\)\n","   - Equation 3: \\( y = \\cos(x) + x^3 \\)\n","\n","2. **Evaluate fitness**: Calculate the error between each equation’s predictions and the actual data.\n","\n","3. **Select and Reproduce**: Choose the best-performing equations to participate in crossover and mutation, producing new equations such as:\n","   - New Equation 1: \\( y = x^2 + \\cos(x) \\)\n","   - New Equation 2: \\( y = 4x - x^3 \\)\n","\n","4. **Iterate**: Repeat the evaluation, selection, crossover, and mutation steps, generating increasingly accurate equations over generations.\n","\n","---\n","\n","### Strengths of Symbolic Regression with Genetic Programming\n","\n","1. **Flexibility in Model Discovery**:\n","   - GP does not require prior assumptions about the functional form of the model. It can explore a vast space of possible equations, from polynomials to trigonometric forms, based on the dataset.\n","\n","2. **Interpretable Equations**:\n","   - GP aims to find equations that are both accurate and interpretable, which is crucial in fields like physics and biology, where simpler equations may reveal fundamental laws.\n","\n","3. **Adaptable Complexity**:\n","   - GP’s ability to find complex or simple equations based on fitness constraints means it can generalize to various problem types, from simple linear relationships to complex nonlinear dynamics.\n","\n","4. **Global Search Capability**:\n","   - GP is a global optimization method, meaning it can escape local minima and potentially find a better solution than conventional regression methods.\n","\n","---\n","\n","### Challenges and Considerations in Symbolic Regression with GP\n","\n","1. **Computational Expense**:\n","   - GP is resource-intensive. Since it must evaluate a large population of candidate equations over multiple generations, it can be computationally costly for large datasets or complex models.\n","\n","2. **Overfitting and Complexity Control**:\n","   - Without proper constraints, GP may generate overly complex equations that fit the data well but do not generalize. Regularization techniques or limits on tree depth and complexity can mitigate this issue.\n","\n","3. **Need for High-Quality Data**:\n","   - Symbolic regression is sensitive to noisy data. Noise can lead GP to generate spurious, complex equations, which may not reflect true underlying patterns.\n","\n","4. **Fine-Tuning of Parameters**:\n","   - Parameters like population size, mutation rate, and generation limits must be carefully tuned to balance exploration (finding diverse equations) and exploitation (optimizing existing candidates).\n","\n","---\n","\n","### Practical Applications\n","\n","Symbolic regression with GP is widely used in scientific and engineering disciplines for equation discovery. Some applications include:\n","\n","- **Physics**: Discovering fundamental laws or equations, such as those describing planetary motion, electrical circuits, or fluid dynamics.\n","- **Biology**: Modeling population dynamics, gene expression, or enzyme kinetics.\n","- **Economics**: Identifying relationships between economic variables without assuming traditional econometric models.\n","- **Engineering**: Developing empirical models for complex systems, such as chemical processes or environmental systems.\n","\n","---\n","\n","### Summary\n","\n","Symbolic regression with genetic programming is a powerful method for **automatically discovering mathematical models** that describe data. By evolving equations as trees through a process that mirrors biological evolution, GP can explore a wide space of potential solutions, from simple linear forms to complex, nonlinear functions. With its strengths in flexibility, interpretability, and global optimization, symbolic regression with GP has proven to be a valuable tool for researchers and engineers looking to unlock hidden relationships in their data."]},{"cell_type":"markdown","metadata":{},"source":["### **Evolutionary algorithm**\n","\n","In computational intelligence (CI), an evolutionary algorithm (EA) is a subset of evolutionary computation,[1] a generic population-based metaheuristic optimization algorithm. An EA uses mechanisms inspired by biological evolution, such as reproduction, mutation, recombination, and selection. Candidate solutions to the optimization problem play the role of individuals in a population, and the fitness function determines the quality of the solutions (see also loss function). Evolution of the population then takes place after the repeated application of the above operators.\n","\n","Evolutionary algorithms often perform well approximating solutions to all types of problems because they ideally do not make any assumption about the underlying fitness landscape. Techniques from evolutionary algorithms applied to the modeling of biological evolution are generally limited to explorations of microevolutionary processes and planning models based upon cellular processes. In most real applications of EAs, computational complexity is a prohibiting factor.[2] In fact, this computational complexity is due to fitness function evaluation. Fitness approximation is one of the solutions to overcome this difficulty. However, seemingly simple EA can solve often complex problems;[3][4][5] therefore, there may be no direct link between algorithm complexity and problem complexity.\n","\n","### Implementation\n","The following is an example of a generic single-objective genetic algorithm.\n","\n","**Step One:** Generate the initial population of individuals randomly. (First generation)\n","\n","**Step Two:** Repeat the following regenerational steps until termination (time limit, sufficient fitness achieved, etc.):\n","\n","Evaluate the fitness of each individual in the population\n","Select the individuals for reproduction based on their fitness. (Parents)\n","Breed new individuals through crossover and mutation operations to give birth to offspring.\n","Replace the least-fit individuals of the population with new individuals.\n","Types\n","Similar techniques differ in genetic representation and other implementation details, and the nature of the particular applied problem.\n","\n","**Genetic algorithm** – This is the most popular type of EA. One seeks the solution of a problem in the form of strings of numbers (traditionally binary, although the best representations are usually those that reflect something about the problem being solved),[2] by applying operators such as recombination and mutation (sometimes one, sometimes both). This type of EA is often used in optimization problems.\n","Genetic programming – Here the solutions are in the form of computer programs, and their fitness is determined by their ability to solve a computational problem. There are many variants of Genetic Programming, including Cartesian genetic programming, gene expression programming, grammatical evolution, linear genetic programming, multi expression programming etc.\n","**Evolutionary programming** – Similar to genetic programming, but the structure of the program is fixed and its numerical parameters are allowed to evolve.\n","**Evolution strategy** – Works with vectors of real numbers as representations of solutions, and typically uses self-adaptive mutation rates. The method is mainly used for numerical optimization, although there are also variants for combinatorial tasks.[6][7]\n","Differential evolution – Based on vector differences and is therefore primarily suited for numerical optimization problems.\n","**Coevolutionary algorithm** – Similar to genetic algorithms and evolution strategies, but the created solutions are compared on the basis of their outcomes from interactions with other solutions. Solutions can either compete or cooperate during the search process. Coevolutionary algorithms are often used in scenarios where the fitness landscape is dynamic, complex, or involves competitive interactions.[8][9]\n","**Neuroevolution** – Similar to genetic programming but the genomes represent artificial neural networks by describing structure and connection weights. The genome encoding can be direct or indirect.\n","**Learning classifier system** – Here the solution is a set of classifiers (rules or conditions). A Michigan-LCS evolves at the level of individual classifiers whereas a Pittsburgh-LCS uses populations of classifier-sets. Initially, classifiers were only binary, but now include real, neural net, or S-expression types. Fitness is typically determined with either a strength or accuracy based reinforcement learning or supervised learning approach.\n","Quality–Diversity algorithms – QD algorithms simultaneously aim for high-quality and diverse solutions. Unlike traditional optimization algorithms that solely focus on finding the best solution to a problem, QD algorithms explore a wide variety of solutions across a problem space and keep those that are not just high performing, but also diverse and unique.[10][11][12]\n","Theoretical background\n","The following theoretical principles apply to all or almost all EAs.\n","\n","**No free lunch theorem**\n","The no free lunch theorem of optimization states that all optimization strategies are equally effective when the set of all optimization problems is considered. Under the same condition, no evolutionary algorithm is fundamentally better than another. This can only be the case if the set of all problems is restricted. This is exactly what is inevitably done in practice. Therefore, to improve an EA, it must exploit problem knowledge in some form (e.g. by choosing a certain mutation strength or a problem-adapted coding). Thus, if two EAs are compared, this constraint is implied. In addition, an EA can use problem specific knowledge by, for example, not randomly generating the entire start population, but creating some individuals through heuristics or other procedures.[13][14] Another possibility to tailor an EA to a given problem domain is to involve suitable heuristics, local search procedures or other problem-related procedures in the process of generating the offspring. This form of extension of an EA is also known as a memetic algorithm. Both extensions play a major role in practical applications, as they can speed up the search process and make it more robust.[13][15]\n","\n","**Convergence**\n","Further information: Convergence (evolutionary computing)\n","See also: Convergent evolution\n","For EAs in which, in addition to the offspring, at least the best individual of the parent generation is used to form the subsequent generation (so-called elitist EAs), there is a general proof of convergence under the condition that an optimum exists. Without loss of generality, a maximum search is assumed for the proof:\n","\n","From the property of elitist offspring acceptance and the existence of the optimum it follows that per generation \n","Given a probability $P > 0$, we have:\n","$$F(x'_1) \\leq F(x'_2) \\leq F(x'_3) \\leq \\cdots \\leq F(x'_k) \\leq \\cdots$$\n","i.e., \n"," the fitness values represent a monotonically non-decreasing sequence, which is bounded due to the existence of the optimum. From this follows the convergence of the sequence against the optimum.\n","\n","Since the proof makes no statement about the speed of convergence, it is of little help in practical applications of EAs. But it does justify the recommendation to use elitist EAs. However, when using the usual panmictic population model, elitist EAs tend to converge prematurely more than non-elitist ones.[16] In a panmictic population model, mate selection (step 2 of the section about implementation) is such that every individual in the entire population is eligible as a mate. In non-panmictic populations, selection is suitably restricted, so that the dispersal speed of better individuals is reduced compared to panmictic ones. Thus, the general risk of premature convergence of elitist EAs can be significantly reduced by suitable population models that restrict mate selection.[17][18]\n","\n","Virtual alphabets\n","With the theory of virtual alphabets, David E. Goldberg showed in 1990 that by using a representation with real numbers, an EA that uses classical recombination operators (e.g. uniform or n-point crossover) cannot reach certain areas of the search space, in contrast to a coding with binary numbers.[19] This results in the recommendation for EAs with real representation to use arithmetic operators for recombination (e.g. arithmetic mean or intermediate recombination). With suitable operators, real-valued representations are more effective than binary ones, contrary to earlier opinion.[20][21]\n","\n","Comparison to biological processes\n","A possible limitation[according to whom?] of many evolutionary algorithms is their lack of a clear genotype–phenotype distinction. In nature, the fertilized egg cell undergoes a complex process known as embryogenesis to become a mature phenotype. This indirect encoding is believed to make the genetic search more robust (i.e. reduce the probability of fatal mutations), and also may improve the evolvability of the organism.[22][23] Such indirect (also known as generative or developmental) encodings also enable evolution to exploit the regularity in the environment.[24] Recent work in the field of artificial embryogeny, or artificial developmental systems, seeks to address these concerns. And gene expression programming successfully explores a genotype–phenotype system, where the genotype consists of linear multigenic chromosomes of fixed length and the phenotype consists of multiple expression trees or computer programs of different sizes and shapes.[25][improper synthesis?]\n","\n","Comparison to Monte-Carlo methods\n","Both method classes have in common that their individual search steps are determined by chance. The main difference, however, is that EAs, like many other metaheuristics, learn from past search steps and incorporate this experience into the execution of the next search steps in a method-specific form. With EAs, this is done firstly through the fitness-based selection operators for partner choice and the formation of the next generation. And secondly, in the type of search steps: In EA, they start from a current solution and change it or they mix the information of two solutions. In contrast, when dicing out new solutions in Monte-Carlo methods, there is usually no connection to existing solutions.[26][27]\n","\n","If, on the other hand, the search space of a task is such that there is nothing to learn, Monte-Carlo methods are an appropriate tool, as they do not contain any algorithmic overhead that attempts to draw suitable conclusions from the previous search. An example of such tasks is the proverbial search for a needle in a haystack, e.g. in the form of a flat (hyper)plane with a single narrow peak.\n","\n","Applications\n","The areas in which evolutionary algorithms are practically used are almost unlimited[5] and range from industry,[28][29] engineering,[2][3][30] complex scheduling,[4][31][32] agriculture,[33] robot movement planning[34] and finance[35][36] to research[37][38] and art. The application of an evolutionary algorithm requires some rethinking from the inexperienced user, as the approach to a task using an EA is different from conventional exact methods and this is usually not part of the curriculum of engineers or other disciplines. For example, the fitness calculation must not only formulate the goal but also support the evolutionary search process towards it, e.g. by rewarding improvements that do not yet lead to a better evaluation of the original quality criteria. For example, if peak utilisation of resources such as personnel deployment or energy consumption is to be avoided in a scheduling task, it is not sufficient to assess the maximum utilisation. Rather, the number and duration of exceedances of a still acceptable level should also be recorded in order to reward reductions below the actual maximum peak value.[39] There are therefore some publications that are aimed at the beginner and want to help avoiding beginner's mistakes as well as leading an application project to success.[39][40][41] This includes clarifying the fundamental question of when an EA should be used to solve a problem and when it is better not to.\n","\n","Related techniques and other global search methods\n","There are some other proven and widely used methods of nature inspired global search techniques such as\n","\n","Memetic algorithm – A hybrid method, inspired by Richard Dawkins's notion of a meme. It commonly takes the form of a population-based algorithm (frequently an EA) coupled with individual learning procedures capable of performing local refinements. Emphasizes the exploitation of problem-specific knowledge and tries to orchestrate local and global search in a synergistic way.\n","A celular evolutionary or memetic algorithm uses a topological neighbouhood relation between the individuals of a population for restricting the mate selection and by that reducing the propagation speed of above-average individuals. The idea is to maintain genotypic diversity in the poulation over a longer period of time to reduce the risk of premature convergence.\n","Ant colony optimization is based on the ideas of ant foraging by pheromone communication to form paths. Primarily suited for combinatorial optimization and graph problems.\n","Particle swarm optimization is based on the ideas of animal flocking behaviour. Also primarily suited for numerical optimization problems.\n","Gaussian adaptation – Based on information theory. Used for maximization of manufacturing yield, mean fitness or average information. See for instance Entropy in thermodynamics and information theory.\n","In addition, many new nature-inspired or methaphor-guided algorithms have been proposed since the beginning of this century. For criticism of most publications on these, see the remarks at the end of the introduction to the article on metaheuristics.\n","\n","Examples\n","In 2020, Google stated that their AutoML-Zero can successfully rediscover classic algorithms such as the concept of neural networks.[42]\n","\n","The computer simulations Tierra and Avida attempt to model macroevolutionary dynamics."]},{"cell_type":"markdown","metadata":{},"source":["### **Evolutionary Computation: An Extensive Overview**\n","\n","Evolutionary Computation (EC) is a subset of artificial intelligence and optimization techniques inspired by natural evolution. These algorithms apply the principles of natural selection, mutation, recombination, and reproduction to iteratively improve a population of candidate solutions, allowing the model to optimize complex problems without the need for a predefined solution structure. In the context of machine learning and symbolic regression, EC is particularly effective in **discovering optimal equations, structures, and parameters** by mimicking the survival-of-the-fittest mechanisms seen in biological evolution.\n","\n","---\n","\n","### Key Concepts in Evolutionary Computation\n","\n","The key concepts of EC are:\n","1. **Population**: A collection of candidate solutions.\n","2. **Fitness Function**: A measure to evaluate how well each solution performs for the given problem.\n","3. **Selection**: The process of choosing candidates for reproduction based on their fitness.\n","4. **Crossover (Recombination)**: Combining parts of two solutions to create a new solution.\n","5. **Mutation**: Introducing random changes to solutions to maintain diversity.\n","6. **Reproduction**: Generating a new population based on selected individuals.\n","\n","---\n","\n","### Step-by-Step Mechanism of Evolutionary Computation\n","\n","1. **Initialization**: \n","   - Start with a randomly generated population of candidate solutions. In symbolic regression, each solution could represent a potential mathematical equation.\n","\n","2. **Fitness Evaluation**:\n","   - Each candidate solution in the population is evaluated using a **fitness function**. The fitness function assigns a score based on how well each candidate solves the problem.\n","   - For symbolic regression, the fitness function could measure the error between the equation’s predictions and the actual data (e.g., Mean Squared Error, Mean Absolute Error).\n","\n","3. **Selection**:\n","   - Solutions with higher fitness scores are selected to pass their “genes” (structural components) to the next generation. This is analogous to natural selection, where only the fittest individuals are more likely to survive and reproduce.\n","   - Common selection methods include:\n","     - **Roulette Wheel Selection**: Candidates are chosen probabilistically based on their fitness proportion.\n","     - **Tournament Selection**: Random subsets of individuals are selected, and the one with the highest fitness within each subset is chosen.\n","\n","4. **Crossover (Recombination)**:\n","   - Selected individuals undergo crossover, where segments from two parent solutions are combined to create offspring. In symbolic regression, crossover might involve swapping parts of two equations to create a new equation.\n","   - This process introduces new candidate solutions, allowing the algorithm to explore different parts of the solution space.\n","\n","5. **Mutation**:\n","   - To prevent the population from becoming too similar and converging prematurely, random changes are applied to some solutions. This may involve modifying constants, changing operations, or introducing new variables.\n","   - Mutation ensures diversity, allowing the algorithm to potentially find better solutions.\n","\n","6. **Reproduction and Termination**:\n","   - The next generation is created using the offspring from selection, crossover, and mutation.\n","   - This process repeats over many generations, with the population ideally improving over time until a **termination condition** is met. Common termination criteria include:\n","     - A maximum number of generations.\n","     - Convergence, where successive generations show little improvement.\n","     - Achievement of an acceptable fitness score.\n","\n","---\n","\n","### Types of Evolutionary Computation Techniques\n","\n","#### 1. Genetic Algorithms (GA)\n","- **Genetic Algorithms (GA)** are perhaps the most popular form of EC. They use a chromosome-like structure to encode candidate solutions and apply genetic operations like selection, crossover, and mutation to evolve solutions.\n","- In symbolic regression, each chromosome represents an equation structure, and GA operations manipulate this structure to optimize the equation for the given dataset.\n","\n","#### 2. Genetic Programming (GP)\n","- **Genetic Programming** is an extension of GA where the solutions are represented as **tree structures**, with nodes as operations (e.g., +, -, *, /) and leaves as constants or variables.\n","- GP is particularly suited for symbolic regression as it can evolve both the structure and parameters of equations.\n","\n","#### 3. Evolution Strategies (ES)\n","- **Evolution Strategies (ES)** are focused on evolving parameters rather than structures. They are often used in numerical optimization, where candidate solutions are represented as vectors of real numbers.\n","- ES is less commonly used for symbolic regression but can be valuable in refining parameters of discovered models.\n","\n","#### 4. Differential Evolution (DE)\n","- **Differential Evolution** is a technique that maintains a population of candidate solutions and combines them in specific ways to find a solution that minimizes the fitness function.\n","- DE is known for its simplicity and effectiveness in optimizing continuous variables, making it useful in model tuning tasks.\n","\n","---\n","\n","### Symbolic Regression and Evolutionary Computation\n","\n","Symbolic regression, a prime application of EC, uses evolutionary techniques to discover mathematical expressions that best describe a dataset. Unlike traditional regression, symbolic regression:\n","- **Does not assume a predefined form** for the model, allowing the algorithm to discover nonlinear or complex relationships.\n","- **Simultaneously optimizes structure and parameters**, which is particularly valuable for discovering scientific laws or empirical models from data.\n","\n","#### Process of Symbolic Regression with Genetic Programming\n","\n","In symbolic regression using GP, equations are represented as **tree structures** with:\n","- **Operators as nodes** (e.g., +, -, *, sin).\n","- **Constants and variables as leaves**.\n","\n","Each tree structure represents a possible equation, and the EC mechanism is used to evolve these trees through selection, crossover, and mutation, iteratively refining them until a satisfactory solution is found.\n","\n","**Example Workflow:**\n","\n","1. **Random Population of Equations**:\n","   - Start with random equations like \\( y = x^2 + 3 \\), \\( y = \\sin(x) - x \\), etc.\n","  \n","2. **Evaluate Fitness**:\n","   - Calculate the fitness of each equation by comparing its predictions with observed data.\n","\n","3. **Selection and Genetic Operations**:\n","   - Select high-fitness equations, recombine parts of their trees (crossover), and introduce mutations to create new equations.\n","\n","4. **Repeat for Multiple Generations**:\n","   - This process continues for many generations until the best-fit equation is found.\n","\n","---\n","\n","### Advantages and Strengths of Evolutionary Computation\n","\n","1. **Flexibility**:\n","   - EC can optimize various types of problems, including both structure (model form) and parameters.\n","\n","2. **Global Search Capability**:\n","   - EC is a global optimization technique, meaning it explores large areas of the solution space and is less likely to get stuck in local optima.\n","\n","3. **Adaptability to Complex Problems**:\n","   - EC can discover complex, nonlinear relationships without requiring assumptions about the model structure, making it valuable for symbolic regression.\n","\n","4. **Automatic Model Discovery**:\n","   - In applications like symbolic regression, EC can autonomously discover interpretable models, equations, or rules from data.\n","\n","---\n","\n","### Challenges and Limitations\n","\n","1. **Computational Intensity**:\n","   - EC requires evaluating many solutions across generations, which can be computationally expensive, especially for large datasets or complex equations.\n","\n","2. **Risk of Overfitting**:\n","   - Without proper control, EC might evolve overly complex solutions that fit the training data well but perform poorly on unseen data.\n","\n","3. **Dependence on Proper Parameter Tuning**:\n","   - EC algorithms often require careful tuning of parameters like population size, mutation rate, and crossover probability to achieve optimal performance.\n","\n","4. **Sensitivity to Noise**:\n","   - Symbolic regression using EC may be sensitive to noise, potentially producing spurious models if the data quality is low.\n","\n","---\n","\n","### Applications of Evolutionary Computation\n","\n","Evolutionary Computation is widely applied across many fields:\n","- **Symbolic Regression**: Discovering empirical equations in physics, biology, and chemistry.\n","- **Optimization Problems**: Solving complex optimization problems in engineering, logistics, and finance.\n","- **Automated Feature Engineering**: Generating new features or transformations in machine learning pipelines.\n","- **Design and Control**: Optimizing control parameters in robotics, autonomous systems, and industrial processes.\n","\n","---\n","\n","### Summary\n","\n","Evolutionary Computation is a versatile and powerful approach that leverages principles of biological evolution to optimize complex, high-dimensional problems. Its applications in symbolic regression are particularly noteworthy, as it enables the discovery of mathematical models and equations from raw data without predefined assumptions. By iteratively evolving populations of solutions, EC provides a robust method for discovering interpretable and potentially insightful models in domains where traditional regression techniques may fail."]},{"cell_type":"markdown","metadata":{},"source":["### **PYTHON PACKAGES FOR SYMBOLIC REGRESSION** "]},{"cell_type":"markdown","metadata":{},"source":["The most popular Python packages for symbolic regression include **PySR (Python Symbolic Regression)**, **SymPy**, and **gplearn**. Each has unique strengths depending on your needs:\n","\n","### 1. **PySR (Python Symbolic Regression)**\n","   - **Pros**: \n","     - Fast and highly accurate; uses the Julia backend for symbolic regression.\n","     - Supports complex mathematical functions, custom operators, and multi-objective optimization.\n","     - Well-suited for scientific applications, including physics and engineering.\n","   - **Cons**: Requires Julia installation, which can add complexity.\n","   - **Best For**: Advanced symbolic regression, particularly if you need to work with complex or custom functions.  \n","   \n","### 2. **SymPy**\n","   - **Pros**: \n","     - Full symbolic computation library, with extensive symbolic manipulation tools.\n","     - Allows for detailed symbolic regression by using `solve` functions combined with custom code.\n","     - Great for algebraic manipulation, simplification, and derivation of expressions.\n","   - **Cons**: Doesn't focus on automated symbolic regression, so you need custom code to implement it.\n","   - **Best For**: Cases where you need robust symbolic math capabilities beyond just regression.  \n","\n","### 3. **gplearn**\n","   - **Pros**: \n","     - Implements genetic programming to evolve symbolic expressions.\n","     - Lightweight and integrates well with scikit-learn’s API.\n","     - Offers basic symbolic regression with customizable functions and operators.\n","   - **Cons**: Limited functionality compared to PySR, less efficient with large datasets.\n","   - **Best For**: Simple symbolic regression tasks, especially if you prefer scikit-learn's API and simplicity.\n","\n","**Recommendation**: For state-of-the-art symbolic regression, **PySR** is often the top choice due to its speed and versatility. For simple use cases or scikit-learn compatibility, **gplearn** may be more straightforward, and **SymPy** is ideal if you need symbolic computation for tasks beyond regression."]},{"cell_type":"markdown","metadata":{},"source":["### 1. **PySR (Python Symbolic Regression)**"]},{"cell_type":"markdown","metadata":{},"source":["PySR is a multi-population evolutionary algorithm with multiple evolutions performed asynchronously.\n","The main loop of PySR, operating on each population independently, is a classic evolutionary algorithm based on tournament selection for individual selection, and several mutations and crossovers for generating new individuals. Evolutionary algorithms for SR was first popularized in 1990s, and has dominated the development of flexible SR algorithms since. PySR\n","makes several modifications to this classic approach, some of which are motivated by results in recent work.\n","\n","PySR does not, per se, have any built-in operators.\n","Due to the just-in-time compiled nature of Julia, any real scalar function from the entirety of the Julia Base language is available, and can be compiled into the search code at runtime. This includes commonly-used operators such as +, -, *, /, ∧, exp, log, sin, cos, tan, abs, and many others.\n","\n","Furthermore, since many domains of science have operators that are unique to their field, it is possible to pass an arbitrary Julia function as an operator, whether it be a binary operator (with two arguments) or a unary operator (with one argument). Any function of the form f : R → R or R\n","2 → R, whether continuous or not, can be used as a user-defined operator. The function need only be designed for scalar\n","input and output, and PySR will use Julia to automatically compile and vectorize it, generating SIMD (Single Instruction, Multiple Data) instructions when possible. \n","\n"," In PySR, an example of this would be:\n","\n","**op = \"special(x, y) = cos(x) * (x + y)\"**\n","\n","**model = PySRRegressor(binary_operators=[op])**\n","\n","where the string *special(x, y)* = **cos(x) * (x + y)** is Julia code giving a function definition.\n","This would define a binary operator special that would be compiled into the search code. To enable custom operators to be defined in the various\n","export functionality, the user must also define equivalent operators in SymPy (here, lambda x, y: sympy.cos(x) * (x + y)), as well as JAX\n","or PyTorch versions if the user wishes to export to those frameworks as well.\n"]},{"cell_type":"markdown","metadata":{},"source":["**Custom Losses**\n","\n","In many machine learning toy datasets for benchmarking regression algorithms, Mean-Square-Error\n","(MSE or L2 loss) is typically used as a learning objective. In a Bayesian framework, MSE is equivalent to assuming every data point is Gaussian distributed, with equal variance per point. Minimizing\n","MSE is equivalent to maximizing the Gaussian loglikelihood. However, in science, one typically works with a likelihood that is very specific to a particular problem, and this is often non-Gaussian. Therefore, it is important for an SR package to allow for custom loss functions. PySR implements this in a way that is very similar to that of custom operators. \n","\n","Given a string such as\n"," \n","**“loss(x, y) = abs(x - y)”**, \n","\n","PySR will pass this to the Julia backend, which will automatically vectorize it and use it as a loss function throughout the search process. In a Bayesian context, this would allow one to define arbitrary likelihoods, even for very complex branching logic. This also works for weighted losses, such as \n","\n","**“loss(x, y, w) = abs(x - y) * w”**"]},{"cell_type":"markdown","metadata":{},"source":["### Detailed Example\n","The following code makes use of as many PySR features as possible."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from pysr import PySRRegressor\n","\n","model = PySRRegressor(\n","    procs=4,\n","    #Number of cores to use for the training process.\n","    # Default: `None` (uses all available cores)\n","    populations=8,\n","    # ^ 2 populations per core, so one is always running.\n","    population_size=50,\n","    # ^ Slightly larger populations, for greater diversity.\n","    ncycles_per_iteration=500,\n","    # ^ Generations between migrations.\n","    niterations=10000000,  # Run forever\n","    early_stop_condition=(\n","        \"stop_if(loss, complexity) = loss < 1e-6 && complexity < 10\"\n","        # Stop early if we find a good and simple equation\n","    ),\n","    timeout_in_seconds=60 * 60 * 24,\n","    # ^ Alternatively, stop after 24 hours have passed.\n","    maxsize=50,\n","    # ^ Allow greater complexity.\n","    maxdepth=10,\n","    # ^ But, avoid deep nesting.\n","    binary_operators=[\"*\", \"+\", \"-\", \"/\"],\n","    unary_operators=[\"square\", \"cube\", \"exp\", \"cos2(x)=cos(x)^2\"],\n","    constraints={\n","        \"/\": (-1, 9),\n","        \"square\": 9,\n","        \"cube\": 9,\n","        \"exp\": 9,\n","    },\n","    # ^ Limit the complexity within each argument.\n","    # \"inv\": (-1, 9) states that the numerator has no constraint,\n","    # but the denominator has a max complexity of 9.\n","    # \"exp\": 9 simply states that `exp` can only have\n","    # an expression of complexity 9 as input.\n","    nested_constraints={\n","        \"square\": {\"square\": 1, \"cube\": 1, \"exp\": 0},\n","        \"cube\": {\"square\": 1, \"cube\": 1, \"exp\": 0},\n","        \"exp\": {\"square\": 1, \"cube\": 1, \"exp\": 0},\n","    },\n","    # ^ Nesting constraints on operators. For example,\n","    # \"square(exp(x))\" is not allowed, since \"square\": {\"exp\": 0}.\n","    complexity_of_operators={\"/\": 2, \"exp\": 3},\n","    # ^ Custom complexity of particular operators.\n","    complexity_of_constants=2,\n","    # ^ Punish constants more than variables\n","    select_k_features=4,\n","    # ^ Train on only the 4 most important features\n","    progress=True,\n","    # ^ Can set to false if printing to a file.\n","    weight_randomize=0.1,\n","    # ^ Randomize the tree much more frequently\n","    cluster_manager=None,\n","    # ^ Can be set to, e.g., \"slurm\", to run a slurm\n","    # cluster. Just launch one script from the head node.\n","    precision=64,\n","    # ^ Higher precision calculations.\n","    warm_start=True,\n","    # ^ Start from where left off.\n","    turbo=True,\n","    # ^ Faster evaluation (experimental)\n","    variable_names=[f\"x{i}\" for i in range(X.shape[1])],\n","    #creation of varibles from the array or dataframe\n","    extra_sympy_mappings={\"cos2\": lambda x: sympy.cos(x)**2},\n","    # extra_torch_mappings={sympy.cos: torch.cos},\n","    # ^ Not needed as cos already defined, but this\n","    # is how you define custom torch operators.\n","    # extra_jax_mappings={sympy.cos: \"jnp.cos\"},\n","    # ^ For JAX, one passes a string.\n",")"]},{"cell_type":"markdown","metadata":{},"source":["# **PySRRegressor Parameters**\n","## The Algorithm\n","### Creating the Search Space\n","**binary_operators**\n","\n","List of strings for binary operators used in the search. See the operators page for more details.\n","\n","Default: [\"+\", \"-\", \"*\", \"/\"]\n","\n","**unary_operators**\n","\n","Operators which only take a single scalar as input. For example, \"cos\" or \"exp\".\n","\n","Default: None\n","\n","**maxsize**\n","\n","Max complexity of an equation.\n","\n","Default: 20\n","\n","**maxdepth**\n","\n","Max depth of an equation. You can use both maxsize and maxdepth. maxdepth is by default not used.\n","\n","Default: None\n","\n","### Setting the Search Size\n","**niterations**\n","\n","Number of iterations of the algorithm to run. The best equations are printed and migrate between populations at the end of each iteration.\n","\n","Default: 40\n","\n","**populations**\n","\n","Number of populations running.\n","\n","Default: 15\n","\n","**population_size**\n","\n","Number of individuals in each population.\n","\n","Default: 33\n","\n","**ncycles_per_iteration**\n","\n","Number of total mutations to run, per 10 samples of the population, per iteration.\n","\n","Default: 550\n","\n","### The Objective\n","**elementwise_loss**\n","\n","String of Julia code specifying an elementwise loss function. Can either be a loss from LossFunctions.jl, or your own loss written as a function. Examples of custom written losses include: myloss(x, y) = abs(x-y) for non-weighted, or myloss(x, y, w) = w*abs(x-y) for weighted. The included losses include: Regression: LPDistLoss{P}(), L1DistLoss(), L2DistLoss() (mean square), LogitDistLoss(), HuberLoss(d), L1EpsilonInsLoss(ϵ), L2EpsilonInsLoss(ϵ), PeriodicLoss(c), QuantileLoss(τ). Classification: ZeroOneLoss(), PerceptronLoss(), L1HingeLoss(), SmoothedL1HingeLoss(γ), ModifiedHuberLoss(), L2MarginLoss(), ExpLoss(), SigmoidLoss(), DWDMarginLoss(q).\n","\n","Default: \"L2DistLoss()\"\n","\n","**loss_function**\n","\n","Alternatively, you can specify the full objective function as a snippet of Julia code, including any sort of custom evaluation (including symbolic manipulations beforehand), and any sort of loss function or regularizations. The default loss_function used in SymbolicRegression.jl is roughly equal to:\n","\n","</function eval_loss(tree, dataset::Dataset{T,L}, options)::L where {T,L}\n","    prediction, flag = eval_tree_array(tree, dataset.X, options)\n","    if !flag\n","        return L(Inf)\n","    end\n","    return sum((prediction .- dataset.y) .^ 2) / dataset.n\n","end>\n","where the example elementwise loss is mean-squared error. You may pass a function with the same arguments as this (note that the name of the function doesn't matter). Here, both prediction and dataset.y are 1D arrays of length dataset.n. If using batching, then you should add an idx argument to the function, which is nothing for non-batched, and a 1D array of indices for batched.\n","Default: None\n","\n","**model_selection**\n","\n","Model selection criterion when selecting a final expression from the list of best expression at each complexity. Can be *'accuracy'*, *'best'*, or *'score'*. 'accuracy' selects the candidate model with the lowest loss (highest accuracy). 'score' selects the candidate model with the highest score. Score is defined as the negated derivative of the log-loss with respect to complexity - if an expression has a much better loss at a slightly higher complexity, it is preferred. 'best' selects the candidate model with the highest score among expressions with a loss better than at least 1.5x the most accurate model.\n","\n","Default: 'best'\n","\n","**dimensional_constraint_penalty**\n","\n","Additive penalty for if dimensional analysis of an expression fails. By default, this is 1000.0.\n","\n","**dimensionless_constants_only**\n","\n","Whether to only search for dimensionless constants, if using units.\n","\n","Default: False\n","\n","### Working with Complexities\n","**parsimony**\n","\n","Multiplicative factor for how much to punish complexity.\n","\n","Default: 0.0032\n","\n","**constraints**\n","\n","Dictionary of int (unary) or 2-tuples (binary), this enforces maxsize constraints on the individual arguments of operators. E.g., 'pow': (-1, 1) says that power laws can have any complexity left argument, but only 1 complexity in the right argument. Use this to force more interpretable solutions.\n","\n","Default: None\n","\n","**nested_constraints**\n","\n","Specifies how many times a combination of operators can be nested. \n","For example, \n","\n","</{\"sin\": {\"cos\": 0}}>, \n","\n","</{\"cos\": {\"cos\": 2}}> \n","\n","specifies that cos may never appear within a sin, but sin can be nested with itself an unlimited number of times. \n","The second term specifies that cos can be nested up to 2 times within a cos, so that *cos(cos(cos(x)))* is allowed (as well as any combination of + or - within it), but *cos(cos(cos(cos(x))))* is not allowed. When an operator is not specified, it is assumed that it can be nested an unlimited number of times. This requires that there is no operator which is used both in the unary operators and the binary operators (e.g., - could be both subtract, and negation). For binary operators, you only need to provide a single number: both arguments are treated the same way, and the max of each argument is constrained.\n","\n","Default: None\n","\n","**complexity_of_operators**\n","\n","If you would like to use a complexity other than 1 for an operator, specify the complexity here. For example, *{\"sin\": 2, \"+\": 1}* would give a complexity of 2 for each use of the sin operator, and a complexity of 1 for each use of the + operator (which is the default). You may specify real numbers for a complexity, and the total complexity of a tree will be rounded to the nearest integer after computing.\n","\n","Default: None\n","\n","**complexity_of_constants**\n","\n","Complexity of constants.\n","\n","Default: 1\n","\n","**complexity_of_variables**\n","\n","Global complexity of variables. To set different complexities for different variables, pass a list of complexities to the fit method with keyword complexity_of_variables. You cannot use both.\n","\n","Default: 1\n","\n","**warmup_maxsize_by**\n","\n","Whether to slowly increase max size from a small number up to the maxsize (if greater than 0). If greater than 0, says the fraction of training time at which the current maxsize will reach the user-passed maxsize.\n","\n","Default: 0.0\n","\n","**use_frequency**\n","\n","Whether to measure the frequency of complexities, and use that instead of parsimony to explore equation space. Will naturally find equations of all complexities.\n","\n","Default: True\n","\n","**use_frequency_in_tournament**\n","\n","Whether to use the frequency mentioned above in the tournament, rather than just the simulated annealing.\n","\n","Default: True\n","\n","**adaptive_parsimony_scaling**\n","\n","If the adaptive parsimony strategy (use_frequency and use_frequency_in_tournament), this is how much to (exponentially) weight the contribution. If you find that the search is only optimizing the most complex expressions while the simpler expressions remain stagnant, you should increase this value.\n","\n","Default: 20.0\n","\n","**should_simplify**\n","\n","Whether to use algebraic simplification in the search. Note that only a few simple rules are implemented.\n","\n","Default: True\n","\n","### Mutations\n","**weight_add_node**\n","\n","Relative likelihood for mutation to add a node.\n","\n","Default: 0.79\n","\n","**weight_insert_node**\n","\n","Relative likelihood for mutation to insert a node.\n","\n","Default: 5.1\n","\n","**weight_delete_node**\n","\n","Relative likelihood for mutation to delete a node.\n","\n","Default: 1.7\n","\n","**weight_do_nothing**\n","\n","Relative likelihood for mutation to leave the individual.\n","\n","Default: 0.21\n","\n","**weight_mutate_constant**\n","\n","Relative likelihood for mutation to change the constant slightly in a random direction.\n","\n","Default: 0.048\n","\n","**weight_mutate_operator**\n","\n","Relative likelihood for mutation to swap an operator.\n","\n","Default: 0.47\n","\n","**weight_swap_operands**\n","\n","Relative likehood for swapping operands in binary operators.\n","\n","Default: 0.1\n","\n","**weight_randomize**\n","\n","Relative likelihood for mutation to completely delete and then randomly generate the equation\n","\n","Default: 0.00023\n","\n","**weight_simplify**\n","\n","Relative likelihood for mutation to simplify constant parts by evaluation\n","\n","Default: 0.0020\n","\n","**weight_optimize**\n","\n","Constant optimization can also be performed as a mutation, in addition to the normal strategy controlled by optimize_probability which happens every iteration. Using it as a mutation is useful if you want to use a large ncycles_periteration, and may not optimize very often.\n","\n","Default: 0.0\n","\n","**crossover_probability**\n","\n","Absolute probability of crossover-type genetic operation, instead of a mutation.\n","\n","Default: 0.066\n","\n","**annealing**\n","\n","Whether to use annealing.\n","\n","Default: False\n","\n","**alpha**\n","\n","Initial temperature for simulated annealing (requires annealing to be True).\n","\n","Default: 0.1\n","\n","**perturbation_factor**\n","\n","Constants are perturbed by a max factor of (perturbation_factor*T + 1). Either multiplied by this or divided by this.\n","\n","Default: 0.076\n","\n","**skip_mutation_failures**\n","\n","Whether to skip mutation and crossover failures, rather than simply re-sampling the current member.\n","\n","Default: True\n","\n","### Tournament Selection\n","**tournament_selection_n**\n","\n","Number of expressions to consider in each tournament.\n","\n","Default: 10\n","\n","**tournament_selection_p**\n","\n","Probability of selecting the best expression in each tournament. The probability will decay as p*(1-p)^n for other expressions, sorted by loss.\n","\n","Default: 0.86\n","\n","### Constant Optimization\n","**optimizer_algorithm**\n","\n","Optimization scheme to use for optimizing constants. Can currently be NelderMead or BFGS.\n","\n","Default: \"BFGS\"\n","\n","**optimizer_nrestarts**\n","\n","Number of time to restart the constants optimization process with different initial conditions.\n","\n","Default: 2\n","\n","**optimize_probability**\n","\n","Probability of optimizing the constants during a single iteration of the evolutionary algorithm.\n","\n","Default: 0.14\n","\n","**optimizer_iterations**\n","\n","Number of iterations that the constants optimizer can take.\n","\n","Default: 8\n","\n","**should_optimize_constants**\n","\n","Whether to numerically optimize constants (Nelder-Mead/Newton) at the end of each iteration.\n","\n","Default: True\n","\n","### Migration between Populations\n","**fraction_replaced**\n","\n","How much of population to replace with migrating equations from other populations.\n","\n","Default: 0.000364\n","\n","**fraction_replaced_hof***\n","\n","How much of population to replace with migrating equations from hall of fame.\n","\n","Default: 0.035\n","\n","**migration***\n","\n","Whether to migrate.\n","\n","Default: True\n","\n","**hof_migration**\n","\n","Whether to have the hall of fame migrate.\n","\n","Default: True\n","\n","**topn**\n","\n","How many top individuals migrate from each population.\n","\n","Default: 12\n","\n","### Data Preprocessing\n","**denoise**\n","\n","Whether to use a Gaussian Process to denoise the data before inputting to PySR. Can help PySR fit noisy data.\n","\n","Default: False\n","\n","**select_k_features**\n","\n","Whether to run feature selection in Python using random forests, before passing to the symbolic regression code. None means no feature selection; an int means select that many features.\n","\n","Default: None\n","\n","### Stopping Criteria\n","**max_evals**\n","\n","Limits the total number of evaluations of expressions to this number.\n","\n","Default: None\n","\n","**timeout_in_seconds**\n","\n","Make the search return early once this many seconds have passed.\n","\n","Default: None\n","\n","**early_stop_condition**\n","\n","Stop the search early if this loss is reached. You may also pass a string containing a Julia function which takes a loss and complexity as input, for example: \n","\n","</\"f(loss, complexity) = (loss < 0.1) && (complexity < 10)\".>\n","\n","Default: None\n","\n","### Performance and Parallelization\n","**procs**\n","\n","Number of processes (=number of populations running).\n","\n","Default: cpu_count()\n","\n","**multithreading**\n","\n","Use multithreading instead of distributed backend. Using procs=0 will turn off both.\n","\n","Default: True\n","\n","**cluster_manager**\n","\n","For distributed computing, this sets the job queue system. Set to one of \"slurm\", \"pbs\", \"lsf\", \"sge\", \"qrsh\", \"scyld\", or \"htc\". If set to one of these, PySR will run in distributed mode, and use procs to figure out how many processes to launch.\n","\n","Default: None\n","\n","**heap_size_hint_in_bytes**\n","\n","For multiprocessing, this sets the --heap-size-hint parameter for new Julia processes. This can be configured when using multi-node distributed compute, to give a hint to each process about how much memory they can use before aggressive garbage collection.\n","\n","**batching**\n","\n","Whether to compare population members on small batches during evolution. Still uses full dataset for comparing against hall of fame.\n","\n","Default: False\n","\n","**batch_size**\n","\n","The amount of data to use if doing batching.\n","\n","Default: 50\n","\n","**precision**\n","\n","What precision to use for the data. By default this is 32 (float32), but you can select 64 or 16 as well, giving you 64 or 16 bits of floating point precision, respectively. If you pass complex data, the corresponding complex precision will be used (i.e., 64 for complex128, 32 for complex64).\n","\n","Default: 32\n","\n","**fast_cycle**\n","\n","Batch over population subsamples. This is a slightly different algorithm than regularized evolution, but does cycles 15% faster. May be algorithmically less efficient.\n","\n","Default: False\n","\n","**turbo**\n","\n","(Experimental) Whether to use LoopVectorization.jl to speed up the search evaluation. Certain operators may not be supported. Does not support 16-bit precision floats.\n","\n","Default: False\n","\n","**bumper**\n","\n","(Experimental) Whether to use Bumper.jl to speed up the search evaluation. Does not support 16-bit precision floats.\n","\n","Default: False\n","\n","**enable_autodiff**\n","\n","Whether to create derivative versions of operators for automatic differentiation. This is only necessary if you wish to compute the gradients of an expression within a custom loss function.\n","\n","Default: False\n","\n","### Determinism\n","**random_state**\n","\n","Pass an int for reproducible results across multiple function calls. See :term:Glossary <random_state>.\n","\n","Default: None\n","\n","**deterministic**\n","\n","Make a PySR search give the same result every run. To use this, you must turn off parallelism (with procs=0, multithreading=False), and set random_state to a fixed seed.\n","\n","Default: False\n","\n","**warm_start**\n","\n","Tells fit to continue from where the last call to fit finished. If false, each call to fit will be fresh, overwriting previous results.\n","\n","Default: False\n","\n","### Monitoring\n","**verbosity**\n","\n","What verbosity level to use. 0 means minimal print statements.\n","\n","Default: 1\n","\n","**update_verbosity**\n","\n","What verbosity level to use for package updates. Will take value of verbosity if not given.\n","\n","Default: None\n","\n","**print_precision**\n","\n","How many significant digits to print for floats.\n","\n","Default: 5\n","\n","**progress**\n","\n","Whether to use a progress bar instead of printing to stdout.\n","\n","Default: True\n","\n","### Environment\n","**temp_equation_file**\n","\n","Whether to put the hall of fame file in the temp directory. Deletion is then controlled with the delete_tempfiles parameter.\n","\n","Default: False\n","\n","**tempdir**\n","\n","directory for the temporary files.\n","\n","Default: None\n","\n","**delete_tempfiles**\n","\n","Whether to delete the temporary files after finishing.\n","\n","Default: True\n","\n","**update**\n","\n","Whether to automatically update Julia packages when fit is called. You should make sure that PySR is up-to-date itself first, as the packaged Julia packages may not necessarily include all updated dependencies.\n","\n","Default: False\n","\n","### Exporting the Results\n","**equation_file**\n","\n","Where to save the files (.csv extension).\n","\n","Default: None\n","\n","**output_jax_format**\n","\n","Whether to create a 'jax_format' column in the output, containing jax-callable functions and the default parameters in a jax array.\n","\n","Default: False\n","\n","**output_torch_format**\n","\n","Whether to create a 'torch_format' column in the output, containing a torch module with trainable parameters.\n","\n","Default: False\n","\n","**extra_sympy_mappings**\n","\n","Provides mappings between custom binary_operators or unary_operators defined in julia strings, to those same operators defined in sympy. E.G if unary_operators=[\"inv(x)=1/x\"], then for the fitted model to be export to sympy, extra_sympy_mappings would be {\"inv\": lambda x: 1/x}.\n","\n","Default: None\n","\n","**extra_torch_mappings**\n","\n","The same as extra_jax_mappings but for model export to pytorch. Note that the dictionary keys should be callable pytorch expressions. For example: extra_torch_mappings={sympy.sin: torch.sin}.\n","\n","Default: None\n","\n","**extra_jax_mappings**\n","\n","Similar to extra_sympy_mappings but for model export to jax. The dictionary maps sympy functions to jax functions. For example: extra_jax_mappings={sympy.sin: \"jnp.sin\"} maps the sympy.sin function to the equivalent jax expression jnp.sin.\n","\n","Default: None"]},{"cell_type":"markdown","metadata":{},"source":["Another Important **SR** in python is **gplearn** \n","\n","\n","## **Introduction to `gplearn`**\n","\n","`gplearn` is a Python library for **genetic programming**, specifically designed to solve regression problems. Genetic programming (GP) is an evolutionary algorithm-based technique inspired by biological evolution to find computer programs that perform a user-defined task. It optimizes mathematical expressions to explain relationships within data, producing interpretable models as mathematical formulas.\n","\n","### **Key Concepts**\n","\n","- **Genetic Programming (GP)**: An optimization technique where expressions or programs evolve over generations to achieve an optimal representation of data relationships.\n","  \n","- **Symbolic Regression**: Regression technique where the model tries to find the best mathematical expression that describes the relationship between input and output data.\n","\n","- **Expression Trees**: The main structure in GP. Each program (or individual) in GP is represented by an expression tree, where nodes are functions (like addition or multiplication), and leaves are variables or constants.\n","\n","---\n","\n","## **Installation**\n","\n","To install `gplearn`, use pip:\n","```bash\n","pip install gplearn\n","```\n","\n","---\n","\n","## **Basic Workflow of `gplearn`**\n","\n","1. **Define the Problem**: Choose input features and the target variable for your regression.\n","2. **Initialize the GP Algorithm**: Set up parameters, function sets, and the population.\n","3. **Run the Evolutionary Process**: The model evolves, with each generation aiming to improve model accuracy while balancing complexity.\n","4. **Evaluate and Interpret Results**: Extract the best-performing model and analyze the mathematical expression produced.\n","\n","---\n","\n","## **Setting Up `gplearn` for Symbolic Regression**\n","\n","Here’s how to use `gplearn` with a step-by-step guide.\n","\n","### **1. Import Libraries**\n","\n","```python\n","import numpy as np\n","import pandas as pd\n","from gplearn.genetic import SymbolicRegressor\n","import matplotlib.pyplot as plt\n","```\n","\n","### **2. Generate or Load Data**\n","\n","To keep things simple, let’s generate a synthetic dataset. Here, we simulate a target relationship with noise:\n","\n","```python\n","np.random.seed(42)\n","X = np.random.rand(100, 1)  # 100 samples, 1 feature\n","y = 4 * (X ** 2) + 3 * X + np.random.normal(0, 0.1, (100, 1))  # y = 4x^2 + 3x with noise\n","```\n","\n","---\n","\n","### **3. Define and Configure the `SymbolicRegressor`**\n","\n","`SymbolicRegressor` is the main class in `gplearn` for symbolic regression. Below are some key configuration options:\n","\n","- **population_size**: Size of the population.\n","- **generations**: Number of generations.\n","- **function_set**: Functions used to construct the models.\n","- **parsimony_coefficient**: Balances model accuracy and simplicity.\n","- **metric**: Evaluation metric, e.g., `\"mean squared error\"`.\n","- **p_crossover**, **p_subtree_mutation**: Probabilities of crossover and mutation events.\n","- **verbose**: Controls output verbosity.\n","  \n","Here’s how to configure a simple symbolic regression model:\n","\n","```python\n","est_gp = SymbolicRegressor(\n","    population_size=500, \n","    generations=20, \n","    stopping_criteria=0.01,\n","    function_set=[\"add\", \"sub\", \"mul\", \"div\"],\n","    p_crossover=0.7,\n","    p_subtree_mutation=0.1,\n","    p_hoist_mutation=0.05,\n","    p_point_mutation=0.05,\n","    parsimony_coefficient=0.001,\n","    max_samples=0.9,\n","    verbose=1,\n","    random_state=42\n",")\n","```\n","\n","### **4. Fit the Model**\n","\n","The fitting process evolves the population to find an optimal model based on the provided data:\n","\n","```python\n","est_gp.fit(X, y.ravel())\n","```\n","\n","---\n","\n","### **5. Evaluate and Interpret Results**\n","\n","Once the model is trained, you can examine the evolved equation and evaluate its performance on the training data.\n","\n","#### **Extracting the Best Program**\n","\n","```python\n","print(f\"The evolved expression: {est_gp._program}\")\n","```\n","\n","#### **Predicting and Plotting Results**\n","\n","Let’s see how well the model’s predictions fit the actual data.\n","\n","```python\n","# Predict on the training set\n","y_pred = est_gp.predict(X)\n","\n","# Plot results\n","plt.scatter(X, y, color='blue', label='Actual Data')\n","plt.plot(X, y_pred, color='red', label='Predicted Fit')\n","plt.xlabel('X')\n","plt.ylabel('y')\n","plt.legend()\n","plt.show()\n","```\n","\n","---\n","\n","## **Example of a Complete Workflow**\n","\n","To demonstrate the full potential, let’s dive into a slightly more complex example with custom functions and parameters.\n","\n","### **Defining Custom Functions**\n","\n","Custom functions allow more expressive relationships. Here’s how you can add a square root function to the function set.\n","\n","```python\n","from gplearn.functions import make_function\n","\n","# Define a custom square root function\n","def sqrt(x):\n","    return np.sqrt(np.abs(x))\n","\n","# Wrap the function with gplearn's make_function\n","sqrt_function = make_function(function=sqrt, name=\"sqrt\", arity=1)\n","\n","# Include it in the function set\n","function_set = [\"add\", \"sub\", \"mul\", \"div\", sqrt_function]\n","```\n","\n","### **Using Custom Functions in `SymbolicRegressor`**\n","\n","Now, let’s configure the regressor to use this custom function.\n","\n","```python\n","# Initialize the regressor with custom settings\n","est_gp_custom = SymbolicRegressor(\n","    population_size=1000,\n","    generations=30,\n","    function_set=function_set,\n","    stopping_criteria=0.01,\n","    p_crossover=0.7,\n","    p_subtree_mutation=0.1,\n","    parsimony_coefficient=0.002,\n","    max_samples=0.9,\n","    verbose=1,\n","    random_state=42\n",")\n","\n","# Fit model to data\n","est_gp_custom.fit(X, y.ravel())\n","\n","# Extract the evolved program\n","print(f\"Evolved expression with custom function: {est_gp_custom._program}\")\n","```\n","\n","---\n","\n","## **Hyperparameter Tuning Tips**\n","\n","Symbolic regression can be sensitive to hyperparameters. Here are some guidelines for tuning:\n","\n","- **Population Size and Generations**: Higher values allow more extensive exploration but take longer to compute.\n","- **Function Set**: Including a variety of functions enables richer expressions but can also increase complexity.\n","- **Parsimony Coefficient**: Higher values will favor simpler models, potentially at the cost of accuracy.\n","- **Mutation Rates**: Increasing mutation rates can introduce diversity but may also destabilize convergence.\n","\n","---\n","\n","## **Advanced Usage**\n","\n","`gplearn` provides additional options for enhancing model robustness and interpretability:\n","\n","- **Custom Metrics**: `gplearn` allows custom metrics by defining your own metric function.\n","- **Cross-Validation**: You can use cross-validation to tune hyperparameters or assess the model's performance.\n","  \n","Here’s an example that demonstrates cross-validation with `SymbolicRegressor`.\n","\n","```python\n","from sklearn.model_selection import cross_val_score\n","\n","# Define model\n","est_gp_cv = SymbolicRegressor(\n","    population_size=1000, \n","    generations=20,\n","    function_set=[\"add\", \"sub\", \"mul\", \"div\"],\n","    random_state=42\n",")\n","\n","# Perform cross-validation\n","scores = cross_val_score(est_gp_cv, X, y.ravel(), cv=5, scoring=\"neg_mean_squared_error\")\n","print(\"Cross-Validation MSE Scores:\", -scores)\n","print(\"Average MSE:\", -scores.mean())\n","```\n","\n","---\n","\n","## **Conclusion**\n","\n","`gplearn` is a powerful tool for symbolic regression that offers a blend of interpretability and expressiveness. It’s well-suited for tasks where an understandable model is essential. By fine-tuning hyperparameters and carefully curating the function set, you can evolve equations that capture complex relationships within your data.\n","\n","This guide provides the foundation to explore and experiment with `gplearn` further. As you gain experience, try tweaking different parameters and using custom functions to achieve models that best fit your unique data and requirements."]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":["### **Sparse Identification of Nonlinear Dynamics (SINDy)**\n","\n","Sparse Identification of Nonlinear Dynamics (SINDy) is a method developed to discover the governing equations of dynamical systems directly from data. It is widely used to find parsimonious (i.e., simple) models that capture the essential dynamics of complex systems, making it a powerful tool for researchers and practitioners who deal with systems in fields like physics, engineering, biology, and finance. SINDy is especially valuable because it promotes interpretability, identifying the few critical terms that drive a system’s behavior while discarding unnecessary complexities.\n","\n","---\n","\n","### Overview of SINDy\n","\n","Traditional approaches to modeling dynamical systems often involve **symbolic regression (SR)**, where models are built based on assumed relationships and structures. However, these methods can be computationally intensive and are not always interpretable. SINDy, on the other hand, applies **sparsity-promoting techniques** to identify the minimal set of terms necessary to describe a system, resulting in simple, interpretable models.\n","\n","\n","\n","#### Key Points of SINDy:\n","\n","1. **Sparse Representation**: It assumes that only a few terms or functions are needed to represent the system dynamics, thus promoting a sparse (minimalistic) model structure.\n","2. **Library of Candidate Functions**: SINDy constructs a library of potential terms or functions and selects the relevant ones using regression techniques.\n","3. **Sparse Regression**: Techniques like **LASSO** (Least Absolute Shrinkage and Selection Operator) are used to select only the significant terms.\n","4. **Data-Driven**: SINDy relies on data rather than predefined equations, which makes it versatile and capable of revealing unknown dynamical relationships.\n","\n","---\n","\n","### Mechanism of SINDy\n","\n","To understand how SINDy works, let’s break down the process step-by-step.\n","\n","1. **Collect Data on System Dynamics**:\n","   - Measure data points that describe the dynamics of the system over time.\n","   - For a system state \\( x(t) \\) and its time derivative \\( \\dot{x}(t) \\), collect time-series data for both \\( x(t) \\) and \\( \\dot{x}(t) \\).\n","\n","2. **Define a Library of Candidate Functions**:\n","   - Create a matrix of potential functions, \\(\\Theta(X)\\), which includes functions like constants, linear terms, polynomial terms, trigonometric terms, etc.\n","   - For example, if \\( x = [x_1, x_2] \\), \\(\\Theta(X)\\) might include terms like \\( 1 \\), \\( x_1 \\), \\( x_2 \\), \\( x_1^2 \\), \\( x_2^2 \\), \\( x_1x_2 \\), \\( \\sin(x_1) \\), and so forth.\n","\n","3. **Sparse Regression**:\n","   - For each component \\( \\dot{x}_i \\) of \\( \\dot{x} \\), perform sparse regression to identify which terms in \\(\\Theta(X)\\) are significant.\n","   - Techniques like **LASSO** are applied to enforce sparsity, resulting in coefficients for only the relevant terms.\n","\n","4. **Construct the Model**:\n","   - After identifying the non-zero terms, construct the governing equation using only those terms.\n","   - The resulting model is a simplified version of the original system dynamics but captures the core behavior.\n","\n","---\n","\n","### Example of SINDy in Action\n","\n","Consider a two-dimensional dynamical system defined by \\( x = [x_1, x_2] \\). Assume we have measurements of \\( x(t) \\) and \\( \\dot{x}(t) \\).\n","\n","#### Step 1: Collect Data\n","Suppose we measure:\n","\\[\n","x_1(t) = \\sin(t), \\quad x_2(t) = \\cos(t)\n","\\]\n","and their derivatives:\n","\\[\n","\\dot{x}_1(t) = \\cos(t), \\quad \\dot{x}_2(t) = -\\sin(t)\n","\\]\n","\n","#### Step 2: Define Candidate Functions\n","Construct a library of candidate functions, \\(\\Theta(X)\\), with terms like:\n","\\[\n","\\Theta(X) = \\begin{bmatrix} 1 & x_1 & x_2 & x_1^2 & x_2^2 & x_1x_2 & \\sin(x_1) & \\cos(x_2) \\end{bmatrix}\n","\\]\n","\n","#### Step 3: Sparse Regression\n","Using sparse regression, identify which terms from \\(\\Theta(X)\\) best represent \\( \\dot{x}_1 \\) and \\( \\dot{x}_2 \\).\n","\n","#### Step 4: Construct the Model\n","Assume that sparse regression selects:\n","\\[\n","\\dot{x}_1 = x_2 \\quad \\text{and} \\quad \\dot{x}_2 = -x_1\n","\\]\n","Thus, the discovered model is:\n","\\[\n","\\dot{x}_1 = x_2, \\quad \\dot{x}_2 = -x_1\n","\\]\n","This model reflects the original dynamics (a harmonic oscillator) and captures the essential behavior with minimal terms.\n","\n","---\n","\n","### Applications of SINDy\n","\n","SINDy’s applications span a variety of fields:\n","\n","1. **Physics**:\n","   - Modeling fundamental systems, such as oscillators, planetary motion, and wave equations.\n","   - Example: Discovering the equations of motion for a chaotic pendulum or identifying forces in a physical system.\n","\n","2. **Biology**:\n","   - Capturing population dynamics, enzyme kinetics, or the spread of diseases.\n","   - Example: Modeling predator-prey interactions or infectious disease dynamics from epidemiological data.\n","\n","3. **Engineering**:\n","   - Control systems, signal processing, and mechanical systems can benefit from SINDy’s parsimonious models.\n","   - Example: Identifying governing equations in fluid dynamics or the behavior of robotic systems.\n","\n","4. **Finance**:\n","   - SINDy can help capture the core dynamics in financial markets by identifying factors influencing asset prices or economic indicators.\n","   - Example: Modeling market behavior based on fundamental economic variables.\n","\n","---\n","\n","### Pros and Cons of SINDy Compared to Symbolic Regression (SR)\n","\n","Both SINDy and Symbolic Regression aim to identify relationships and governing equations from data, but they differ in approach and characteristics.\n","\n","#### Pros of SINDy\n","\n","1. **Computational Efficiency**:\n","   - SINDy is generally faster than traditional SR methods due to its reliance on sparse regression rather than complex evolutionary techniques.\n","\n","2. **Interpretability**:\n","   - The sparse model SINDy produces is often simpler and more interpretable than models generated by SR, making it ideal for scientific applications.\n","\n","3. **Model Parsimony**:\n","   - By promoting sparsity, SINDy only includes the essential terms, which reduces the risk of overfitting and increases the model’s generalizability.\n","\n","4. **Structured Approach**:\n","   - SINDy uses a structured approach based on the assumption that only a few terms are needed, making it more targeted and less prone to unnecessary complexity.\n","\n","#### Cons of SINDy\n","\n","1. **Limited Function Library**:\n","   - SINDy relies on a predefined library of candidate functions. If the true dynamics are not represented in this library, the method may fail to capture the correct model.\n","\n","2. **Requires Sparse Structure**:\n","   - SINDy’s effectiveness hinges on the assumption of sparsity. If the system dynamics are highly complex with many interacting terms, SINDy may struggle to identify the correct model.\n","\n","3. **Noise Sensitivity**:\n","   - SINDy can be sensitive to noise, which may lead to incorrect terms being selected or significant terms being omitted.\n","\n","#### Comparison with Symbolic Regression (SR)\n","\n","| Aspect                  | SINDy                                      | Symbolic Regression (SR)                   |\n","|-------------------------|--------------------------------------------|--------------------------------------------|\n","| **Model Complexity**    | Promotes sparsity and parsimony            | Can produce complex models                |\n","| **Computation**         | Relatively fast with sparse regression     | Computationally intensive (e.g., genetic programming) |\n","| **Assumptions**         | Assumes sparse, simple dynamics            | No assumptions on sparsity or simplicity  |\n","| **Function Flexibility**| Limited by predefined function library     | Can evolve complex and nonlinear functions|\n","| **Noise Robustness**    | Sensitive to noise                         | More robust to noise                      |\n","| **Interpretability**    | High (sparse models are easy to interpret) | Moderate to high (depends on model complexity) |\n","\n","---\n","\n","### Example Application Scenario\n","\n","Consider a robotic system navigating a terrain. The system’s dynamics, including velocity and orientation, change based on the landscape and the robot’s position. Assume we have:\n","- **Data**: Time-series data of the robot’s position, velocity, and orientation.\n","- **Goal**: Identify a model to predict the robot’s motion dynamics based on its current state.\n","\n","Using SINDy:\n","\n","1. **Construct a Library of Candidate Terms**:\n","   - Include terms like linear velocities, accelerations, trigonometric functions (e.g., \\( \\sin \\) and \\( \\cos \\) of angles), and interaction terms (e.g., \\( x \\cdot v \\) where \\( x \\) is position and \\( v \\) is velocity).\n","\n","2. **Sparse Regression**:\n","   - Use LASSO or a similar sparse regression technique to identify which terms are significant in determining the robot’s state changes over time.\n","\n","3. **Resulting Model**:\n","   - SINDy might reveal a simplified model capturing the core dynamics, such as:\n","     \\[\n","     \\dot{x} = v \\cos(\\theta), \\quad \\dot{y} = v \\sin(\\theta)\n","     \\]\n","     where \\( v \\) is the robot’s speed and \\( \\theta \\) its orientation.\n","\n","This model is parsimonious and interpretable, making it suitable for real-time control\n","\n"," and prediction in robotics.\n","\n","---\n","\n","### Summary\n","\n","SINDy offers a powerful approach for identifying the core governing equations of complex dynamical systems in a computationally efficient and interpretable manner. While it requires sparsity assumptions and may not perform as well in highly noisy or complex systems, SINDy’s strengths in promoting model parsimony and interpretability make it an essential tool in the data-driven discovery of dynamical systems."]}],"metadata":{"kernelspec":{"display_name":"boot_DA","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":2}
